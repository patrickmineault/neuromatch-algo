Row,RegistrantID,NameFirst,NameLast,Email,Affiliation,Phone,AddressCity,AddressState,AddressZIP,AddressCountry,RepresentativeWork,MindMatchPersons,MindMatchExclude,mindMatchScholarID
1,1134,Jascha,Achterberg,jascha.achterberg@mrc-cbu.cam.ac.uk,University of Cambridge,+44-7873-440-025,Cambridge,,CB2 7EF,United Kingdom,"Much animal learning is slow, with cumulative changes in behavior driven by reward prediction errors. When the abstract structure of a problem is known, however, both animals and formal learning models can rapidly attach new items to their roles within this structure, sometimes in a single trial. Frontal cortex is likely to play a key role in this process. To examine information seeking and use in a known problem structure, we trained monkeys in an explore/exploit task, requiring the animal first to test objects for their association with reward, then, once rewarded objects were found, to reselect them on further trials for further rewards. Many cells in the frontal cortex showed an explore/exploit preference aligned with one-shot learning in the monkeys' behavior: the population switched from an explore state to an exploit state after a single trial of learning but partially maintained the explore state if an error indicated that learning had failed. Binary switch from explore to exploit was not explained by continuous changes linked to expectancy or prediction error. Explore/exploit preferences were independent for two stages of the trial: object selection and receipt of feedback. Within an established task structure, frontal activity may control the separate processes of explore and exploit, switching in one trial between the two.

Brain networks exist in a biophysical world. In this world, the brain as a whole needs to control and navigate its organism within a complex environment, while its constituent neurons must economically balance the resources they use to grow connections. To build and sustain connections, they need to overcome the strain caused by long distances in physical 3D and their own topological space. Due to being exposed to the same basic forces and hence optimization problem, many brains converge on similar features in their structure and function. To observe the effects of these basic forces on a network’s optimization process, we introduce the spatially-embedded RNN (seRNN). We find that seRNNs, due to existing in 3D Euclidean and topological space, naturally converge on solving a task using modular small-world networks in which functionally similar units cluster in space and utilise a mixed selective code. This shows (a) how fundamental, but seemingly unrelated, neuroscientific findings can be attributed to a network’s biophysical optimization process and (b) how spatially-embedded neural networks can serve as model systems to bridge between structural and functional research communities to move neuroscientific understanding forward.","Chelsea Finn, Ida Momennejad, Deepak Pathak",Danyal Akarca,108064006
2,1361,Gautam,Agarwal,gagarwal@kecksci.claremont.edu,Claremont Colleges,510-859-7606,Claremont,CA,91711,United States,"[Humans learning a complex task are picky and sticky]

While neural networks approach human levels of performance in many complex tasks, they require much more training than humans. This may be because only humans can infer and apply generalizable principles from prior experience (Lake, Ullman, Tenenbaum, & Gershman, 2017). However, the statistics that underlie the human learning process are poorly understood and hard to investigate in the large state spaces found in most complex tasks (van Opheusden & Ma, 2019). We thus designed a cognitive task whose potential solutions are few enough for subjects to densely sample policy space, but complex enough to compel intelligent search. We launched the game as a smartphone-based app (hexxed.io) to collect data from 10k human participants. We find that unlike reinforcement learning agents (Deep-Q Networks ; DQNs), humans (1) search a highly restricted subset of the policy space; (2) attempt even poor solutions many times before discarding them; (3) arrive at the optimal policy suddenly and unpredictably with a “leap of insight”. Our data suggest a “top-down” learning process by which humans propose explanatory solutions which they replace only upon collecting sufficient evidence to the contrary, in contrast to the “bottom-up” learning of DQNs that associates states with rewarding actions.",,"Dongrui Deng, Tiago Quendera",
3,1438,Seoyoung,Ahn,ahnseoyoung@gmail.com,Stony Brook University,5168301411,Stony Brook,New York,11790,United States,"[A brain-inspired object-based attention network for multi-object recognition and visual reasoning]
The visual system uses sequences of selective glimpses to objects to support behavioral goals, but how is this attention control learned? Here we present an encoder-decoder model inspired by the interacting bottom-up and top-down visual pathways making up the recognition-attention system in the brain. At every iteration, a new glimpse is taken from the image and is processed through the 9what9 encoder, a hierarchy of feedforward, recurrent, and capsule layers, to obtain an object-centric (object-file) representation. This representation feeds to the 9where9 decoder, where the evolving recurrent representation provides top-down attentional modulation to plan subsequent glimpses and impact routing in the encoder. We demonstrate how the attention mechanism significantly improves the accuracy of classifying highly overlapping digits. In a visual reasoning task requiring comparison of two objects, our model achieves near-perfect accuracy and significantly outperforms larger models in generalizing to unseen stimuli. Our work demonstrates the benefits of object-based attention mechanisms taking sequential glimpses of objects.

[Using object reconstruction as top-down attentional feedback yields a shape bias and robustness in object recognition]
Many theories of vision posit the existence of top-down inference in visual perception, but little is known about how this visual inference occurs in the brain and the role it plays in robust object recognition.  Here we built an iterative encoder-decoder network that generates an object reconstruction---a visualized prediction about the possible appearance of an object---and uses it as top-down attentional feedback to bias the feed-forward processing into forming one globally coherent object representation (e.g., shape). We tested this model using the challenging out-of-distribution digit recognition task, MNIST-C, where 15 different types of transformation and corruption are applied to handwritten digit images. The proposed model showed strong generalization performance against various image perturbations, on average outperforming all other models including feedfoward CNNs and other advanced models (e.g., adversarially trained networks). Our model is particularly robust to corruptions such as blur, noise, and occlusion, where shape perception plays an important role, consistent with our suggestion that an object reconstruction is used by top-down attention to impose a shape bias on the perception of an object in the visual input. 
","Paul Soulos, Jesse Breedlove, Ching Fang, Guillermo Horga, David Coggan, Choongwan Woo",,144078005
4,1394,Ismail,Akturk,ismail.akturk@ozyegin.edu.tr,Ozyegin University,+90-216-564-9875,İstanbul,,34794,Turkey,"[Exploiting Refractory Period for Functional Multiplexing and Short-Term Memory in Spiking Neural Networks]
Spiking Neural Networks (SNNs) have recently received attention in robotics due to their low power and efficiency prospects. However, we argue that existing implementations of SNNs don’t exploit the greater potential inherent to spiking neurons – particularly refractory period – that could enable functional multiplexing and short-term memory. We demonstrate how refractory period enables functional multiplexing and form a short-term memory in SNNs which would support complex functionalities, and learning methods with smaller number of neurons compared to traditional SNNs implementations that do not model the refractory period.

[Temporal-Rate Encoding to Realize Unary Positional Representation in Spiking Neural Systems]
Unary representation is straightforward, error tolerant and requires simple logic while its latency is a concern. On the other hand, positional representation (like binary) is compact and requires less space, but it is sensitive to errors. A hybrid representation called unary positional encoding reduces the latency of unary computation and length of the encoded stream, thus achieves the compactness of positional representation while preserving the error tolerance of unary encoding.
In this paper, we discuss the prospect of unary positional encoding in spiking neural systems by incorporating temporal and rate encoding.

[Predictive Hybrid Encoding for Energy-Efficient Spiking Neural Networks]
In the quest of achieving the level of energy efficiency of a biological brain to empower computing capabilities of next-generation systems, this project focuses on brain-inspired encoding schemes in Spiking Neural Networks (SNNs). In particular, we are inspired by the predictive and multimodal encoding of brain that minimizes the energy expenditure for communication, however, implications and adaptability of brain inspired encoding schemes on SNNs are poorly understood. Therefore, our objectives for this work are (i) to build a computational model that allows to evaluate the impact of predictive nature of brain in encoding, (ii) to quantify the effectiveness of envisioned predictive hybrid encoding scheme for different data types, and (iii) to determine the consequences/tradeoffs of encoding schemes in building SNNs, in terms of energy, performance, reliability, area and accuracy.


",,,40600553
5,1109,Andrea,Alamia,andrea.alamia@cnrs.fr,"CerCo, CNRS",+33-5-62-74-61-57,Toulouse,,31300,France,"[Alpha oscillations and traveling waves: Signatures of predictive coding?]
Predictive coding is a key mechanism to understand the computational processes underlying brain functioning: in a hierarchical network, higher levels predict the activity of lower levels, and the unexplained residuals (i.e., prediction errors) are passed back to higher layers. Because of its recursive nature, we wondered whether predictive coding could be related to brain oscillatory dynamics. First, we show that a simple 2-level predictive coding model of visual cortex, with physiological communication delays between levels, naturally gives rise to alpha-band rhythms, similar to experimental observations. Then, we demonstrate that a multilevel version of the same model can explain the occurrence of oscillatory traveling waves across levels, both forward (during visual stimulation) and backward (during rest). Remarkably, the predictions of our model are matched by the analysis of 2 independent electroencephalography (EEG) datasets, in which we observed oscillatory traveling waves in both directions.

",,,
6,1205,Kristijan,Armeni,karmeni1@jhu.edu,The Johns Hopkins University,410-670-1305,Baltimore,Maryland,21218,United States,"[A contextual encoding model for human ECoG responses to a spoken narrative]
Language understanding depends on context at multiple levels of linguistic granularity. How does this context-dependence differ across different cortical regions supporting language processing? Recently, it has been shown that electrophysiological responses to narrative stimuli can be predicted by contextualized word vector representations extracted from next-word prediction models. Here, we set out to apply and extend this approach within an electrocorticography (ECoG) dataset of 9 participants listening to a 7-minute narrative. For each word in the story, we predicted the neural response based on: (i) sensory features; (ii) non-contextualized word vectors; and contextualized word vectors with context scrambled at the word, sentence and paragraph levels. We show that contextualized embeddings, on average, are better predictors of broadband high-frequency (70+ Hz) power responses compared with non-contextualized
embeddings. Moreover, the improved encoding performance of contextualized embeddings specifically depended on the preceding context being provided intact to the model. These initial results provide the basis for mapping the timescale of context-dependence (word, sentence, and paragraph level) for each intracranial site across the cortical surface.

[Characterizing Verbatim Short-Term Memory in Neural Language Models]
When a language model is trained to predict natural language sequences, its prediction at each moment depends on a representation of prior context. What kind of information about the prior context can language models retrieve? We tested whether language models could retrieve the exact words that occurred previously in a text. In our paradigm, language models (transformers and LSTMs) processed English text in which a list of nouns occurred twice. We operationalized retrieval as the reduction in surprisal from the first to the second list. We found that the transformers retrieved both the identity and ordering of nouns from the first list. Further, the transformers' retrieval was markedly enhanced when they were trained on a larger corpus and with greater model depth. Lastly, their ability to index prior tokens was dependent on learned attention patterns. In contrast, LSTMs exhibited less precise retrieval, which was limited to list-initial tokens and to short intervening texts. The LSTMs' retrieval was not sensitive to the order of nouns and it improved when the list was semantically coherent. We conclude that transformers implement something akin to a working memory system that can flexibly retrieve individual token representations across arbitrary delays; conversely, LSTMs maintain a coarser and more rapidly-decaying semantic gist of prior tokens, weighted toward the earliest items.

[A 10-hour within-participant magnetoencephalography narrative dataset to test models of language comprehension]
Recently, cognitive neuroscientists have increasingly studied the brain responses to narratives. At the same time, we are witnessing exciting developments in natural language processing where large-scale neural network models can be used to instantiate cognitive hypotheses in narrative processing. Yet, they learn from text alone and we lack ways of incorporating biological constraints during training. To mitigate this gap, we provide a narrative comprehension magnetoencephalography (MEG) data resource that can be used to train neural network models directly on brain data. We recorded from 3 participants, 10 separate recording hour-long sessions each, while they listened to audiobooks in English. After story listening, participants answered short questions about their experience. To minimize head movement, the participants wore MEG-compatible head casts, which immobilized their head position during recording. We report a basic evoked-response analysis showing that the responses accurately localize to primary auditory areas. The responses are robust and conserved across 10 sessions for every participant. We also provide usage notes and briefly outline possible future uses of the resource.",,"Micha Heilbron, fMRI, decision-making, cognitive control, aging, ADHD, connectivity",
7,1100,Sarah,Ashcroft-Jones,sarah.ashcroft-jones@psy.ox.ac.uk,University of Oxford,+44-7478-323-793,Oxford,Oxfordshire,OX4 4QP,United Kingdom,"[Limitations of Proposed Signatures of Bayesian Confidence] The Bayesian model of confidence posits that confidence reflects the observer's posterior probability that the decision is correct. Hangya, Sanders, and Kepecs (2016) have proposed that researchers can test the Bayesian model by deriving qualitative signatures of Bayesian confidence (i.e., patterns that one would expect to see if an observer were Bayesian) and looking for those signatures in human or animal data. We examine two proposed signatures, showing that their derivations contain hidden assumptions that limit their applicability and that they are neither necessary nor sufficient conditions for Bayesian confidence. One signature is an average confidence of 0.75 on trials with neutral evidence. This signature holds only when class-conditioned stimulus distributions do not overlap and when internal noise is very low. Another signature is that as stimulus magnitude increases, confidence increases on correct trials but decreases on incorrect trials. This divergence signature holds only when stimulus distributions do not overlap or when noise is high. Navajas et al. (2017) have proposed an alternative form of this signature; we find no indication that this alternative form is expected under Bayesian confidence. Our observations give us pause about the usefulness of the qualitative signatures of Bayesian confidence. To determine the nature of the computations underlying confidence reports, there may be no shortcut to quantitative model comparison.
[Comparing Bayesian and non-Bayesian accounts of human confidence reports] Humans can meaningfully report their confidence in a perceptual or cognitive decision. It is widely believed that these reports reflect the Bayesian probability that the decision is correct, but this hypothesis has not been rigorously tested against non-Bayesian alternatives. We use two perceptual categorization tasks in which Bayesian confidence reporting requires subjects to take sensory uncertainty into account in a specific way. We find that subjects do take sensory uncertainty into account when reporting confidence, suggesting that brain areas involved in reporting confidence can access low-level representations of sensory uncertainty, a prerequisite of Bayesian inference. However, behavior is not fully consistent with the Bayesian hypothesis and is better described by simple heuristic models that use uncertainty in a non-Bayesian way. Both conclusions are robust to changes in the uncertainty manipulation, task, response modality, model comparison metric, and additional flexibility in the Bayesian model. Our results suggest that adhering to a rational account of confidence behavior may require incorporating implementational constraints.
[Response-time tests of logical-rule models of categorization.] A recent resurgence in logical-rule theories of categorization has motivated the development of a class of models that predict not only choice probabilities but also categorization response times (RTs; Fific, Little, & Nosofsky, 2010). The new models combine mental-architecture and random-walk approaches within an integrated framework and predict detailed RT-distribution data at the level of individual participants and individual stimuli. To date, however, tests of the models have been limited to validation tests in which participants were provided with explicit instructions to adopt particular processing strategies for implementing the rules. In the present research, we test conditions in which categories are learned via induction over training exemplars and in which participants are free to adopt whatever classification strategy they choose. In addition, we explore how variations in stimulus formats, involving either spatially separated or overlapping dimensions, influence processing modes in rule-based classification tasks. In conditions involving spatially separated dimensions, strong evidence is obtained for application of logical-rule strategies operating in a serial-self-terminating processing mode. In conditions involving spatially overlapping dimensions, preliminary evidence is obtained that a mixture of serial and parallel processing underlies the application of rule-based classification strategies. The logical-rule models fare considerably better than major extant alternative models in accounting for the categorization RTs.",,,
8,1122,Akram,Bakkour,bakkour@uchicago.edu,University of Chicago,401-338-2493,Chicago,IL,60637,United States,"[Decisions about what to eat recruit the orbitofrontal cortex (OFC) and involve the evaluation of food-related attributes such as taste and health. These attributes are used differently by healthy individuals and patients with disordered eating behavior, but it is unclear whether these attributes are decodable from activity in the OFC in both groups and whether neural representations of these attributes are differentially related to decisions about food. We used fMRI combined with behavioral tasks to investigate the representation of taste and health attributes in the human OFC and the role of these representations in food choices in healthy women and women with anorexia nervosa (AN). We found that subjective ratings of tastiness and healthiness could be decoded from patterns of activity in the OFC in both groups. However, health-related patterns of activity in the OFC were more related to the magnitude of choice preferences among patients with AN than healthy individuals. These findings suggest that maladaptive decision-making in AN is associated with more consideration of health information represented by the OFC during deliberation about what to eat.]
[While making decisions, we often rely on past experiences to guide our choices. However, not all experiences are remembered equally well, and some elements of an experience are more memorable than others. Thus, the intrinsic memorability of past experiences may bias our decisions. Here, we hypothesized that individuals would tend to choose more memorable options than less memorable ones. We investigated the effect of item memorability on choice in two experiments. First, using food images, we found that the same items were consistently remembered, and others consistently forgotten, across participants. However, contrary to our hypothesis, we found that participants did not prefer or choose the more memorable over the less memorable items when choice options were matched for the individuals’ valuation of the items. Second, we replicated these findings in an alternate stimulus domain, using words that described the same food items. These findings suggest that stimulus memorability does not play a significant role in determining choice based on subjective value.]
[Choosing between two items involves deliberation and comparison of the features of each item and its value. Such decisions take more time when choosing between options of similar value, possibly because these decisions require more evidence, but the mechanisms involved are not clear. We propose that the hippocampus supports deliberation about value, given its well-known role in prospection and relational cognition. We assessed the role of the hippocampus in deliberation in two experiments. First, using fMRI in healthy participants, we found that BOLD activity in the hippocampus increased as a function of deliberation time. Second, we found that patients with hippocampal damage exhibited more stochastic choices and longer reaction times than controls, possibly due to their failure to construct value-based or internal evidence during deliberation. Both sets of results were stronger in value-based decisions compared to perceptual decisions.]",,"Tom Schonberg, David Barack, Russell Poldrack, Daphna Shohamy, Michael Shadlen",1775048
9,1042,Domenic,Bersch,domenic.bersch@yahoo.de,Johann Wolfgang Goethe-Universität Frankfurt,+49-176-997-7326,Frankfurt am Main,Hessen,60629,Germany,"[Net2Brain: A Toolbox to compare artificial vision models with human brain responses]

We introduce Net2Brain, a graphical and command-line user interface toolbox for comparing the representational spaces of artificial deep neural networks (DNNs) and human brain recordings. While different toolboxes facilitate only single functionalities or only focus on a small subset of supervised image classification models, Net2Brain allows the extraction of activations of more than 600 DNNs trained to perform a diverse range of vision-related tasks (e.g semantic segmentation, depth estimation, action recognition, etc.), over both image and video datasets. The toolbox computes the representational dissimilarity matrices (RDMs) over those activations and compares them to brain recordings using representational similarity analysis (RSA), weighted RSA, both in specific ROIs and with searchlight search. In addition, it is possible to add a new data set of stimuli and brain recordings to the toolbox for evaluation. ",,,
10,1062,Caroline,Bévalot,bevalot.caroline@gmail.com,Inserm,+33-6-87-29-60-36,Paris,Île-de-France,75014,France,"[Dissociation Between The Use of Implicit and Explicit Priors in Bayesian Perceptual Inference]
Our brain constantly uses prior knowledge that reflects the statistics of our environment to shape our perception. Those statistics can be implicit, not directly observable but learned from observations, or explicit, communicated directly to the observer, especially in humans. Those different origins and mechanisms for acquiring priors may influence perception differently. Here, we manipulated the strength of priors and sensory likelihood to study perceptual inference in both implicit and explicit contexts. Using Bayesian models of learning and decision, we showed that subjects performed worse in the explicit than implicit context because they neglected more the sensory likelihood. The weight of the likelihood was highly correlated between contexts (but different on average) across individuals, but the weight of priors was unrelated. Those results support a dissociation in perceptual inference between the use of implicit vs. explicit priors. Many previous studies reported suboptimality of perceptual inference in healthy subjects or psychiatric disorders; those results could be reinterpreted in light of the implicit-explicit origin of priors.

[Psychotic Features Impact Perception Rather Than Learning]
The nascent field of computational psychiatry has used the framework of Bayesian inference to explain psychosis and aberrant perceptions like hallucinations. Recent studies have proposed that psychosis is characterized by an abnormal relative weight of priors over sensory likelihood, but their conclusions are sometimes conflicting. We propose to gain insight by disentangling the weights of the prior and the sensory likelihood (rather than using a relative weight) and disentangling the learning of priors from their use in a perceptual decision. Unlike previous studies, we can do so by combining a dedicated pair of categorization tasks (with and without learning of priors) and a computational model. We explored inter-individual differences in psychiatric features and both model-free and model-based analyses of behavior. Subjects’ perception was generally shaped by priors (be they learned or not) and sensory likelihood. However, subjects with more severe psychotic features exhibited a reduced weight of the sensory likelihood in their decision, both when priors are learned or explicitly given (multiple linear regression of the psychometric features on the logistic weights of likelihood. A computational analysis of learning dynamics showed that this reduced weight of the sensory likelihood did not alter the learning of priors and was specific to the decision stage (multiple linear regression of the psychometric features on the fitted parameters). Those results are consistent with previous studies and enable new interpretations to previously conflicting studies on psychosis.
","""Philipp Sterzer"", ""Megan Peters"", ""Katharina Schmack"", ""Yael Niv"", ""Quentin Huys"", ""Konrad Kording""",,
11,1114,Michael,Bonner,mfbonner@jhu.edu,The Johns Hopkins University,215-813-0985,Baltimore,MD,21218,United States,"[How much do we know about visual representations? \\Quantifying the dimensionality gap between DNNs and visual cortex]
Deep neural networks (DNNs) can explain a large portion of variance in image-evoked cortical responses by accounting for the highest variance latent dimensions in neural data, such as dimensions corresponding to animacy, aspect ratio, and curvature. However, there is a long tail of low-variance latent dimensions in image-evoked cortical responses that may nonetheless be critical to human vision. We wondered if these low-variance dimensions are meaningful and whether current DNNs are successful in explaining them. To answer these questions, we estimated the number of reliable dimensions in visual cortex in a large-scale human fMRI dataset and assessed how well DNNs performed at explaining these dimensions. We found that hundreds of dimensions contained reliable stimulus-relevant information. However, standard DNN encoding models explain a much smaller number of these dimensions\textemdash often an order of magnitude smaller than the number of reliable dimensions in the data. Our findings demonstrate the surprisingly low-dimensional nature of explained variance in computational models of visual cortex, and reveal the long-tail of complex, stimulus-relevant information in cortical responses that remains to be explained.
[Do We Need Deep Learning? Towards High-Performance Encoding Models of Visual Cortex Using Modules of Canonical Computations]
The field of computational neuroscience has witnessed a surge of interest in convolutional neural networks (CNNs) trained through deep learning, following the finding that they can recapitulate representations of visual information along the ventral stream. This has led to the routine use of CNNs as standard encoding models of visual cortex, despite limitations such as a large dependency on training data and low interpretability. Here, we propose an alternative approach that addresses such limitations without sacrificing performance. We introduce a family of hand-engineered models based on a module of convolution operations combined with a set of canonical neural computations, resulting in a high-performance model that requires little to no training. We present one such architecture and compare its encoding performance to a standard CNN by linearly mapping each model’s features to fMRI responses. We show that, with no learning involved, the performance of the hand-engineered model competes with the trained CNN for predicting object-evoked and scene-evoked fMRI responses in visual cortex. Our approach opens the possibility of designing high-performance encoding models without relying on deep learning, and it has promise for revealing critical inductive biases and computational efficiencies of visual cortex.
[High-performing neural network models of visual cortex benefit from high latent dimensionality]
Geometric descriptions of deep neural networks (DNNs) have the potential to uncover core principles of computational models in neuroscience, while abstracting over the details of model architectures and training paradigms. Here we examined the geometry of DNN models of visual cortex by quantifying the latent dimensionality of their natural image representations. The prevailing view holds that optimal DNNs compress their representations onto low-dimensional manifolds to achieve invariance and robustness, which suggests that better models of visual cortex should have low-dimensional geometries. Surprisingly, we found a strong trend in the opposite direction---neural networks with high-dimensional image manifolds tend to have better generalization performance when predicting cortical responses to held-out stimuli in both monkey electrophysiology and human fMRI data. These findings held across a diversity of design parameters for DNNs, and they suggest a general principle whereby high-dimensional geometry confers a striking benefit to DNN models of visual cortex.","Kendrick Kay, Tim Kietzmann, Thomas Naselaris, Ian Charest, SueYeon Chung, Blake Richards ","Russell Epstein, Chris Baker, Talia Konkle, Martin Hebart, Mark Lescroart, Michelle Greene, Leyla Isik",
12,1039,Tiffany,Bounmy,tbounmy@gmail.com,"NeuroSpin Center / CEA Paris-Saclay, Université de Paris",+33-6-46-75-86-57,Paris,,75013,France,"Meyniel & Dehaene, 2017
[Brain networks for confidence weighting and hierarchical inference during probabilistic learning]
Learning is difficult when the world fluctuates randomly and ceaselessly. Classical learning algorithms, such as the delta rule with constant learning rate, are not optimal. Mathematically, the optimal learning rule requires weighting prior knowledge and incoming evidence according to their respective reliabilities. This “confidence weighting” implies the maintenance of an accurate estimate of the reliability of what has been learned. Here, using fMRI and an ideal-observer analysis, we demonstrate that the brain’s learning algorithm relies on confidence weighting. While in the fMRI scanner, human adults attempted to learn the transition probabilities underlying an auditory or visual sequence, and reported their confidence in those estimates. They knew that these transition probabilities could change simultaneously at unpredicted moments, and therefore that the learning problem was inherently hierarchical. Subjective confidence reports tightly followed the predictions derived from the ideal observer. In particular, subjects managed to attach distinct levels of confidence to each learned transition probability, as required by Bayes-optimal inference. Distinct brain areas tracked the likelihood of new observations given current predictions, and the confidence in those predictions. Both signals were combined in the right inferior frontal gyrus, where they operated in agreement with the confidence-weighting model. This brain region also presented signatures of a hierarchical process that disentangles distinct sources of uncertainty. Together, our results provide evidence that the sense of confidence is an essential ingredient of probabilistic learning in the human brain, and that the right inferior frontal gyrus hosts a confidence-based statistical learning algorithm for auditory and visual sequences.

Behrens et al., 2007
[Learning the value of information in an uncertain world]
Our decisions are guided by outcomes that are associated with decisions made in the past. However, the amount of influence each past outcome has on our next decision remains unclear. To ensure optimal decision-making, the weight given to decision outcomes should reflect their salience in predicting future outcomes, and this salience should be modulated by the volatility of the reward environment. We show that human subjects assess volatility in an optimal manner and adjust decision-making accordingly. This optimal estimate of volatility is reflected in the fMRI signal in the anterior cingulate cortex (ACC) when each trial outcome is observed. When a new piece of information is witnessed, activity levels reflect its salience for predicting future outcomes. Furthermore, variations in this ACC signal across the population predict variations in subject learning rates. Our results provide a formal account of how we weigh our different experiences in guiding our future actions.

Geurts et al., 2022
[Subjective confidence reflects representation of Bayesian probability in cortex]
What gives rise to the human sense of confidence? Here we tested the Bayesian hypothesis that confidence is based on a probability distribution represented in neural population activity. We implemented several computational models of confidence and tested their predictions using psychophysics and functional magnetic resonance imaging. Using a generative model-based decoding technique, we extracted probability distributions from neural population activity in human visual cortex. We found that subjective confidence tracks the shape of the decoded distribution. That is, when sensory evidence was more precise, as indicated by the decoded distribution, observers reported higher levels of confidence. We furthermore found that neural activity in the insula, anterior cingulate and prefrontal cortex was linked to both the shape of the decoded distribution and reported confidence, in ways consistent with the Bayesian model. Altogether, our findings support recent statistical theories of confidence and suggest that probabilistic information guides the computation of one’s sense of confidence.",,"Florent Meyniel, Cédric Foucault, Caroline Bévalot",
13,1268,Jesse,Breedlove,jbreedlo@umn.edu,University of Minnesota,864-653-0082,Minneapolis,MN,55413,United States,"An fMRI account of non-optic sight in blindness
There is a growing appreciation that visual perception is not built from the bottom-up, relying solely on retinal input, but is constructed internally using the brain's rich knowledge about the world. A particularly compelling demonstration of this is non-optic sight in blindness. We present a case study of this phenomenon: 34-year-old NS who, after losing her sight to retinal degeneration, now “sees” objects she touches or hears. Unlike imagery, these representations are determinate, involuntary, and persist as long as she understands the object to be within her line-of-sight. We used 3T fMRI to record BOLD activity while NS placed objects on a plexiglass tray that held them suspended in her field-of-view. A GLM analysis found significant patterns of activation in her visual cortex that resembled the patterns of activation in a sighted control who viewed the same objects through typical retinal vision. The presence of increased activity in visual areas of NS’s brain while “seeing” but no longer touching the objects is consistent with her reported vision-like experience. These findings suggest that visual cortex can support vivid visual experiences that accurately interpret non-retinal sensory input.

With or without the retina: analyses of non-optic visual activity in the brain 
One way to investigate the contribution of cognition on activity in the visual cortex is to fix or remove the retinal input altogether. There are many such non-optic visual experiences to draw from (e.g., mental imagery, synesthesia,  hallucinations), all of which produce brain activity patterns consistent with the visual content of the experience. But how does the visual system manage to both accurately represent the external world and synthesize visual experiences? We approach this question by expanding on a theory that the human visual system embodies a probabilistic generative model of the visual world. We propose that retinal vision is just one form of inference that this internal model can support, and that activity in visual cortex observed in the absence of retinal stimulation can be interpreted as the most probable consequence unpacked from imagined, remembered, or otherwise assumed causes. When applied to mental imagery, this theory predicts that the encoding of imagined stimuli in low-level visual areas will resemble the encoding of seen stimuli in higher areas. We confirmed this prediction by estimating imagery encoding models from brain activity measured while subjects imagined complex visual stimuli accompanied by unchanging retinal input. In a different fMRI study, we investigated another far rarer form of non-optic vision: a case subject who, after losing their sight to retinal degeneration, now “sees” objects they touch or hear. The existence of this phenomenon further supports visual perception being a generative process that depends as much on top-down inference as on retinal input.
",,"Logan Dowdle, Thomas Naselaris, Ghislain St-Yves, Tiasha Saha Roy, Koustav Banerjee, Kendrick Kay ",https://www.semanticscholar.org/author/J.-Breedlove/2119508
14,1163,Matthias,Brucklacher,m.m.brucklacher@uva.nl,University of Amsterdam,+31-6-5178-9755,Amsterdam,Noord-Holland,1090 GE,Netherlands,"The ventral visual processing hierarchy of the cortex needs to fulfill at least two key functions: Perceived objects must be mapped to high-level representations invariantly of the precise viewing conditions, and a generative model must be learned that allows, for instance, to fill in occluded information guided by visual experience. Here, we show how a multilayered predictive coding network can learn to recognize objects from the bottom up and to generate specific representations via a top-down pathway through a single learning rule: the local minimization of prediction errors. Trained on sequences of continuously transformed objects, neurons in the highest network area become tuned to object identity invariant of precise position, comparable to inferotemporal neurons in macaques. Drawing on this, the dynamic properties of invariant object representations reproduce experimentally observed hierarchies of timescales from low to high levels of the ventral processing stream. The predicted faster decorrelation of error-neuron activity compared to representation neurons is of relevance for the experimental search for neural correlates of prediction errors. Lastly, the generative capacity of the network is confirmed by reconstructing specific object images, robust to partial occlusion of the inputs. By learning invariance from temporal continuity within a generative model, the network goes beyond purely input-reconstructing models of predictive coding and generalizes the framework to moving inputs in a more biologically plausible way than self-supervised networks with non-local error-backpropagation. [Local minimization of prediction errors drives learning of invariant object representations in a generative network model of visual perception]

The ability to decompose large tasks into smaller subtasks allows humans to solve complex problems step-by-step. To transfer this ability to an automated system, we propose a spiking neural network inspired by the neurobiological mechanics of spatial cognition to represent space on multiple levels of abstraction. As behavioral experiments suggest that humans integrate spatial knowledge in a graph of places, neurons in the state-action network encode locations while connections between them represent transition actions. In a series of simulation experiments, the influence of hierarchy on planning speed and on the resulting route choice in comparison to single-level models is investigated. We find that the model chooses biased subgoals in line with experiments on human navigation. [Hierarchical Planning in Multilayered State-Action Networks]","Kimberly Stachenfeld, Nathaniel Daw, Joshua Tenenbaum, ",Kwangjun Lee,https://www.semanticscholar.org/author/2142122415
15,1190,Sebastian,Bruijns,sebastian.bruijns@tuebingen.mpg.de,Max Planck Institute for Biological Cybernetics,+49-162-218-4781,Tübingen,Baden-Württemberg,72076,Germany,"[Exploring learning trajectories with infinite hidden Markov models]
Learning the contingencies of a complex experiment is no easy task for animals. Individuals learn in an idiosyncratic manner, revising their strategies multiple times as they are shaped, or shape themselves, and potentially ending up with different asymptotic strategies. This long-run learning is therefore a tantalizing target for the sort of quantitatively individualized characterization that sophisticated modelling can provide. However, any such model requires a flexible and extensible structure which can capture radically new behaviours as well as slow changes in existing ones. To this end, we suggest a dynamic input-output infinite hidden Markov model whose latent states are associated with specific behavioural patterns. This model includes a countably infinite number of potential states and so has the capacity for describing new behaviour by introducing states, while the dynamics in the model allow it to capture adaptations to existing behaviours. We fit this to data collected from mice as they learn a contrast detection task over multiple stages and around ten thousand trials each. We quantify different stages of learning via the number and psychometric characteristics of behavioural states. Our approach provides in-depth insight into the process of animal learning and offers potentially valuable predictors for analyzing neural data.",,Fabian Renz,
16,1123,Eivinas,Butkus,eb3407@columbia.edu,Columbia University,203-747-42425,New York,New York,10027,United States,"[Attentional dynamics during multiple object tracking are explained at subsecond resolution by a new 'hypothesis-driven adaptive computation' framework] A tremendous amount of work on visual attention has helped to characterize *what* we attend to, but has focused less on precisely *how* and *why* attention is allocated to dynamic scenes across time. Nowhere is this contrast more apparent than in multiple object tracking (MOT). Hundreds of papers have explored MOT as a paradigmatic example of selective attention, in part because it so well captures attention as a dynamic process. It is especially ironic, then, that nearly all of this work reduces each MOT trial to a single value (i.e. the number of targets successfully tracked) -- when in reality, each MOT trial presents an experiment unto itself, with constantly shifting attention over time. Here we seek to capture this dynamic ebb and flow of attention at a subsecond resolution, both empirically and computationally. Empirically, observers completed MOT trials during which they also had to detect sporadic momentary probes, as a measure of the moment-by-moment degree of attention being allocated to each object. Computationally, we characterize (for the first time, to our knowledge) an algorithmic architecture of just how and why such dynamic attentional shifts occur. To do so, we introduce a new 'hypothesis driven adaptive computation' model. Whereas previous models employed many MOT-specific assumptions, this new approach generalizes to any task-driven context. It provides a unified account of attention as the dynamic allocation of computing resources, based on task-driven hypotheses about the properties (e.g. location, target status) of each object. Here, this framework was able to explain the observed probe detection performance measured at a subsecond resolution, independent of general spatial factors (such as the proximity of each probe to the MOT targets' centroid). This case study provides a new way to think about attention and how it interfaces with perception in terms of rational resource allocation.

[Modeling temporal attention in dynamic scenes: Hypothesis-driven resource allocation using adaptive computation explains both objective tracking performance and subjective effort judgments]
Most work on attention (in terms of both psychophysical experiments and computational modeling) involves selection in static scenes. And even when dynamic displays are used, performance is still typically characterized with only a single variable (such as the number of items correctly tracked in Multiple Object Tracking; MOT). But the allocation of attention in daily life (e.g. during foraging, navigation, or play) involves both objective performance and subjective effort, and both can vary dramatically from moment to moment. Here we attempt to capture this sort of rich temporal ebb and flow of attention in a novel and generalizable adaptive computation architecture. Compute is allocated across both objects (in space) and moments (in time) based on hypothesis-driven inferences that are continuously refined, and during MOT this framework is able to explain both object tracking performance and the subjective sense of trial-by-trial effort.","Samuel Gershman, Joshua Tenenbaum, Tom Griffiths, Tomer Ullman, Nathaniel Daw, Kimberly Stachenfeld","Nikolaus Kriegeskorte, Todd Gureckis",2047737001
17,1049,Catherine,Chen,cathychen@berkeley.edu,"University of California, Berkeley",610-573-3159,Berkeley,California,94703,United States,"[The Cortical Representation of Language Timescales is Shared between Reading and Listening]
Evidence from functional neuroimaging experiments suggests that many cortical regions are activated in response to both written and spoken language. Prior work has shown that in these cortical regions spoken language is integrated across different timescales, but it is unclear whether language components at different timescales are represented in the same way for both written and spoken language. To address this question, we re-analyzed fMRI data that was recorded while participants read and listened to the same set of natural language narratives in each modality \citep{huth2016, deniz2019}. We transformed the stimulus narratives into feature spaces that each reflect a particular set of language timescales. These timescale-specific feature spaces were then used to estimate voxelwise models that describe how different timescales of language are represented in the brain for each modality and participant separately. Comparisons of timescale selectivity between reading and listening show that the cortical organization of timescale selectivity is highly similar between the two modalities. These results suggest that the human cortex contains a hierarchy of areas that each represent particular language timescales, and that this hierarchy is largely independent from stimulus modality.

[Constructing Taxonomies from Pretrained Language Models]
We present a method for constructing taxonomic trees (e.g., WordNet) using pretrained language models. Our approach is composed of two modules, one that predicts parenthood relations and another that reconciles those pairwise predictions into trees. The parenthood prediction module produces likelihood scores for each potential parent-child pair, creating a graph of parent-child relation scores. The tree reconciliation module treats the task as a graph optimization problem and outputs the maximum spanning tree of this graph. We train our model on subtrees sampled from WordNet, and test on nonoverlapping WordNet subtrees. We show that incorporating web-retrieved glosses can further improve performance. On the task of constructing subtrees of English WordNet, the model achieves 66.7 ancestor F1, a 20.0% relative increase over the previous best published result on this task. In addition, we convert the original English dataset into nine other languages using Open Multilingual WordNet and extend our results across these languages.

[Learning to perform role-filler binding with schematic knowledge]
Through specific experiences, humans learn the relationships that underlie the structure of events in the world. Schema theory suggests that we organize this information in mental frameworks called “schemata,” which represent our knowledge of the structure of the world. Generalizing knowledge of structural relationships to new situations requires role-filler binding, the ability to associate specific “fillers” with abstract “roles.” For instance, when we hear the sentence Alice ordered a tea from Bob, the role-filler bindings customer:Alice, drink:tea and barista:Bob allow us to understand and make inferences about the sentence. We can perform these bindings for arbitrary fillers—we understand this sentence even if we have never heard the names Alice, tea, or Bob before. In this work, we define a model as capable of performing role-filler binding if it can recall arbitrary fillers corresponding to a specified role, even when these pairings violate correlations seen during training. Previous work found that models can learn this ability when explicitly told what the roles and fillers are, or when given fillers seen during training. We show that networks with external memory learn to bind roles to arbitrary fillers, without explicitly labeled role-filler pairs. We further show that they can perform these bindings on role-filler pairs that violate correlations seen during training, while retaining knowledge of training correlations. We apply analyses inspired by neural decoding to interpret what the networks have learned.",,,2109063330
18,1108,Bhavin,Choksi,bhavin.choksi@cnrs.fr,"CerCo, CNRS",+33-7-52-48-84-20,Toulouse,,31300,France,"[Predify: Augmenting deep neural networks with brain-inspired predictive coding dynamics]
Deep neural networks excel at image classification, but their performance is far less robust to input perturbations than human perception. In this work we explore whether this shortcoming may be partly addressed by incorporating brain-inspired recurrent dynamics in deep convolutional networks. We take inspiration from a popular framework in neuroscience: 'predictive coding'. At each layer of the hierarchical model, generative feedback 'predicts' (i.e., reconstructs) the pattern of activity in the previous layer. The reconstruction errors are used to iteratively update the network's representations across timesteps, and to optimize the network's feedback weights over the natural image dataset-a form of unsupervised training. We show that implementing this strategy into two popular networks, VGG16 and EfficientNetB0, improves their robustness against various corruptions and adversarial attacks. We hypothesize that other feedforward networks could similarly benefit from the proposed framework. To promote research in this direction, we provide an open-sourced PyTorch-based package called Predify, which can be used to implement and investigate the impacts of the predictive coding dynamics in any convolutional neural network.

[Multimodal neural networks better explain multivoxel patterns in the hippocampus]
The human hippocampus possesses ""concept cells"", neurons that fire when presented with stimuli belonging to a specific concept, regardless of the modality. Recently, similar concept cells were discovered in a multimodal network called CLIP (Radford et at., 2021). Here, we ask whether CLIP can explain the fMRI activity of the human hippocampus better than a purely visual (or linguistic) model. We extend our analysis to a range of publicly available uni- and multi-modal models. We demonstrate that ""multimodality"" stands out as a key component when assessing the ability of a network to explain the multivoxel activity in the hippocampus.

[Predictive coding feedback results in perceived illusory contours in a recurrent neural network]
Modern feedforward convolutional neural networks (CNNs) can now solve some computer vision tasks at super-human levels. However, these networks only roughly mimic human visual perception. One difference from human vision is that they do not appear to perceive illusory contours (e.g. Kanizsa squares) in the same way humans do. Physiological evidence from visual cortex suggests that the perception of illusory contours could involve feedback connections. Would recurrent feedback neural networks perceive illusory contours like humans? In this work we equip a deep feedforward convolutional network with brain-inspired recurrent dynamics. The network was first pretrained with an unsupervised reconstruction objective on a natural image dataset, to expose it to natural object contour statistics. Then, a classification decision head was added and the model was finetuned on a form discrimination task: squares vs. randomly oriented inducer shapes (no illusory contour). Finally, the model was tested with the unfamiliar “illusory contour” configuration: inducer shapes oriented to form an illusory square. Compared with feedforward baselines, the iterative “predictive coding” feedback resulted in more illusory contours being classified as physical squares. The perception of the illusory contour was measurable in the luminance profile of the image reconstructions produced by the model, demonstrating that the model really “sees” the illusion. Ablation studies revealed that natural image pretraining and feedback error correction are both critical to the perception of the illusion. Finally we validated our conclusions in a deeper network (VGG): adding the same predictive coding feedback dynamics again leads to the perception of illusory contours.
","Kendrick Kay, Marcel Van Gerven, Thomas Naselaris, Timothy Lillicrap, Jean-Remy King, Alexander Ecker","Andrea Alamia, Sabine Muzellec",93165011
19,1374,Colin,Conwell,conwell@g.harvard.edu,Harvard University,610-331-9662,Cambridge,Ma,02138,United States,"[The Perceptual Primacy of Feeling:Affectless machine vision models robustly predict human visual arousal, valence, and aesthetics]

Looking at the world often involves not only seeing things but feeling things as well:a coupling of perception and affect that has for the most part eluded a concrete, computationally-grounded characterization. Modern feedforward machine vision systems that learn to perceive the world in the absence of active behavior, higher order thought, or any form of feedback that resembles human affective experience, offer compelling tools to demystify this relationship, and to assess how much of visually evoked affect may simply be a function of representation learning over natural image statistics. In this work, we deploy a large and diverse sample of 180 deep neural networks trained only on canonical computer vision tasks to predict human ratings of arousal, valence, and aesthetics for images from multiple categories (objects, faces, landscapes, art) across two distinct datasets. Importantly, we  use  the  features  of  these  models  without  any  additional  learning,  linearly decoding human affective responses from network activity in the same way we decode information from neuroimaging signals.  We find the features of purely perceptual models sufficient to predict average ratings of affect with remarkably high accuracy across the board – in many cases beyond the predictions we would make based on the responses of the most representative (’taste-typical’) human subjects. The top purely perceptual model in our survey explains roughly 73% of the explainable variance in human responses. In addition to assessing the overall robustness of this decoding approach on individual subjects and image categories, we  leverage  the  diversity  of  models  in  our  survey  to  triangulate  a  number  of properties that make certain feature spaces more predictive of affect than others:(1) Training: Randomly-initialized models categorically fail to predict the same quantities of variance as trained models, but trained category-supervised models are no more accurate than trained self-supervised models.  (2) Depth: In almost all models, there is a linear, monotonic increase in predictive accuracy from the earliest layers of the network to the deepest. (3) Language, maybe: Hybrid vision-language models like CLIP and SLIP slightly outperform purely perceptual models(by margins of 10-15%), but nuisance factors (like differences in visual diet) leave somewhat unclear whether these gains are attributable explicitly to language per se.  Taken together, these results provide strong computational evidence for an information-processing account of visually evoked affect linked directly to efficient representation learning over natural image statistics, and hint at a primary locus of affective and aesthetic experience directly proximate to perception.

[On the use of Cortical Magnification and Saccades as Biological Proxies for Data Augmentation]

Self-supervised learning is a powerful way to learn useful representations from natural data. It has also been suggested as one possible means of building visual representation in humans, but the specific objective and algorithm are unknown. Currently, most self-supervised methods encourage the system to learn an invariant representation of different transformations of the same image in contrast to those of other images. However, such transformations are generally non-biologically plausible, and often consist of contrived perceptual schemes such as random cropping and color jittering. In this paper, we attempt to reverse-engineer these augmentations to be more biologically or perceptually plausible while still conferring the same benefits for encouraging robust representation. Critically, we find that random cropping can be substituted by cortical magnification, and saccade-like sampling of the image could also assist the representation learning. The feasibility of these transformations suggests a potential way that biological visual systems could implement self-supervision. Further, they break the widely accepted spatially-uniform processing assumption used in many computer vision algorithms, suggesting a role for spatially-adaptive computation in humans and machines alike.

[Large-Scale Benchmarking of Diverse Artificial Vision Models in Prediction of 7T Human Neuroimaging Data]

Rapid simultaneous advances in machine vision and cognitive neuroimaging present an unparalleled opportunity to (re)assess the current state of artificial models of the human visual system. Here, we perform a large-scale benchmarking analysis of 85 modern deep neural network models (e.g. CLIP, BarlowTwins, Mask-RCNN) to characterize with robust statistical power how differences in architecture and training task contribute to the prediction of human fMRI activity across 16 distinct regions of the human visual system. We find: one, that even stark architectural differences (e.g. the absence of convolution in transformers and MLP-mixers) have very little consequence in emergent fits to brain data; two, that differences in task have clear effects--with categorization and self-supervised models showing relatively stronger brain predictivity across the board; three, that feature reweighting leads to substantial improvements in brain predictivity, without overfitting -- yielding model-to-brain regression weights that generalize at the same level of predictivity to brain responses over 1000s of new images. Broadly, this work presents a lay-of-the-land for the emergent correspondences between the feature spaces of modern deep neural network models and the representational structure inherent to the human visual system.",,,117698162
20,1439,Garrison,Cottrell,gary@ucsd.edu,UCSD,619-823-3033,La Jolla,CA,92093-0404,United States,"[The face inversion effect and the anatomical mapping from the visual field to the primary visual cortex] 
The face-inversion effect, or the drastic decrease in accuracy seen when a participant is asked to identify inverted faces when compared to upright faces, is an effect that is not found in object inversion. Here we suggest a new explanation of this effect using computational models to show that the phenomenon can be explained by the anatomical mapping from the visual field to primary visual cortex. We propose that the way inverted faces are mapped onto the cortex is fundamentally different from the way upright faces are mapped. Our work first shows the advantages of this mapping due to its scale and rotation invariance when used as input to a convolutional neural network. We train the network to perform recognition tasks and show it exhibits scale and realistically constrained rotation invariance. We then confirm that the decline in accuracy seen when a participant is asked to identify inverted faces is not seen in the network with inverted object recognition tasks. With the support of these two findings, we test the face-inversion effect on our network and are able to show the unique decline in accuracy, suggesting that the way the visual field is mapped onto the primary visual cortex is a key facet in the manifestation of this effect.
[Visual Expertise and the Log-Polar Transform Explain Image Inversion Effects]
Subjects perform poorly at recognizing upside-down faces. This face-inversion effect is in contrast to subjects’ performance with inverted objects, which is not as drastically impaired. Experimental results have suggested that a similar effect, though to a lesser degree, may be seen in the inversion of mono-oriented objects, such as cars, where subjects’ performance on inverted mono-oriented objects is between that of faces and other objects. We build an anatomically-inspired neurocomputational model to explore this effect. Our model includes a foveated retina and the log-polar mapping from the visual field to V1. This transformation causes changes in scale to appear as horizontal translations, leading to scale equivariance. Rotation is similarly equivariant, leading to vertical translations. When fed into a standard convnet, this provides rotation and scale invariance. It may be surprising that a rotation-invariant network shows any inversion effect at all. This is because there is a crucial topological difference between scale and rotation: Rotational invariance is discontinuous, with V1 ranging from 90 degrees (vertically up) to 270 degrees (vertically down). Hence when a face is inverted, the configural information in the face is disrupted while feature information is relatively unaffected. We show that the inversion effect arises as a result of visual expertise, where configural information becomes relevant as more objects are learned at the subordinate level. Our model matches the classic result: faces suffer more from inversion than mono-oriented objects and are more disrupted than non-mono-oriented objects.","Nikolaus Kriegeskorte, Daniel Yamins, James DiCarlo, Bruno Olshausen, David Noelle, Randy O'Reilly",,48524582
21,1362,Dongrui,Deng,dongruideng@rice.edu,Rice University,347-805-1382,Houston,Texas,77005,United States,"[Humans learning a complex task are picky and sticky]

While neural networks approach human levels of performance in many complex tasks, they require much more training than humans. This may be because only humans can infer and apply generalizable principles from prior experience (Lake, Ullman, Tenenbaum, & Gershman, 2017). However, the statistics that underlie the human learning process are poorly understood and hard to investigate in the large state spaces found in most complex tasks (van Opheusden & Ma, 2019). We thus designed a cognitive task whose potential solutions are few enough for subjects to densely sample policy space, but complex enough to compel intelligent search. We launched the game as a smartphone-based app (hexxed.io) to collect data from 10k human participants. We find that unlike reinforcement learning agents (Deep-Q Networks ; DQNs), humans (1) search a highly restricted subset of the policy space; (2) attempt even poor solutions many times before discarding them; (3) arrive at the optimal policy suddenly and unpredictably with a “leap of insight”. Our data suggest a “top-down” learning process by which humans propose explanatory solutions which they replace only upon collecting sufficient evidence to the contrary, in contrast to the “bottom-up” learning of DQNs that associates states with rewarding actions.",,"Gautam Agarwal, Tiago Quendera",
22,1239,Logan,Dowdle,logan.dowdle@gmail.com,Center for Magnetic Resonance Research,843-901-0804,Minneapolis,MN,55413,United States,"[Clarifying the role of higher-level cortices in resolving perceptual ambiguity using ultra high field fMRI] The brain is organized into distinct, flexible networks. Within these networks, cognitive variables such as attention can modulate sensory representations in accordance with moment-to-moment behavioral requirements. These modulations can be studied by varying task demands; however, the tasks employed are often incongruent with the postulated functions of a sensory system, limiting the characterization of the system in relation to natural behaviors. Here we combine domain-specific task manipulations and ultra-high field fMRI to study the nature of top-down modulations. We exploited faces, a visual category underpinned by a complex cortical network, and instructed participants to perform either a stimulus-relevant/domain-specific or a stimulus-irrelevant task in the scanner. We found that 1. perceptual ambiguity (i.e. difficulty of achieving a stable percept) is encoded in top-down modulations from higher-level cortices; 2. the right inferior-temporal lobe is active under challenging conditions and uniquely encodes trial-by-trial variability in face perception.
[Statistical power or more precise insights into neuro-temporal dynamics? Assessing the benefits of rapid temporal sampling in fMRI] Functional magnetic resonance imaging (fMRI), a non-invasive and widely used human neuroimaging method, is most known for its spatial precision. However, there is a growing interest in its temporal sensitivity. This is despite the temporal blurring of neuronal events by the blood oxygen level dependent (BOLD) signal, the peak of which lags neuronal firing by 4–6 seconds. Given this, the goal of this review is to answer a seemingly simple question – “What are the benefits of increased temporal sampling for fMRI?”. To answer this, we have combined fMRI data collected at multiple temporal scales, from 323 to 1000 milliseconds, with a review of both historical and contemporary temporal literature. After a brief discussion of technological developments that have rekindled interest in temporal research, we next consider the potential statistical and methodological benefits. Most importantly, we explore how fast fMRI can uncover previously unobserved neuro-temporal dynamics – effects that are entirely missed when sampling at conventional 1 to 2 second rates. With the intrinsic link between space and time in fMRI, this temporal renaissance also delivers improvements in spatial precision. Far from producing only statistical gains, the array of benefits suggest that the continued temporal work is worth the effort.
[Improving Sensitivity to Functional Responses without a Loss of Spatiotemporal Precision in Human Brain Imaging] As the neuroimaging field moves towards detecting smaller effects at higher spatial resolutions, and faster sampling rates, there is increased attention given to the deleterious contribution of unstructured, thermal noise. Here, we critically evaluate the performance of a recently developed reconstruction method, termed NORDIC, for suppressing thermal noise using datasets acquired with various field strengths, voxel sizes, sampling rates, and task designs.
Following minimal preprocessing, statistical activation (t-values) of NORDIC processed data was compared to the results obtained with alternative denoising methods. Additionally, we examined the consistency of unbiased estimations of task responses at the single-voxel, single run level, using a finite impulse response model. To examine the potential impact on effective image resolution, the overall smoothness of the data processed with different methods was estimated. Finally, to determine if NORDIC alters or removes important temporal information, we employed an exhaustive leave-p-out cross validation approach, using unbiased task responses to predict held out timeseries, quantified using R2.
After NORDIC, the t-values are increased, an improvement comparable to what could be achieved by 1.5 voxels smoothing, and task events are clearly visible and have less cross-run error. These advantages are achieved without detecting a significant compromise in spatial and temporal resolution. Cross-validated R2s based on the unbiased models show that NORDIC is not distorting the temporal structure of the data and is the best predictor of non-denoised time courses. The results demonstrate that 1 run of NORDIC data is equivalent to using 2 to 3 original runs and performs equally well across a diverse array of functional imaging protocols.",,"Jesse Breedlove, Thomas Naselaris, Kendrick Kay, Ghislaine St. Ives, Maggie Mae Mell, ",3922347
23,1343,William,Dudley,william.dudley19@imperial.ac.uk,Imperial College London,+44-7479-634-634,London,,SW10 9NQ,United Kingdom,"[The role of haptic communication in dyadic collaborative object manipulation tasks]
ntuitive and efficient physical human-robot col-
laboration relies on the mutual observability of the human and
the robot, i.e. the two entities being able to interpret each
other’s intentions and actions. This is remedied by a myriad
of methods involving human sensing or intention decoding, as
well as human-robot turn-taking and sequential task planning.
However, the physical interaction establishes a rich channel of
communication through forces, torques and haptics in general,
which is often overlooked in industrial implementations of
human-robot interaction. In this work, we investigate the role of
haptics in human collaborative physical tasks, to identify how
to integrate physical communication in human-robot teams.
We present a task to balance a ball at a target position on
a board either bimanually by one participant, or dyadically
by two participants, with and without haptic information. The
task requires that the two sides coordinate with each other,
in real-time, to balance the ball at the target. We found that
with training the completion time and number of velocity peaks
of the ball decreased, and that participants gradually became
consistent in their braking strategy. Moreover we found that
the presence of haptic information improved the performance
(decreased completion time) and led to an increase in overall
cooperative movements. Overall, our results show that humans
can better coordinate with one another when haptic feedback
is available. These results also highlight the likely importance
of haptic communication in human-robot physical interaction,
both as a tool to infer human intentions and to make the robot
behaviour interpretable to humans.



[Real-World Human-Robot Collaborative Reinforcement Learning]
The intuitive collaboration of humans and intel-
ligent robots (embodied AI) in the real-world is an essential
objective for many desirable applications of robotics. Whilst
there is much research regarding explicit communication, we
focus on how humans and robots interact implicitly, on motor
adaptation level. We present a real-world setup of a human-
robot collaborative maze game, designed to be non-trivial and
only solvable through collaboration, by limiting the actions to
rotations of two orthogonal axes, and assigning each axes to one
player. This results in neither the human nor the agent being
able to solve the game on their own. We use deep reinforcement
learning for the control of the robotic agent, and achieve results
within 30 minutes of real-world play, without any type of
pre-training. We then use this setup to perform systematic
experiments on human/agent behaviour and adaptation when
co-learning a policy for the collaborative game. We present
results on how co-policy learning occurs over time between the
human and the robotic agent resulting in each participant’s
agent serving as a representation of how they would play the
game. This allows us to relate a person’s success when playing
with different agents than their own, by comparing the policy
of the agent with that of their own agent.",,,
24,1260,Elizabeth,DuPre,emdupre@stanford.edu,Stanford University,650-725-2400,Stanford,CA,94305,United States,"[An empirical evaluation of functional alignment using inter-subject decoding ]
Inter-individual variability in the functional organization of the brain presents a major obstacle to identifying generalizable neural coding principles. Functional alignment-a class of methods that matches subjects' neural signals based on their functional similarity-is a promising strategy for addressing this variability. To date, however, a range of functional alignment methods have been proposed and their relative performance is still unclear. In this work, we benchmark five functional alignment methods for inter-subject decoding on four publicly available datasets. Specifically, we consider three existing methods: piecewise Procrustes, searchlight Procrustes, and piecewise Optimal Transport. We also introduce and benchmark two new extensions of functional alignment methods: piecewise Shared Response Modelling (SRM), and intra-subject alignment. We find that functional alignment generally improves inter-subject decoding accuracy though the best performing method depends on the research context. Specifically, SRM and Optimal Transport perform well at both the region-of-interest level of analysis as well as at the whole-brain scale when aggregated through a piecewise scheme. We also benchmark the computational efficiency of each of the surveyed methods, providing insight into their usability and scalability. Taking inter-subject decoding accuracy as a quantification of inter-subject similarity, our results support the use of functional alignment to improve inter-subject comparisons in the face of variable structure-function organization. We provide open implementations of all methods used. 

[When is functional alignment useful? Examining the impact of experimental context]
Individual differences in cortical anatomy and organization challenge group-level inferences in human brain mapping.
Standard neuroimaging reference spaces---such as MNI space---are designed to provide a common coordinate system for mapping structural and functional characteristics across individual brains.
In doing so, they necessarily represent a consensus or average anatomy across many participants.
In areas where participants may have variable anatomical features (e.g., duplication of Heschel's gyrus; \citealt{Marie2015-gh}), however, such an average anatomy may not map accurately to any individual subject.
Thus, there is no ideal correspondence in these regions between individual anatomy and a reference template.
In our previous work \citet{Bazeille2021-dd}, we benchmarked four of these algorithms to align individual pairs of subjects across a range of publicly available datasets.
While algorithm choice outlines the kinds of transformations that can be learned and therefore substantially impacts derived results, experimental context constrains the kinds of data available for alignment transformations.
In particular, the relationship between the data in which the alignment is learned and the data to which the alignment is applied---as well as the brain region in which this data is evaluated---all are likely to significantly influence the success of functional alignment.
Here, we extend on our previous results to better capture the performance of functional alignment in cognitive neuroscience applications.
We consider how a single functional alignment algorithm interacts with a range of experimental factors to assess their relative impacts.",,Russ Poldrack,
25,1412,Celia,Durkin,celia.durkin@gmail.com,Columbia University,3106507367,New York,NY,10025,United States,"
It is widely assumed that our experience of the world consists of mental representations that vary across individuals. However, while a great deal of  research quantifies shared neural and behavioral responses across individuals, many open questions remain about how you and I can see the same thing and interpret it differently.  To address this question, we focus on the experience of abstract art, a context in which interpretations of the same thing are allowed – and even encouraged – to vary.  We use fMRI and written descriptions to compare neural and cognitive representations evoked by experiencing abstract art with figurative art, which grounds us in shared representations of the world.  We used analysis techniques that quantify cross-subject dissimilarity in neural representations to quantify individual differences in the subjective experience of a painting and to determine where in the brain such differences are found.  As expected, we find that abstract paintings elicit more cross-subject dissimilarity than figurative paintings, and that these neural representations, while similar in low level visual areas, vary in areas responsible for more abstract cognitive processes (such as narrative interpretation).  Parallel findings were observed with semantic data (written captions), which also vary across subjects for abstract paintings.  These results give insight into a uniquely human experience of how we process abstract art and provide neural evidence of individual differences in subjective experience. ",,,
26,1341,Eduardo,E Sandoval,esandoval@berkeley.edu,"University of California, Berkeley",516-474-0302,Berkeley,CA,94710,United States,"Abstract: Probing Temporal Integration and Separation Behaviorally
Temporal Integration and Separation underlies our ability to generate memories. Previously, fMRI has demonstrated using RSA that higher order brain regions rely on greater time scales than lower-order sensory regions using narrative stimuli. Does this extend to linguistic stimuli, or other shorter forms of stimuli? Here, we developed a psychological experiment that aims to use ambiguous sensory and linguistic stimuli and extended pauses to influence participants' answers on comprehension questions. With our validated behavioral results, we will collect intra-cranial data and investigate whether syntactic phrases and short patterns demonstrate the same pattern of integration and separation that longer narrative stories seem to exhibit at event boundaries.

Abstract: Spiking Neural Network with STDP can recreate V1 simple cell receptive fields.
Implementation of spiking neural networks using generative rules, local inhibition for sparse coding, and an adaptive threshold using event-based STDP rules. Currently investigating temporal response properties of this network.

Abstract: Neural correlates of Groove
Looking at neural correlates of harmonic and melodic complexity, and perception of groove in a music listening task.",,"Dyana Muller, Jingjing Li, Jen Holmberg, Tyler Toueg, David Quiroga, Leyao Yu",
27,1046,Tahra,Eissa,tahra.eissa@colorado.edu,"University of Colorado, Boulder",310-994-2025,Boulder,Colorado,80309,United States,"[Learning efficient representations of heterogeneity in attractors for working memory]

Natural stimulus distributions often exhibit biases that we may learn to predict what we will observe next. Correspondingly, humans performing delayed estimation tasks are biased in favor of more commonly represented stimuli (e.g., common colors in visual tasks). These systematic biases may emerge from attractors in neural ensemble activity representing memoranda during delay periods. However, it is unclear how these heterogeneous representations are learned and how they may be represented in the brain. Here, we present a recurrently coupled neural network model that can learn heterogeneous input distributions via a combination of long term potentiation and homeostatic plasticity, generating an underlying potential landscape that governs the preferred positions of a bump attractor on a ring. The resulting spatially-heterogeneous synaptic network probabilistically represents the experienced stimulus history of the environment. We validate this theory using recently published response data from a delayed estimation task in which the distribution of stimuli was heterogeneous. Our results suggest that models that learn heterogeneities in the stimulus distribution explain the data more often than models for which connectivity is uniform throughout the duration of the experiment

[Suboptimal human inference inverts the bias-variance trade-off for decisions with asymmetric evidence]

Solutions to challenging inference problems are often subject to a fundamental trade-off between bias (being systematically wrong) that is minimized with complex inference strategies and variance (being oversensitive to uncertain observations) that is minimized with simple inference strategies. However, this trade-off is based on the assumption that the strategies being considered are optimal for their given complexity and thus has unclear relevance to the frequently suboptimal inference strategies used by humans. We examined inference problems involving rare, asymmetrically available evidence, which a large population of human subjects solved using a diverse set of strategies that were suboptimal relative to the Bayesian ideal observer. These suboptimal strategies reflected an inversion of the classic bias-variance trade-off: subjects who used more complex, but imperfect, Bayesian-like strategies tended to have lower variance but high bias because of incorrect tuning to latent task features, whereas subjects who used simpler heuristic strategies tended to have higher variance because they operated more directly on the observed samples but displayed weaker, near-normative bias. Our results yield new insights into the principles that govern individual differences in behavior that depends on rare-event inference, and, more generally, about the information-processing trade-offs that are sensitive to not just the complexity, but also the optimality of the inference process",,,
28,1098,Mengting,Fang,mtfang@sas.upenn.edu,University of Pennsylvania,646-248-0703,Philadelphia,Pennsylvania,19104,United States,"[Angular gyrus responses show joint statistical dependence with brain regions selective for different categories]
Category-selectivity is a fundamental principle of organi- zation of perceptual brain regions. Human occipitotem- poral cortex is subdivided into areas that respond prefer- entially to faces, bodies, artifacts, and scenes. However, observers need to combine information about objects from different categories to form a coherent understand- ing of the world. How is this multi-category information encoded in the brain? Studying the multivariate interac- tions between brain regions with fMRI and artificial neural networks, we found that the angular gyrus shows joint statistical dependence with multiple category-selective regions. Additional analyses revealed a cortical map of areas that encode information across different subsets of categories, indicating that multi-category information is not encoded in a single stage at a centralized location, but in multiple distinct brain regions.

[Can a recurrent neural network learn to count things?]
We explore a recurrent neural network model of counting based on the differentiable recurrent attentional model of Gregor et al.(2015). Our results reveal that the model can learn to count the number of items in a display, pointing to each of the items in turn and producing the next item in the count sequence at each step, then saying ‘done’when there are no more blobs to count. The model thus demonstrates that the ability to learn to count does not depend on special knowledge relevant to the counting task. We find that the model’s ability to count depends on how well it has learned to point to each successive item in the array, underscoring the importance of coordination of the visuospatial act of pointing with the recitation of the count list. The model learns to count items in a display more quickly if it has previously learned to touch all the items in such a display correctly, capturing the relationship between touching and counting noted by Alibali and DiRusso. In such cases it achieves performance sometimes thought to result from a semantic induction of the ‘cardinality principle’. Yet the errors that it makes have similarities with the patterns seen in human children’s counting errors, consistent with idea that children rely on graded and somewhat variable mechanisms similar to our neural networks.

[Pymvpd: A toolbox for multivariate pattern dependence]
Cognitive tasks engage multiple brain regions. Studying how these regions interact is key to under- stand the neural bases of cognition. Standard approaches to model the interactions between brain regions rely on univariate statistical dependence. However, newly developed methods can capture multivariate dependence. Multivariate Pattern Dependence (MVPD) is a powerful and flexible approach that trains and tests multivariate models of the interactions between brain regions using independent data. In this article, we introduce PyMVPD: an open source toolbox for Multivariate Pattern Depen- dence. The toolbox includes pre-implemented linear regression models and artificial neural network models of the interactions between regions. It is designed to be easily customizable. We demonstrate example applications of PyMVPD using well-studied seed regions such as the fusiform face area (FFA) and the parahippocampal place area (PPA). Next, we compare the performance of different model architectures. Overall, artificial neural networks outperform linear regression. Importantly, the best performing architecture is region-dependent: MVPD subdivides cortex in distinct, contiguous regions whose interaction with FFA and PPA is best captured by different models.",,,
29,1347,Jenelle,Feather,jfeather@mit.edu,Massachusetts Institute of Technology,814-341-9611,Cambridge,MA,02139,United States,"[Model metamers illuminate divergences between biological and artificial neural networks]
Deep neural network models of sensory systems are often proposed to learn representational transformations with invariances like those in the brain. To reveal these invariances we generated “model metamers” – stimuli whose activations within a model stage are matched to those of a natural stimulus. Metamers for state-of-the-art supervised and unsupervised neural network models of vision and audition were often completely unrecognizable to humans when generated from deep model stages, suggesting differences between model and human invariances. Targeted model changes improved human-recognizability of model metamers, but did not eliminate the overall human-model discrepancy. The human-recognizability of a model’s metamers was well predicted by their recognizability by other models, suggesting that models learn idiosyncratic invariances in addition to those required by the task. Metamer recognition dissociated from both traditional brain-based benchmarks and adversarial vulnerability, revealing a distinct failure mode of existing sensory models and providing a complementary benchmark for model assessment.

[Neural Population Geometry Reveals the Role of Stochasticity in Robust Perception]
Adversarial examples are often cited by neuroscientists and machine learning researchers as an example of how computational models diverge from biological sensory systems. Recent work has proposed adding biologically-inspired components to visual neural networks as a way to improve their adversarial robustness. One surprisingly effective component for reducing adversarial vulnerability is response stochasticity, like that exhibited by biological neurons. Here, using recently developed geometrical techniques from computational neuroscience, we investigate how adversarial perturbations influence the internal representations of standard, adversarially trained, and biologically-inspired stochastic networks. We find distinct geometric signatures for each type of network, revealing different mechanisms for achieving robust representations. Next, we generalize these results to the auditory domain, showing that neural stochasticity also makes auditory models more robust to adversarial perturbations. Geometric analysis of the stochastic networks reveals overlap between representations of clean and adversarially perturbed stimuli, and quantitatively demonstrates that competing geometric effects of stochasticity mediate a tradeoff between adversarial and clean performance. Our results shed light on the strategies of robust perception utilized by adversarially trained and stochastic networks, and help explain how stochasticity may be beneficial to machine and biological computation.

[Auditory texture synthesis from task-optimized convolutional neural networks]
Models of sensory systems have traditionally been hand designed from engineering principles, but modern-day machine learning allows models to be learned from data. We sought to compare hand-engineered and learned models of the auditory system by generating synthetic sound textures. We synthesized sounds that produce the same time-averaged values in each model’s representation as those measured from a natural texture using gradient-based optimization. Such stimuli should evoke the same texture percept if the model replicates the representations underlying auditory texture perception. Previous texture models involved statistics measured from multiple stages of standard visual or auditory processing cascades. We found that auditory textures generated simply from the time-averaged power in the first layer activations of a task-optimized convolutional neural network were as realistic and recognizable as the best previous auditory texture model. Unlike textures generated from traditional models, the textures from task-optimized filters did not require statistics from earlier stages in the sensory model (i.e., the cochlear stage). Further, the textures generated from the task-optimized CNN filters were more realistic than textures generated from a widely used hand-engineered model of primary auditory cortex. The results demonstrate that better sensory models can be obtained by task-optimizing sensory representations.
",,Greta Tuckute,25452274
30,1256,Ma,Feilong,feilong.ma@dartmouth.edu,Dartmouth College,603-277-0885,Hanover,NH,03755,United States,"[The Individualized Neural Tuning Model: Precise and generalizable cartography of functional architecture in individual brains]
How brain functional architecture differs across people is a key question of human neuroscience, and understanding these differences is critical for building brain-based biomarkers. However, current individualized models of brain functional organization are based on brain regions and networks, limiting their use to study fine-grained vertex- or voxel-level differences. In this work, we present the Individualized Neural Tuning (INT) model, a fine-grained individualized model of brain functional organization. The first part of the INT model models each individual’s brain responses as a linearly transformed functional template, such that it captures both functional and topographic idiosyncrasies. The second part of the INT model factorizes the modeled brain responses, separating temporal information capturing how the stimulus changes over time (shared across individuals) and stimulus-general neural tuning (specific to each individual and each cortex). The two parts of the INT model are designed in such a way that (a) the INT model has vertex-level granularity; (b) it models both functional differences and topographic differences; and (c) the modeled neural tuning is stimulus-general in that it generalizes to new stimuli. Through a series of analyzes, we demonstrate that (a) the modeled brain functional organization is highly specific to the individual and reliable across independent data; (b) the model can predict an individual’s responses to new stimuli based on others’ responses, including category selectivity maps and retinotopic maps; (c) the model can predict fine-grained response patterns, which can be used to distinguish responses to different time points of a movie; (d) the model performance keeps improving with more data, but 10–20 minutes of movie are usually sufficient for good performance. Together, these analyses demonstrate that the INT model affords an individualized fine-grained model of brain functional architecture, which is reliable, precise, and generalizable across stimuli. 

[The neural basis of intelligence in fine-grained cortical topographies]
Intelligent thought is the product of efficient neural information processing, which is embedded in fine-grained, topographically-organized population responses and supported by fine-grained patterns of connectivity among cortical fields. Previous work on the neural basis of intelligence, however, has focused on coarse-grained features of brain anatomy and function, because cortical topographies are highly idiosyncratic at a finer scale, obscuring individual differences in fine-grained connectivity patterns. We used a computational algorithm, hyperalignment, to resolve these topographic idiosyncrasies, and found that predictions of general intelligence based on fine-grained (vertex-by-vertex) connectivity patterns were markedly stronger than predictions based on coarse-grained (region-by-region) patterns. Intelligence was best predicted by fine-grained connectivity in the default and frontoparietal cortical systems, both of which are associated with self-generated thought. Previous work overlooked fine-grained architecture because existing methods couldn’t resolve idiosyncratic topographies, preventing investigation where the keys to the neural basis of intelligence are more likely to be found. 

[Modeling naturalistic face processing in humans with deep convolutional neural networks]
Deep convolutional neural networks (DCNNs) trained for face identification can rival and even exceed human-level performance. The relationships between internal representations learned by DCNNs and those of the primate face processing system are not well understood, especially in naturalistic settings. We developed the largest naturalistic dynamic face stimulus set in human neuroimaging research (700+ naturalistic video clips of unfamiliar faces) to investigate this problem. DCNN representational geometries were weakly but significantly correlated with neural response geometries across the human face processing system. Intermediate layers better matched visual, face-selective cortices, and behavioral similarity judgments than the final fully-connected layers. Our results showed DCNNs captured only a small amount of the rich information in the neural representations during naturalistic face viewing. Future artificial neural networks trained with more ecological objective functions may help advance artificial intelligence toward the ultimate goal of mimicking human intelligence in naturalistic, real-world scenarios. 
","Nikolaus Kriegeskorte, Margaret Livingstone, Kendrick Kay, Michael Bonner, Daniel Yamins, Jack Gallant",,51051553
31,1031,Dawn,Finzi,dfinzi@stanford.edu,Stanford University,240-601-7780,Stanford,CA,94305,United States,"[Do deep convolutional neural networks accurately model representations beyond the ventral stream?]
While primate visual cortex has typically been divided into two processing streams, recent research suggests that there may be at least three functionally distinct streams, extending along the ventral, lateral, and parietal surfaces of the brain. Here, we leveraged the Natural Scenes Dataset (Allen et al., 2022) to compare and model responses across these proposed streams. We show that cortical responses cluster by stream and reflect the hierarchical organization of cortex. We then tested how accurately deep convolutional neural net-
works (DCNNs) trained on supervised object categorization and action recognition objectives could predict responses in each stream. Given the differences in responses across streams and the prevailing view that only the ventral stream serves object categorization, we were surprised to find that these models fit ventral and lateral responses equally well, though they were slightly worse at predicting parietal responses. These findings suggest that additional constraints are required for model predictivity to match the functional organization of visual cortex.

	
[Differential spatial computations in ventral and lateral face-selective regions are scaffolded by structural connections]
Face-processing occurs across ventral and lateral visual streams, which are involved in static and dynamic face perception, respectively. However, the nature of spatial computations across streams is unknown. Using functional MRI and population receptive field (pRF) mapping, we measured pRFs in face-selective regions. Results reveal that spatial computations by pRFs in ventral face-selective regions are concentrated around the center of gaze (fovea), but spatial computations in lateral face-selective regions extend peripherally. Diffusion MRI reveals that these differences are mirrored by a preponderance of white matter connections between ventral face-selective regions and foveal early visual cortex (EVC), while connections with lateral regions are distributed more uniformly across EVC eccentricities. These findings suggest a rethinking of spatial computations in face-selective regions, showing that they vary across ventral and lateral streams, and further propose that spatial computations in high-level regions are scaffolded by the fine-grain pattern of white matter connections from EVC.","Robert Geirhos, Kamila Maria Jozwik","Insub Kim, Eline Kupers, development",
32,1096,Cédric,Foucault,cedric.foucault@gmail.com,"NeuroSpin, INSERM-CEA",+33-6-40-19-09-73,Paris,Ile-de-France,75005,France,"[Gated recurrence enables simple and accurate sequence prediction in stochastic, changing, and structured environments.] From decision making to perception to language, predicting what is coming next is crucial. It is also challenging in stochastic, changing, and structured environments; yet the brain makes accurate predictions in many situations. What computational architecture could enable this feat? Bayesian inference makes optimal predictions but is prohibitively difficult to compute. Here, we show that a specific recurrent neural network architecture enables simple and accurate solutions in several environments. This architecture relies on three mechanisms: gating, lateral connections, and recurrent weight training. Like the optimal solution and the human brain, such networks develop internal representations of their changing environment (including estimates of the environment’s latent variables and the precision of these estimates), leverage multiple levels of latent structure, and adapt their effective learning rate to changes without changing their connection weights. Being ubiquitous in the brain, gated recurrence could therefore serve as a generic building block to predict in real-life environments.

[A neural code for probabilities.] Most events in our lives are probabilistic. Estimating their probability is crucial for adaptive behavior. Humans can estimate such probabilities, and even explicitly report their estimate when asked. The neural basis of such probability estimates remains elusive. Here, we found a neural code of probability estimate in humans during a changing probability estimation task. This neural code explains the measured fMRI activity in prefrontal and parietal cortex. It contains a majority of non-linear responses tuned to certain preferred values of probability, which contrast with the mostly linear responses observed for another quantity—the confidence associated with each estimate. This neural code may serve as a basis for probability estimation, which underlies many behaviors.

[Human Inference in Changing Environments With Temporal Structure.] To make informed decisions in natural environments that change over time, humans must update their beliefs as new observations are gathered. Studies exploring human inference as a dynamical process that unfolds in time have focused on situations in which the statistics of observations are history-independent. Yet temporal structure is everywhere in nature, and yields history-dependent observations. Do humans modify their inference processes depending on the latent temporal statistics of their observations? We investigate this question experimentally and theoretically using a change-point inference task. We show that humans adapt their inference process to fine aspects of the temporal structure in the statistics of stimuli. As such, humans behave qualitatively in a Bayesian fashion, but, quantitatively, deviate away from optimality. Perhaps more importantly, humans behave suboptimally in that their responses are not deterministic, but variable. We show that this variability itself is modulated by the temporal statistics of stimuli. To elucidate the cognitive algorithm that yields this behavior, we investigate a broad array of existing and new models that characterize different sources of suboptimal deviations away from Bayesian inference. While models with ‘output noise’ that corrupts the response-selection process are natural candidates, human behavior is best described by sampling-based inference models, in which the main ingredient is a compressed approximation of the posterior, represented through a modest set of random samples and updated over time. This result comes to complement a growing literature on sample-based representation and learning in humans.
Here, by framing a reversal learning task either as cue-based or outcome-based inference, we found that humans perceive the same volatile environment as more stable when inferring its hidden state by interaction with uncertain outcomes than by observation of equally uncertain cues. Multivariate patterns of magnetoencephalographic (MEG) activity reflected this behavioral difference in the neural interaction between inferred beliefs and incoming evidence, an effect originating from associative regions in the temporal lobe. Together, these findings indicate that the degree of control over the sampling of volatile environments shapes human learning and decision-making under uncertainty.
","Keno Juechems, Todd Hare, Jill O'Reilly, Paula Kaanders","Caroline Bévalot, Tiffany Bounmy, Alexander Paunov, Florent Meyniel. Mice, rodent, Zebrafish, non-human.",
33,1399,Maëlle,Freteault,maelle.freteault@umontreal.ca,"Université de Montréal, IMT Atlantique",33602618436,Montreal,Quebec,H3W 1W5,Canada,"[Neural Encoding of Auditory Features during Music Perception and Imagery]
Despite many behavioral and neuroimaging investigations, it remains unclear how the human cortex represents spectrotemporal sound features during auditory imagery, and how this representation compares to auditory perception. To assess this, we recorded electrocorticographic signals from an epileptic patient with proficient music ability in 2 conditions. First, the participant played 2 piano pieces on an electronic piano with the sound volume of the digital keyboard on. Second,
the participant replayed the same piano pieces, but without auditory feedback, and the participant was asked to imagine hearing the music in his mind. In both conditions, the sound output of the keyboard was recorded, thus allowing precise time-locking between the neural activity and the spectrotemporal content of the music imagery. This novel task design provided a unique opportunity to apply receptive field modeling techniques to quantitatively study neural encoding during
auditory mental imagery. In both conditions, we built encoding models to predict high gamma neural activity (70–150 Hz) from the spectrogram representation of the recorded sound. We found robust spectrotemporal receptive fields during auditory imagery with substantial, but not complete overlap in frequency tuning and cortical location compared to receptive fields measured during auditory perception.

[The Importance of Amodal Completion in Everyday Perception]
Amodal completion is the representation of those parts of the perceived object that we get no sensory stimulation from. In the case of vision, it is the representation of occluded parts of objects we see: When we see a cat behind a picket fence, our perceptual system represents those parts of the cat that are occluded by the picket fence. The aim of this piece is to argue that amodal completion plays a constitutive role in our everyday perception and trace the theoretical consequences of this claim.

[A Task-Optimized Neural Network Replicates Human Auditory Behavior, Predicts Brain Responses, and Reveals a Cortical Processing Hierarchy]
A core goal of auditory neuroscience is to build quantitative models that predict cortical responses to natural sounds. Reasoning that a complete model of auditory cortex must solve ecologically relevant tasks, we optimized hierarchical neural networks for speech and music recognition. The best-performing network contained separate music and speech pathways following early shared processing, potentially replicating human cortical organization. The network performed both tasks as well as humans and exhibited human-like errors despite not being optimized to do so, suggesting common constraints on network and human performance. The network predicted fMRI voxel responses substantially better than traditional spectrotemporal filter models throughout auditory cortex. It also provided a quantitative signature of cortical representational hierarchy—primary and non-primary responses were best predicted by intermediate and late network layers, respectively. The results suggest that task optimization provides a powerful set of tools for modeling sensory systems.
",,"François Paugam, Anirudha Kemtur, ",
34,1433,Haydée,García Lázaro,haydee@ski.org,Smith-Kettlewell Eye Research Institute,415-483-6223,San Francisco,CA,94115,United States,"[Representational similarity analysis – connecting the branches
of systems neuroscience]

A fundamental challenge for systems neuroscience is to quantitatively relate its three major branches of research: brain-activity measurement, behavioral measurement, and computational modeling. Using measured brain-activity patterns to evaluate computational network models is complicated by the need to defi ne the correspondency between the units of the model and the channels of the brain-activity data, e.g., single-cell recordings or voxels from functional magnetic resonance imaging (fMRI). Similar correspondency problems complicate relating activity patterns between different modalities of brain-activity measurement (e.g., fMRI and invasive or scalp electrophysiology), and between subjects and species. In order to bridge these divides, we suggest abstracting from the activity patterns themselves and computing
representational dissimilarity matrices (RDMs), which characterize the information carried by a given representation in a brain or model. Building on a rich psychological and mathematical literature on similarity analysis, we propose a new experimental and data-analytical framework
called representational similarity analysis (RSA), in which multi-channel measures of neural activity are quantitatively related to each other and to computational theory and behavior by comparing RDMs. We demonstrate RSA by relating representations of visual objects as measured with
fMRI in early visual cortex and the fusiform face area to computational models spanning a wide range of complexities. The RDMs are simultaneously related via second-level application of multidimensional scaling and tested using randomization and bootstrap techniques. We discuss the broad potential of RSA, including novel approaches to experimental design, and argue that these ideas, which have deep roots in psychology and neuroscience, will allow the integrated quantitative analysis of data from all three branches, thus contributing to a more unified systems neuroscience.

[Multivariate pattern analysis for MEG: A comparison of
dissimilarity measures] 

Multivariate pattern analysis (MVPA) methods such as decoding and representational similarity analysis (RSA) are growing rapidly in popularity for the analysis of magnetoencephalography (MEG) data. However, little is known about the relative performance and characteristics of the specific dissimilarity measures used to describe differences between evoked activation patterns. Here we used a multisession MEG data set to qualitatively characterize a range of dissimilarity measures and to quantitatively compare them with respect to decoding accuracy (for decoding) and between-session reliability of representational dissimilarity matrices (for RSA). We tested
dissimilarity measures from a range of classifiers (Linear Discriminant Analysis – LDA, Support Vector Machine –SVM, Weighted Robust Distance – WeiRD, Gaussian Naïve Bayes – GNB) and distances (Euclidean distance,
Pearson correlation). In addition, we evaluated three key processing choices: 1) preprocessing (noise normalisation, removal of the pattern mean), 2) weighting decoding accuracies by decision values, and 3) computing
distances in three different partitioning schemes (non-cross-validated, cross-validated, within-class-corrected).
Four main conclusions emerged from our results. First, appropriate multivariate noise normalization substantially
improved decoding accuracies and the reliability of dissimilarity measures. Second, LDA, SVM and WeiRD yielded high peak decoding accuracies and nearly identical time courses. Third, while using decoding accuracies for RSA
was markedly less reliable than continuous distances, this disadvantage was ameliorated by decision-valueweighting of decoding accuracies. Fourth, the cross-validated Euclidean distance provided unbiased distance
estimates and highly replicable representational dissimilarity matrices. Overall, we strongly advise the use of multivariate noise normalisation as a general preprocessing step, recommend LDA, SVM and WeiRD as classifiers
for decoding and highlight the cross-validated Euclidean distance as a reliable and unbiased default choice for RSA.

[Are you for real? Decoding realistic AI-generated faces from neural activity] 

Can we trust our eyes? Until recently, we rarely had to question whether what we see is indeed what exists, but this is changing. Artificial neural networks can now generate realistic images that challenge our perception of what is real. This new reality can have significant implications for cybersecurity, counterfeiting, fake news, and border security. We investigated how the human brain encodes and interprets realistic artificially generated images using behaviour and brain imaging. We found that we could reliably decode AI generated faces using people’s neural activity. However, while at a group level people performed near chance classifying real and realistic fakes, participants tended to interchange the labels, classifying real faces as realistic fakes and vice versa. Understanding this difference between brain and behavioural responses may be key in determining the ’real’ _in our new reality.


","Konrad Kording, Nikolaus Kriegeskorte, Megan Peters, Anne Collins",,https://www.semanticscholar.org/author/Haydee-G.-Garcia-Lazaro/1403517574
35,1144,Richard,Gerum,richard.gerum@protonmail.com,York University,+1-647-765-6848,Toronto,Ontario,M5V 4A5,Canada,"
[Different Spectral Representations in Optimized Artificial Neural Networks and Brains]
Recent studies suggest that artificial neural networks (ANNs) that match the spectral
properties of the mammalian visual cortex—namely, the ~ 1/n eigenspectrum
of the covariance matrix of neural activities—achieve higher object recognition
performance and robustness to adversarial attacks than those that do not. To our
knowledge, however, no previous work systematically explored how modifying the
ANN’s spectral properties affects performance. To fill this gap, we performed a
systematic search over spectral regularizers, forcing the ANN’s eigenspectrum to
follow 1/na power laws with different exponents a. We found that larger powers
(around 2–3) lead to better validation accuracy and more robustness to adversarial
attacks on dense networks. This surprising finding applied to both shallow and deep
networks and it overturns the notion that the brain-like spectrum (corresponding to
a ~ 1) always optimizes ANN performance and/or robustness. For convolutional
networks, the best a values depend on the task complexity and evaluation metric:
lower a values optimized validation accuracy and robustness to adversarial attack
for networks performing a simple object recognition task (categorizing MNIST
images of handwritten digits); for a more complex task (categorizing CIFAR-
10 natural images), we found that lower a values optimized validation accuracy
whereas higher a values optimized adversarial robustness. These results have two
main implications. First, they cast doubt on the notion that brain-like spectral
properties (a ~ 1) always optimize ANN performance. Second, they demonstrate
the potential for fine-tuned spectral regularizers to optimize a chosen design metric,
i.e., accuracy and/or robustness.

[Improving the Accuracy and Robustness of CNNs Using a Deep CCA Neural Data Regularizer]
As convolutional neural networks (CNNs) become more accurate at object recognition, their representations become more similar to the primate visual system. This finding has inspired us and other researchers to ask if the implication also runs the other way: If CNN representations become more brain-like, does the network become more accurate? Previous attempts to address this question showed very modest gains in accuracy, owing in part to limitations of the regularization method. To overcome these limitations, we developed a new neural data regularizer for CNNs that uses Deep Canonical Correlation Analysis (DCCA) to optimize the resemblance of the CNN's image representations to that of the monkey visual cortex. Using this new neural data regularizer, we see much larger performance gains in both classification accuracy and within-super-class accuracy, as compared to the previous state-of-the-art neural data regularizers. These networks are also more robust to adversarial attacks than their unregularized counterparts. Together, these results  confirm  that neural data regularization can push CNN performance higher, and introduces a new method that obtains a larger performance boost.

[Integration of Leaky-Integrate-and-Fire Neurons in Standard Machine Learning Architectures to Generate Hybrid Networks: A Surrogate Gradient Approach]
Up to now, modern machine learning (ML) has been based on approx-
imating big data sets with high-dimensional functions, taking advan-
tage of huge computational resources. We show that biologically inspired
neuron models such as the leaky-integrate-and-fire (LIF) neuron provide
novel and efficient ways of information processing. They can be inte-
grated in machine learning models and are a potential target to improve
ML performance. Thus, we have derived simple update rules for LIF units
to numerically integrate the differential equations. We apply a surrogate
gradient approach to train the LIF units via backpropagation. We demon-
strate that tuning the leak term of the LIF neurons can be used to run the
neurons in different operating modes, such as simple signal integrators
or coincidence detectors. Furthermore, we show that the constant surro-
gate gradient, in combination with tuning the leak term of the LIF units,
can be used to achieve the learning dynamics of more complex surrogate
gradients.
To prove the validity of our method, we applied it to established im-
age data sets (the Oxford 102 flower data set, MNIST), implemented
various network architectures, used several input data encodings and
demonstrated that the method is suitable to achieve state-of-the-art clas-
sification performance.
We provide our method as well as further surrogate gradient methods
to train spiking neural networks via backpropagation as an open-source
KERAS package to make it available to the neuroscience and machine
learning community. To increase the interpretability of the underlying
effects and thus make a small step toward opening the black box of ma-
chine learning, we provide interactive illustrations, with the possibil-
ity of systematically monitoring the effects of parameter changes on the
learning characteristics",,,
36,1040,Alessandro,Gifford,alessandro.gifford@gmail.com,Freie Universität Berlin,+39-345-697-4116,Berlin,,13347,Germany,"[A large and rich EEG dataset for modeling human visual object recognition]
The human brain achieves visual object recognition through multiple stages of nonlinear transformations operating at a millisecond scale. To predict and explain these rapid transformations, computational neuroscientists employ machine learning modeling techniques. However, state-of-the-art models require massive amounts of data to properly train, and to the present day there is a lack of vast brain datasets which extensively sample the temporal dynamics of visual object recognition. Here we collected a large and rich dataset of high temporal resolution EEG responses to images of objects on a natural background. This dataset includes 10 participants, each with 82,160 trials spanning 16,740 image conditions. Through computational modeling we established the quality of this dataset in five ways. First, we trained linearizing encoding models that successfully synthesized the EEG responses to arbitrary images. Second, we correctly identified the recorded EEG data image conditions in a zero-shot fashion, using EEG synthesized responses to hundreds of thousands of candidate image conditions. Third, we show that both the high number of conditions as well as the trial repetitions of the EEG dataset contribute to the trained models’ prediction accuracy. Fourth, we built encoding models whose predictions well generalize to novel participants. Fifth, we demonstrate full end-to-end training of randomly initialized DNNs that output M/EEG responses for arbitrary input images. We release this dataset as a tool to foster research in visual neuroscience and computer vision.",,,
37,1269,Lakshmi Narasimhan,Govindarajan,lakshmi_govindarajan@brown.edu,Brown University,857-283-0073,Providence,Rhode Island,02906,United States,"[Stable and expressive recurrent vision models]
Primate vision depends on recurrent processing for reliable perception. A growing body of literature also suggests that recurrent connections improve the learning efficiency and generalization of vision models on classic computer vision challenges. Why then, are current large-scale challenges dominated by feedforward networks? We posit that the effectiveness of recurrent vision models is bottlenecked by the standard algorithm used for training them, ""back-propagation through time"" (BPTT), which has O(N) memory-complexity for training an N step model. Thus, recurrent vision model design is bounded by memory constraints, forcing a choice between rivaling the enormous capacity of leading feedforward models or trying to compensate for this deficit through granular and complex dynamics. Here, we develop a new learning algorithm, ""contractor recurrent back-propagation"" (C-RBP), which alleviates these issues by achieving constant O(1) memory-complexity with steps of recurrent processing. We demonstrate that recurrent vision models trained with C-RBP can detect long-range spatial dependencies in a synthetic contour tracing task that BPTT-trained models cannot. We further show that recurrent vision models trained with C-RBP to solve the large-scale Panoptic Segmentation MS-COCO challenge outperform the leading feedforward approach, with fewer free parameters. C-RBP is a general-purpose learning algorithm for any application that can benefit from expansive recurrent dynamics.

[Likelihood approximation networks (LANs) for fast inference of simulation models in cognitive neuroscience]
In cognitive neuroscience, computational modeling can formally adjudicate between theories and affords quantitative fits to behavioral/brain data. Pragmatically, however, the space of plausible generative models considered is dramatically limited by the set of models with known likelihood functions. For many models, the lack of a closed-form likelihood typically impedes Bayesian inference methods. As a result, standard models are evaluated for convenience, even when other models might be superior. Likelihood-free methods exist but are limited by their computational cost or their restriction to particular inference scenarios. Here, we propose neural networks that learn approximate likelihoods for arbitrary generative models, allowing fast posterior sampling with only a one-off cost for model simulations that is amortized for future inference. We show that these methods can accurately recover posterior parameter distributions for a variety of neurocognitive process models. We provide code allowing users to deploy these methods for arbitrary hierarchical model instantiations without further training.

",,,
38,1139,Michelle,Greene,mgreene2@bates.edu,Bates College,303-653-3125,Lewiston,ME,04240,United States,"[Shared spatiotemporal category representations in biological and artificial deep neural networks]
Visual scene category representations emerge very rapidly, yet the computational transformations that enable such invariant categorizations remain elusive. Deep convolutional neural networks (CNNs) perform visual categorization at near human-level accuracy using a feedforward architecture, providing neuroscientists with the opportunity to assess one successful series of representational transformations that enable categorization in silico. The goal of the current study is to assess the extent to which sequential scene category representations built by a CNN map onto those built in the human brain as assessed by high-density, time-resolved event-related potentials (ERPs). We found correspondence both over time and across the scalp: earlier (0–200 ms) ERP activity was best explained by early CNN layers at all electrodes. Although later activity at most electrode sites corresponded to earlier CNN layers, activity in right occipito-temporal electrodes was best explained by the later, fully-connected layers of the CNN around 225 ms post-stimulus, along with similar patterns in frontal electrodes. Taken together, these results suggest that the emergence of scene category representations develop through a dynamic interplay between early activity over occipital electrodes as well as later activity over temporal and frontal electrodes.

[Visual scenes are categorized by function]
How do we know that a kitchen is a kitchen by looking? Traditional models posit that scene categorization is achieved through recognizing necessary and sufficient features and objects, yet there is little consensus about what these may be. However, scene categories should reflect how we use visual information. Therefore, we test the hypothesis that scene categories reflect functions, or the possibilities for actions within a scene. Our approach is to compare human categorization patterns with predictions made by both functions and alternative models. We collected a large-scale scene category distance matrix (5 million trials) by asking observers to simply decide whether 2 images were from the same or different categories. Using the actions from the American Time Use Survey, we mapped actions onto each scene (1.4 million trials). We found a strong relationship between ranked category distance and functional distance (r = .50, or 66% of the maximum possible correlation). The function model outperformed alternative models of object-based distance (r = .33), visual features from a convolutional neural network (r = .39), lexical distance (r = .27), and models of visual features. Using hierarchical linear regression, we found that functions captured 85.5% of overall explained variance, with nearly half of the explained variance captured only by functions, implying that the predictive power of alternative models was because of their shared variance with the function-based model. These results challenge the dominant school of thought that visual features and objects are sufficient for scene categorization, suggesting instead that a scene's category may be determined by the scene's function. 

[Disentangling the Independent Contributions of Visual and Conceptual Features to the Spatiotemporal Dynamics of Scene Categorization]
Human scene categorization is characterized by its remarkable speed. While many visual and conceptual features have been linked to this ability, significant correlations exist between feature spaces, impeding our ability to determine their relative contributions to scene categorization. Here, we used a whitening transformation to decorrelate a variety of visual and conceptual features and assess the time course of their unique contributions to scene categorization. Participants (both sexes) viewed 2250 full-color scene images drawn from 30 different scene categories while having their brain activity measured through 256-channel EEG. We examined the variance explained at each electrode and time point of visual event-related potential (vERP) data from nine different whitened encoding models. These ranged from low-level features obtained from filter outputs to high-level conceptual features requiring human annotation. The amount of category information in the vERPs was assessed through multivariate decoding methods. Behavioral similarity measures were obtained in separate crowdsourced experiments. We found that all nine models together contributed 78% of the variance of human scene similarity assessments and were within the noise ceiling of the vERP data. Low-level models explained earlier vERP variability (88 ms after image onset), whereas high-level models explained later variance (169 ms). Critically, only high-level models shared vERP variability with behavior. Together, these results suggest that scene categorization is primarily a high-level process, but reliant on previously extracted low-level features.
SIGNIFICANCE STATEMENT In a single fixation, we glean enough information to describe a general scene category. Many types of features are associated with scene categories, ranging from low-level properties, such as colors and contours, to high-level properties, such as objects and attributes. Because these properties are correlated, it is difficult to understand each property's unique contributions to scene categorization. This work uses a whitening transformation to remove the correlations between features and examines the extent to which each feature contributes to visual event-related potentials over time. We found that low-level visual features contributed first but were not correlated with categorization behavior. High-level features followed 80 ms later, providing key insights into how the brain makes sense of a complex visual world.",,,
39,1338,Kalanit,Grill-Spector,kalanit@stanford.edu,Stanford University,650-269-9605,Stanford,CA,94305,United States,"[Spatiotemporal population receptive fields 
Deep convolutional neural networks
Central 
visual stream]
","Niko Kriegeskorte 
Kendrick Kay
Kohjit Karr
",,Kalanit 
40,1345,Max,Grogan,max.grogan19@imperial.ac.uk,Imperial College London,+44-7365-528-170,London,,SE11 6BW,United Kingdom,"[Deep neuroethology of a virtual rodent]
Parallel developments in neuroscience and deep learning have led to mutually productive exchanges, pushing our understanding of real and artificial neural networks in sensory and cognitive systems. However, this interaction between fields is less developed in the study of motor control. In this work, we develop a virtual rodent as a platform for the grounded study of motor activity in artificial models of embodied control. We then use this platform to study motor activity across contexts by training a model to solve four complex tasks. Using methods familiar to neuroscientists, we describe the behavioral representations and algorithms employed by different layers of the network using a neuroethological approach to characterize motor activity relative to the rodent's behavior and goals. We find that the model uses two classes of representations which respectively encode the task-specific behavioral strategies and task-invariant behavioral kinematics. These representations are reflected in the sequential activity and population dynamics of neural subpopulations. Overall, the virtual rodent facilitates grounded collaborations between deep reinforcement learning and motor neuroscience. 

[Contextual inference underlies the learning of sensorimotor repertoires]

Humans spend a lifetime learning, storing and refining a repertoire of motor memories. For example, through experience, we become proficient at manipulating a large range of objects with distinct dynamical properties. However, it is unknown what principle underlies how our continuous stream of sensorimotor experience is segmented into separate memories and how we adapt and use this growing repertoire. Here we develop a theory of motor learning based on the key principle that memory creation, updating and expression are all controlled by a single computation—contextual inference. Our theory reveals that adaptation can arise both by creating and updating memories (proper learning) and by changing how existing memories are differentially expressed (apparent learning). This insight enables us to account for key features of motor learning that had no unified explanation: spontaneous recovery1, savings2, anterograde interference3, how environmental consistency affects learning rate4,5 and the distinction between explicit and implicit learning6. Critically, our theory also predicts new phenomena—evoked recovery and context-dependent single-trial learning—which we confirm experimentally. These results suggest that contextual inference, rather than classical single-context mechanisms1,4,7,8,9, is the key principle underlying how a diverse set of experiences is reflected in our motor behaviour.

[A neural network that finds a naturalistic solution for the production of muscle activity]

It remains an open question how neural responses in motor cortex relate to movement. We explored the hypothesis that motor cortex reflects dynamics appropriate for generating temporally patterned outgoing commands. To formalize this hypothesis, we trained recurrent neural networks to reproduce the muscle activity of reaching monkeys. Models had to infer dynamics that could transform simple inputs into temporally and spatially complex patterns of muscle activity. Analysis of trained models revealed that the natural dynamical solution was a low-dimensional oscillator that generated the necessary multiphasic commands. This solution closely resembled, at both the single-neuron and population levels, what was observed in neural recordings from the same monkeys. Notably, data and simulations agreed only when models were optimized to find simple solutions. An appealing interpretation is that the empirically observed dynamics of motor cortex may reflect a simple solution to the problem of generating temporally patterned descending commands.",,,
41,1264,Jiahui,Guo,jiahui.guo@dartmouth.edu,Dartmouth College,603-277-0856,Hanover,NH,03755,United States,"[Modeling naturalistic face processing in humans with deep convolutional neural networks]
Deep convolutional neural networks (DCNNs) trained for face identification can rival and even exceed human-level performance. The relationships between internal representations learned by DCNNs and those of the primate face processing system are not well understood, especially in naturalistic settings. We developed the largest naturalistic dynamic face stimulus set in human neuroimaging research (700+ naturalistic video clips of unfamiliar faces) to investigate this problem. DCNN representational geometries were weakly but significantly correlated with neural response geometries across the human face processing system. Intermediate layers better matched visual, face-selective cortices, and behavioral similarity judgments than the final fully-connected layers. Our results showed DCNNs captured only a small amount of the rich information in the neural representations during naturalistic face viewing. Future artificial neural networks trained with more ecological objective functions may help advance artificial intelligence toward the ultimate goal of mimicking human intelligence in naturalistic, real-world scenarios.

[Predicting individual face-selective topography using naturalistic stimuli]
Subject-specific, functionally defined areas are conventionally estimated with functional localizers and a simple contrast analysis between responses to different stimulus categories. Compared with functional localizers, naturalistic stimuli provide several advantages such as stronger and widespread brain activation, greater engagement, and increased subject compliance. In this study we demonstrate that a subject’s idiosyncratic functional topography can be estimated with high fidelity from that subject’s fMRI data obtained while watching a naturalistic movie using hyperalignment to project other subjects’ localizer data into that subject’s idiosyncratic cortical anatomy. These findings lay the foundation for developing an efficient tool for mapping functional topographies for a wide range of perceptual and cognitive functions in new subjects based only on fMRI data collected while watching an engaging, naturalistic stimulus and other subjects’ localizer data from a normative sample.

[Developmental prosopagnosics have widespread selectivity reductions across category-selective visual cortex]
Developmental prosopagnosia (DP) is a neurodevelopmental disorder characterized by severe deficits with facial identity recognition. It is unclear which cortical areas contribute to face processing deficits in DP, and no previous studies have investigated whether other category-selective areas function normally in DP. To address these issues, we scanned 22 DPs and 27 controls using a dynamic localizer consisting of video clips of faces, scenes, bodies, objects, and scrambled objects. We then analyzed category selectivity, a measure of the tuning of a cortical area to a particular visual category. DPs exhibited reduced face selectivity in all 12 face areas, and the reductions were significant in three posterior and two anterior areas. DPs and controls showed similar responses to faces in other category-selective areas, which suggests the DPs’ behavioral deficits with faces result from problems restricted to the face network. DPs also had pronounced scene-selectivity reductions in four of six scene-selective areas and marginal body-selectivity reductions in two of four body-selective areas. Our results demonstrate that DPs have widespread deficits throughout the face network, and they are inconsistent with a leading account of DP which proposes that posterior face-selective areas are normal in DP. The selectivity reductions in other category-selective areas indicate many DPs have deficits spread across high-level visual cortex.",,,
42,1319,Todd,Gureckis,todd.gureckis@nyu.edu,New York University,949-439-0867,New York,New York,10003,United States,"[Creativity, Compositionality, and Common Sense in Human Goal Generation] 
Inspired by notions of intrinsic motivation (Schmidhuber, 2010) and play as proposing and solving arbitrary problems (Chu and Schulz, 2020) we report initial progress toward computational modeling of playful goal generation. We create an embodied, 3D environment resembling a child's bedroom, and ask study participants to play in the environment and then create a scorable game. We propose to model games using a domain-specific language, which represents each game as a computer program. These programs act as reward-generating functions, mapping states visited by an agent as they play a game to the score they should receive. We then analyze our corpus of program representations to highlight four key aspects of human games that would contribute to constructing effective computational models of game generation: creativity, compositionality, common sense, and context sensitivity.

[Limits on simulation approaches in intuitive physics]
A popular explanation of the human ability for physical reasoning is that it depends on a sophisticated ability to perform mental simulations. According to this perspective, physical reasoning problems are approached by repeatedly simulating relevant aspects of a scenario, with noise, and making judgments based on aggregation over these simulations. In this paper, we describe three core tenets of simulation approaches, theoretical commitments that must be present in order for a simulation approach to be viable. The identification of these tenets threatens the plausibility of simulation as a theory of physical reasoning, because they appear to be incompatible with what we know about cognition more generally. To investigate this apparent contradiction, we describe three experiments involving simple physical judgments and predictions, and argue their results challenge these core predictions of theories of mental simulation.

[Fast and flexible: Human program induction in abstract reasoning tasks]
The Abstraction and Reasoning Corpus (ARC) is a challenging program induction dataset that was recently proposed by Chollet (2019). Here, we report the first set of results collected from a behavioral study of humans solving a subset of tasks from ARC (40 out of 1000). Although this subset of tasks contains considerable variation, our results showed that humans were able to infer the underlying program and generate the correct test output for a novel test input example, with an average of 80% of tasks solved per participant, and with 65% of tasks being solved by more than 80% of participants. Additionally, we find interesting patterns of behavioral consistency and variability within the action sequences during the generation process, the natural language descriptions to describe the transformations for each task, and the errors people made. Our findings suggest that people can quickly and reliably determine the relevant features and properties of a task to compose a correct solution. Future modeling work could incorporate these findings, potentially by connecting the natural language descriptions we collected here to the underlying semantics of ARC.",,,3013567
43,1067,Marie,Habermann,m.habermann@uke.de,University Medical Center Hamburg-Eppendorf,+49-177-744-1466,Hamburg,,20246,Germany,"[Stress-sensitive inference of task controllability]

Estimating the controllability of the environment enables agents to better predict upcoming events and decide when to engage
controlled action selection. How does the human brain estimate controllability? Trial-by-trial analysis of choices, decision
times and neural activity in an explore-and-predict task demonstrate that humans solve this problem by comparing the predictions
of an ‘actor’ model with those of a reduced ‘spectator’ model of their environment. Neural blood oxygen level-dependent
responses within striatal and medial prefrontal areas tracked the instantaneous difference in the prediction errors generated
by these two statistical learning models. Blood oxygen level-dependent activity in the posterior cingulate, temporoparietal and
prefrontal cortices covaried with changes in estimated controllability. Exposure to inescapable stressors biased controllability
estimates downward and increased reliance on the spectator model in an anxiety-dependent fashion. Taken together, these
findings provide a mechanistic account of controllability inference and its distortion by stress exposure.


[Biased pain reports through vicarious information: A computational approach to investigate the role of uncertainty]

Expectations about an impeding pain stimulus strongly shape its perception, yet the degree that uncertainty
might affect perception is far less understood. To explore the influence of uncertainty on pain ratings,
we performed a close replication of the study of Yoshida, Seymour, Koltzenburg, and Dolan (2013),
who manipulated vicarious information about upcoming heat pain and found evidence for uncertaintyinduced
hyperalgesia. In our study, we presented eight fictitious ratings of previous participants prior the
delivery of electrocutaneous pain. The vicarious information was either biased to over- or underreport
pain levels based on the participant’s psychometric function. We induced uncertainty by manipulating
the variation of the vicarious information. As in Yoshida et al. (2013), four computational models were
formulated, such that each model represented a different way of how the pain ratings might have been
generated by the physical stimulus and the vicarious information. The four competing models were
tested against the data of each participant separately. Using a formal model selection criterion, the best
model was selected and interpreted. Contrary to the original study, the preferred model for the majority
of participants suggested that pain ratings were biased towards the average vicarious information, ignoring
the degree of uncertainty. Possible reasons for these diverging results are discussed.

",,,
44,1435,Andrew,Hansen,aghansen@uci.edu,University of California - Irvine,5125384340,Costa Mesa,California,92626,United States,"[Morphogenic Learning in 3D Enzymatic Reaction-Diffusion Networks]
Alan Turing’s final work, “The Chemical Basis of Morphogenesis,” provided a rigorous framework for analyzing the emergence of complex “Turing” patterns in perturbed media. Since then, our understanding of both reaction-diffusion (R-D) systems and neural networks (NNs) have advanced significantly. Diffusion dynamics have provided a chemical basis for memory, as well as an array of neuron models, such as the Hodgkin-Huxley model. NNs are universal function approximators which have resulted in a recent explosion in network-based cognitive models for perception and learning. Despite computational biology and cognitive neuroscience remaining largely separated, there is growing interest in providing a biological foundation for explaining learning in NNs. Recent work has shown that learning can occur within genetic frameworks, and there is growing evidence of learning in single neurons, but no unifying basis for such learning exists, especially in terms of the fundamental chemistry itself.

[Higher-Order Explanations of Graph Neural Networks via Relevant Walks]
Graph Neural Networks (GNNs) are a popular approach for predicting graph structured data. As GNNs tightly entangle the input graph into the neural network structure, common explainable AI approaches are not applicable. To a large extent, GNNs have remained black-boxes for the user so far. In this paper, we show that GNNs can in fact be naturally explained using higher-order expansions, i.e. by identifying groups of edges that jointly contribute to the prediction. Practically, we find that such explanations can be extracted using a nested attribution scheme, where existing techniques such as layer-wise relevance propagation (LRP) can be applied at each step. The output is a collection of walks into the input graph that are relevant for the prediction. Our novel explanation method, which we denote by GNN-LRP, is applicable to a broad range of graph neural networks and lets us extract practically relevant insights on sentiment analysis of text data, structure-property relationships in quantum chemistry, and image classification.

[The claustrum’s proposed role in consciousness is supported by the effect and target localization of Salvia divinorum]
This article brings together three findings and ideas relevant for the understanding of human consciousness: (I) Crick’s and Koch’s theory that the claustrum is a “conductor of consciousness” crucial for subjective conscious experience. (II) Subjective reports of the consciousness-altering effects the plant Salvia divinorum, whose primary active ingredient is salvinorin A, a ?-opioid receptor agonist. (III) The high density of ?-opioid receptors in the claustrum. Fact III suggests that the consciousness-altering effects of S. divinorum/salvinorin A (II) are due to a ?-opioid receptor mediated inhibition of primarily the claustrum and, additionally, the deep layers of the cortex, mainly in prefrontal areas. Consistent with Crick and Koch’s theory that the claustrum plays a key role in consciousness (I), the subjective effects of S. divinorum indicate that salvia disrupts certain facets of consciousness much more than the largely serotonergic hallucinogen lysergic acid diethylamide (LSD). Based on this data and on the relevant literature, we suggest that the claustrum does indeed serve as a conductor for certain aspects of higher-order integration of brain activity, while integration of auditory and visual signals relies more on coordination by other areas including parietal cortex and the pulvinar.",,,
45,1167,Bruce,Hansen,bchansen@colgate.edu,Colgate University,315-228-7349,Hamilton,NY,13346,United States,"Dynamic Electrode-to-Image (DETI) mapping reveals the human brain’s spatiotemporal code of visual information

A number of neuroimaging techniques have been employed to understand how visual information is transformed along the visual pathway. Although each technique has spatial and temporal limitations, they can each provide important insights into the visual code. While the BOLD signal of fMRI can be quite informative, the visual code is not static and this can be obscured by fMRI’s poor temporal resolution. In this study, we leveraged the high temporal resolution of EEG to develop an encoding technique based on the distribution of responses generated by a population of real-world scenes. This approach maps neural signals to each pixel within a given image and reveals location-specific transformations of the visual code, providing a spatiotemporal signature for the image at each electrode. Our analyses of the mapping results revealed that scenes undergo a series of nonuniform transformations that prioritize different spatial frequencies at different regions of scenes over time. This mapping technique offers a potential avenue for future studies to explore how dynamic feedforward and recurrent processes inform and refine high-level representations of our visual world.

Disentangling the Independent Contributions of Visual and Conceptual Features to the Spatiotemporal Dynamics of Scene Categorization

Human scene categorization is characterized by its remarkable speed. While many visual and conceptual features have been linked to this ability, significant correlations exist between feature spaces, impeding our ability to determine their relative contributions to scene categorization. Here, we used a whitening transformation to decorrelate a variety of visual and conceptual features and assess the time course of their unique contributions to scene categorization. Participants (both sexes) viewed 2250 full-color scene images drawn from 30 different scene categories while having their brain activity measured through
256-channel EEG. We examined the variance explained at each electrode and time point of visual event-related potential (vERP) data from nine different whitened encoding models. These ranged from low-level features obtained from filter outputs to high-level conceptual features requiring human annotation. The amount of category information in the vERPs was assessed through multivariate decoding methods. Behavioral similarity measures were obtained in separate crowdsourced experiments. We found that all nine models together contributed 78% of the variance of human scene similarity assessments and were within the noise ceiling of the vERP data. Low-level models explained earlier vERP variability (88 ms after image onset), whereas high-level models explained later variance (169 ms). Critically, only high-level models shared vERP variability with behavior. Together, these results suggest that scene categorization is primarily a high-level process, but reliant on previously extracted low-level features.

Towards a state-space geometry of neural responses to natural scenes: A steady-state approach

Our understanding of information processing by the mammalian visual system has come through a variety of techniques ranging from psychophysics and fMRI to single unit recording and EEG. Each technique provides unique insights into the processing framework of the early visual system. Here, we focus on the nature of the information that is carried by steady state visual evoked potentials (SSVEPs). To study the information provided by SSVEPs, we presented human participants with a population of natural scenes and measured the relative SSVEP response. Rather than focus on particular features of this signal, we focused on the full state-space of possible responses and investigated how the evoked responses are mapped onto this space. Our results show that it is possible to map the relatively high-dimensional signal carried by SSVEPs onto a 2-dimensional space with little loss. We also show that a simple biologically plausible model can account for a high proportion of the explainable variance (~73%) in that space. Finally, we describe a technique for measuring the mutual information that is available about images from SSVEPs. The techniques introduced here represent a new approach to understanding the nature of the information carried by SSVEPs. Crucially, this approach is general and can provide a means of comparing results across different neural recording methods. Altogether, our study sheds light on the encoding principles of early vision and provides a much needed reference point for understanding subsequent transformations of the early visual response space to deeper knowledge structures that link different visual environments.",,,
46,1340,John,Haracz,jharacz@berkeley.edu,Indiana University,510-910-2025,Berkeley,CA,94704,United States,"[Neuroeconomics of Asset-Price Bubbles: Neuroimaging, FinTech, and Market Design for the Prediction and Prevention of Major Bubbles]
Asset-price bubbles challenge the explanatory and predictive power of standard economic theory, so neuroeconomic measures should be explored as potential tools for improving the predictive power of standard theory.  This exploration is begun by reviewing results from functional magnetic resonance imaging studies of lab asset-price bubbles and herding behavior (i.e., following others' decisions).  These results support a neuroeconomics-based hypothesis of asset-price bubbles.  In this view, decision making during bubble or non-bubble periods of financial-market activity is driven by, respectively, evolutionarily ancient or new neurocircuitry.  Neuroimaging studies that test this or other neuroeconomics-based hypotheses of asset-price bubbles may yield a bubble-related biomarker (e.g., low trade-related lateral neocortical activity associated with traders’ herding-based decisions).  Wearable functional near-infrared spectroscopy technology could determine the prevalence of such a biomarker among financial-market participants, thereby enabling the real-time detection of an emerging bubble.  Market designs are described by which this early-warning signal could be used to implement a negative feedback loop in self-regulatory or government-administered policies for financial-system stabilization.  Digital technology (i.e., financial technology or FinTech) may offer an even more readily achievable alternative to neuroimaging (e.g., behavioral precursors to price bubbles may be identified in analyses of investors’ interactions with asset-trading platforms).  In summary, neuroimaging- or FinTech-based financial-system regulation with negative feedback may be useful for distinguishing bubble from non-bubble periods and preventing major price bubbles.  To clarify the role of cognitive distortions in these cyclical periods, prices are hypothesized to act as heuristic signals that guide price heuristics, thereby eliciting bubble or crash biases.
[Overcoming the Failure of Neoclassical Economics to Capture Excessive Demand: A Learning-to-Neuroforecast Experimental Approach]
Dynamic stochastic general equilibrium models have been widely criticized for failing to forecast the Global Financial Crisis of 2008-2009 (Guzman & Stiglitz, 2020; Vines & Wills, 2020; Yellen, 2010). This and other flaws of neoclassical economics are presently proposed to arise from the failure of equilibrium-based models to capture excessive demand, which exceeds the balanced excess demand in general equilibrium theory. The present theoretical study seeks potential neuroeconomic biomarkers of excessive demands. A learning-to-neuroforecast (LtN) experimental approach is proposed for elucidating computational mechanisms that underlie excessive demands. Learning to forecast (LtF) experiments have revealed that subjects coordinate on a price trend-following rule in lab asset markets with large price bubbles (Anufriev & Hommes, 2012; Hommes, 2013). Therefore, neuroimaging applied in the LtF setting (i.e., LtN experiments) may yield biomarkers of excessive demands that destabilize financial or commodity markets. A high biomarker prevalence in real markets could indicate that financial- or commodity-market demands have exceeded boundary conditions, beyond which equilibrium-oriented models are less applicable than alternatives (e.g., novel disequilibrium [Guzman & Stiglitz, 2020] or multiple equilibrium models [Vines & Wills, 2020]).
[The importance of negative feedback and countervailing measures for financial-system stabilization and constrained inequality: A Covid-19-induced reminder]
Positive-feedback mechanisms destabilize asset markets and promote wealth or income inequality.  The Covid-19 pandemic exposed mispricing in asset markets and triggered a price crash.  The pandemic also exacerbated inequality.  Runaway positive-feedback mechanisms can be counteracted by implementing negative-feedback loops for financial-system stabilization.  Asset markets are sorely lacking the types of negative-feedback loops that promote stability and equilibrium in biological or engineering systems.  For introducing negative feedback into asset markets, investors could be fed back the prevalence of destabilizing investment strategies.  A rise in this prevalence would warn investors to exit overheated asset markets, thereby cooling off these markets endogenously.  Price bubbles in overheated markets represent a market-failure mode, in which prices fail to closely track asset fundamental value.  To prevent this failure, a market design should feed back the prevalence of investors’ destabilizing impulsive strategies, so traders could judge the degree of mispricing and voluntarily exit overheated markets.  By exacerbating inequalities in income and health, the pandemic also is a reminder of the need for implementing countervailing measures to constrain inequality.  These measures could include modest New Deal-like programs that would be more readily implemented than the original New Deal in the US because existing government programs could be scaled up (e.g., housing vouchers for low-income renters, Medicaid, and the Earned Income Tax Credit).  In summary, the widely recognized need for government spending in response to the pandemic should be seen as an opportunity to implement transformative measures aimed at stabilizing asset markets and constraining inequality.

","Nathaniel Daw, Evelina Fedorenko, Jessica Hamrick, Konrad Kording, Nikolaus Kriegeskorte, David Redish",,6581683
47,1251,Sevan,Harootonian,skh@princeton.edu,Princeton University,818-636-7062,Princeton,New Jersey,08540,United States,"[Bayesian Theory of Mind: Modeling Joint Belief-Desire Attribution]

We present a computational framework for understanding Theory of Mind (ToM): the human capacity for reasoning about agents’ mental states such as beliefs and desires. Our Bayesian model of ToM (or BToM) expresses the predictive model of belief- and desire-dependent action at the heart of ToM as a partially observable Markov decision process (POMDP), and reconstructs an agent’s joint belief state and reward function using Bayesian inference, conditioned on observations of the agent’s behavior in some environmental context. We test BToM by showing participants sequences of agents moving in simple spatial scenarios and asking for joint inferences about the agents’ desires and beliefs about unobserved aspects of the environment. BToM performs substantially better than two simpler variants: one in which desires are inferred without reference to an agent’s beliefs, and another in which beliefs are inferred without reference to the agent’s dynamic observations in the environment.

[Collaborative decision making is grounded in representations of other people’s competence and effort]

By collaborating with others, humans can pool their limited knowledge, skills, and resources to achieve goals that outstrip the abilities of any one person. What cognitive capacities make human collaboration possible? Here, we propose that collaboration is grounded in an intuitive understanding of how others think and of what they can do—in other words, of their mental states and competence. We present a belief-desire-competence framework that formalizes this proposal by extending existing models of commonsense psychological reasoning. Our framework predicts that agents recursively reason how much effort they and their partner will allocate to a task, based on the rewards at stake and on their own and their collaborator’s competence. Across three experiments (N = 249), we show that the belief-desire-competence framework captures human judgments in a variety of contexts that are critical to collaboration, including predicting whether a joint activity will succeed (Experiment 1), selecting incentives for collaborators (Experiment 2), and choosing which individuals to recruit for a collaborative task (Experiment 3). Our work provides a theoretical framework for understanding how commonsense psychological reasoning contributes to collaborative achievements.

[Computational Models for the Combination of Advice andIndividual Learning]

Decision making often takes place in social environments where other actors influence individuals’ decisions. The present article examines how advice affects individual learning. Five social learning models combining advice and individual learning-four based on reinforcement learning and one on Bayesian learning-and one individual learning model are tested against each other. In two experiments, some participants received good or bad advice prior to a repeated multioption choice task. Receivers of advice adhered to the advice, so that good advice improved performance. The social learning models described the observed learning processes better than the individual learning model. Of the models tested, the best social learning model assumes that outcomes from recommended options are more positively evaluated than outcomes from nonrecommended options. This model correctly predicted that receivers first adhere to advice, then explore other options, and finally return to the recommended option. The model also predicted accurately that good advice has a stronger impact on learning than bad advice. One-time advice can have a long-lasting influence on learning by changing the subjective evaluation of outcomes of recommended options.",,"Mark Ho, Erie Boorman, ",
48,1337,Jennifer,Hart,jhart2@bates.edu,Bates College,207-753-6979,Lewiston,ME,04240,United States,"[Sampling Human Visual Experience Through Text and Media Messages]
How do locations, actions, and goals interact to affect how we categorize our environments? Recent work has shown that scene categorization is mainly a high-level process focusing on high-level properties like objects and attributes that are processed iteratively with their settings. Additional progress on this topic has been limited by a paucity of data on where people spend their time and the types of activities done in these different locations. This study sought to investigate how observers’ locations, actions, and goals influenced the scenes they categorized throughout the course of a month. We uncovered which actions or goals were most commonly associated with certain locations and vice versa. We did this by conducting a virtual study in the month of December 2020 involving ten participants residing in Maine with ages ranging from 20 to 53. Participants received ten daily text messages that asked them to specify their present environment or send a picture of their location, identify the action they were performing, and describe the goal they were trying to achieve. The results suggest that all participants spent the majority of their time inside. The data revealed that 90.9% of all locations participants reported were inside, 3.4% were in a car, and 5.7% were outside. There was a numerical tendency to spend more time indoors with increasing age (R = 0.06). Participants partook in 157 unique activities/goals. The most frequent activity reported was working, followed by relaxing, eating, cooking, and watching videos. The frequency of activities exhibits a power law distribution that obeys Zipf’s law. This study provides more insight into how humans categorize scenes, and the data helps identify which scenes are most commonly experienced by humans on a daily basis.
[What we don’t see in image databases]
The rise of large-scale image databases has accelerated productivity in both human and machine vision communities. Most extant databases were created in three phases: (1) Obtaining a comprehensive list of categories to sample; (2) Scraping images from the web; (3) Verifying category labels through crowdsourcing. Subtle biases can arise in each stage: offensive labels can get reified as categories; images represent what is typical of the internet, rather than what is typical of daily experience, and verification is dependent on the knowledge and cultural competence of the annotators that provide “ground truth” labels. Here, we describe two studies that examine the bias in extant visual databases and the deep neural networks trained from them. 66 observers took part in an experience sampling experiment via text message. Each received 10 messages per day at random intervals for 30 days, and sent a picture of their surroundings if possible (N=6280 images). Category predictions were obtained from CNNs pretrained on the Places database. The dCNNs showed poor classification performance for these images. A second study investigated cultural biases. We scraped images of private homes from Airbnb from 219 countries. Pre-trained deep neural networks were less accurate and less confident in recognizing images from the Global South. We observed significant correlations between dCNN confidence and GDP per capita (r=0.30) and literacy rate (r=0.29). These studies show a dissociation between lived visual content and web-based content, and suggest caution when using the internet as a proxy for visual experience.",,Michelle Greene,
49,1285,Mark,Ho,mho@princeton.edu,Princeton University,917-327-1820,Princeton,New Jersey,08544,United States,"[People construct simplified mental representations to plan] One of the most striking features of human cognition is the ability to plan. Two aspects of human planning stand out—its efficiency and flexibility. Efficiency is especially impressive because plans must often be made in complex environments, and yet people successfully plan solutions to many everyday problems despite having limited cognitive resources1,2,3. Standard accounts in psychology, economics and artificial intelligence have suggested that human planning succeeds because people have a complete representation of a task and then use heuristics to plan future actions in that representation4,5,6,7,8,9,10,11. However, this approach generally assumes that task representations are fixed. Here we propose that task representations can be controlled and that such control provides opportunities to quickly simplify problems and more easily reason about them. We propose a computational account of this simplification process and, in a series of preregistered behavioural experiments, show that it is subject to online cognitive control12,13,14 and that people optimally balance the complexity of a task representation and its utility for planning and acting. These results demonstrate how strategically perceiving and conceiving problems facilitates the effective use of limited cognitive resources.

[DreamCoder: bootstrapping inductive program synthesis with wake-sleep library learning] We present a system for inductive program synthesis called DreamCoder, which inputs a corpus of synthesis problems each specified by one or a few examples, and automatically derives a library of program components and a neural search policy that can be used to efficiently solve other similar synthesis problems. The library and search policy bootstrap each other iteratively through a variant of ""wake-sleep"" approximate Bayesian learning. A new refactoring algorithm based on E-graph matching identifies common sub-components across synthesized programs, building a progressively deepening library of abstractions capturing the structure of the input domain. We evaluate on eight domains including classic program synthesis areas and AI tasks such as planning, inverse graphics, and equation discovery. We show that jointly learning the library and neural search policy leads to solving more problems, and solving them more quickly.

[AN INTEGRATIVE THEORY OF PREFRONTAL CORTEX FUNCTION] The prefrontal cortex has long been suspected to play an important role in cognitive control, in the ability to orchestrate thought and action in accordance with internal goals. Its neural basis, however, has remained a mystery. Here, we propose that cognitive control stems from the active maintenance of patterns of activity in the prefrontal cortex that represent goals and the means to achieve them. They provide bias signals to other brain structures whose net effect is to guide the flow of activity along neural pathways that establish the proper mappings between inputs, internal
states, and outputs needed to perform a given task. We review neurophysiological, neurobiological, neuroimaging, and computational studies that support this theory and discuss its implications as well as further issues to be addressed.","Joseph Fins, Anne Collins, Constantin Rothkopf, Dominik Straub, Tomer Ullman, Erie Boorman","Ida Momennejad, Tom Griffiths, Fred Callaway, Sev Harootonian, ",
50,1342,Simon,Hofmann,simon.hofmann@cbs.mpg.de,Max Planck Institute for Human Cognitive & Brain Sciences,+49-341-9940-2243,Leipzig,Saxony,04103,Germany,"Brain-age (BA) estimates based on deep learning are increasingly used as neuroimaging biomarker for brain health; however, the underlying neural features have remained unclear. We combined ensembles of convolutional neural networks with Layer-wise Relevance Propagation (LRP) to detect which brain features contribute to BA. Trained on magnetic resonance imaging (MRI) data of a population-based study (n = 2637, 18–82 years), our models estimated age accurately based on single and multiple modalities, regionally restricted and whole-brain images (mean absolute errors 3.37–3.86 years). We find that BA estimates capture ageing at both small and large-scale changes, revealing gross enlargements of ventricles and subarachnoid spaces, as well as white matter lesions, and atrophies that appear throughout the brain. Divergence from expected ageing reflected cardiovascular risk factors and accelerated ageing was more pronounced in the frontal lobe. Applying LRP, our study demonstrates how superior deep learning models detect brain-ageing in healthy and at-risk individuals throughout adulthood.


Faces are socially relevant stimuli that can be distinguished by the spatial arrangements of their visual features. However, face perception has been mostly investigated with static 2D images, which differs from everyday life experience. In an online study, we investigate face perception in two viewing conditions (2D & 3D). We compare the cognitive face space for these conditions, by modeling the acquired human similarity ratings with similarity matrices computed from physical face attributes and feature maps of deep learning-based face recognition models. Lastly, we fit these models to the human similarity judgements to explore relevant facial features between the viewing conditions. Unveiling differences between 2D and 3D perception of faces will further our understanding on the role of stimulus presentation on face processing.

Immersive virtual reality (VR) enables naturalistic neuroscientific studies while main- taining experimental control, but dynamic and interactive stimuli pose methodological challenges. We here probed the link between emotional arousal, a fundamental property of affective expe- rience, and parieto-occipital alpha power under naturalistic stimulation: 37 young healthy adults completed an immersive VR experience, which included rollercoaster rides, while their EEG was recorded. They then continuously rated their subjective emotional arousal while viewing a replay
of their experience. The association between emotional arousal and parieto-occipital alpha power was tested and confirmed by (1) decomposing the continuous EEG signal while maximizing the comodulation between alpha power and arousal ratings and by (2) decoding periods of high and low arousal with discriminative common spatial patterns and a long short-term memory recurrent neural network. We successfully combine EEG and a naturalistic immersive VR experience to extend previous findings on the neurophysiology of emotional arousal towards real-world neuroscience.",,Martin Hebart,
51,1232,Laurence,Hunt,laurence.hunt@psych.ox.ac.uk,University of Oxford,+44-1865-618-346,Oxford,Oxfordshire,OX2 6GG,United Kingdom,"[Decision-making in dynamic, continuously evolving environments: quantifying the flexibility of human choice]

During perceptual decision-making tasks, centroparietal EEG potentials report an evidence accumulation-to-bound process that is time locked to trial onset. However, decisions in real-world environments are rarely confined to discrete trials; they instead unfold continuously, with accumulation of time-varying evidence being recency-weighted towards its immediate past. Confronted with time-varying stimuli, humans can appropriately adapt their weighting of recent evidence according to the statistics of the environment. The neural mechanisms supporting this adaptation currently remain unclear. Here, we show that humans’ ability to adapt evidence weighting to different sensory environments is reflected in changes in centroparietal EEG potentials. We use a novel continuous task design to show that the Centroparietal Positivity (CPP) becomes more sensitive to fluctuations in sensory evidence when large shifts in evidence are less frequent, and is primarily sensitive to fluctuations in decision-relevant (not decision-irrelevant) sensory input. A complementary triphasic component over parietal cortex encodes the sum of recently accumulated sensory evidence, and its magnitude covaries with the duration over which different individuals integrate sensory evidence. Our findings reveal how adaptations in centroparietal responses reflect flexibility in evidence accumulation to the statistics of dynamic sensory environments. 

[Formalizing planning and information search in naturalistic decision-making]

Decisions made by mammals and birds are often temporally extended. They require planning and sampling of decision-relevant information. Our understanding of such decision-making remains in its infancy compared with simpler, forced-choice paradigms. However, recent advances in algorithms supporting planning and information search provide a lens through which we can explain neural and behavioral data in these tasks. We review these advances to obtain a clearer understanding for why planning and curiosity originated in certain species but not others; how activity in the medial temporal lobe, prefrontal and cingulate cortices may support these behaviors; and how planning and information search may complement each other as means to improve future action selection.

[Temporal scaling of human scalp-recorded potentials]

Much of human behaviour is governed by common processes that unfold over varying timescales. Standard event-related potential analysis assumes fixed-latency responses relative to experimental events. However, recent single unit recordings in animals have revealed neural activity scales to span different durations during behaviours demanding flexible timing. Here, we employed a general linear modelling approach using a novel combination of fixed-duration and variable-duration regressors to unmix fixed-time and scaled-time components in human magneto/electroencephalography (M/EEG) data. We use this to reveal consistent temporal scaling of human scalp-recorded potentials across four independent EEG datasets, including interval perception, production, prediction and value-based decision making. Between-trial variation in the temporally scaled response predicts between-trial variation in subject reaction times, demonstrating the relevance of this temporally scaled signal for temporal variation in behaviour. Our results provide a general approach for studying flexibly timed behaviour in the human brain.","Evelina Fedorenko, Phillip Witkowski, Constantin Rothkopf, Dominik Straub, Moira Dillon, Mark Ho","Maria Ruesseler, David Redish, Malcolm MacIver, Nathaniel Daw, Kim Stachenfeld",49261338
52,1089,Hyeon-Ae,Jeon,jeonha@dgist.ac.kr,Daegu Gyeongbuk Institute of Science and Technology (DGIST),+82-10-3367-3321,Daegu,"Korea, Republic of",42988,Korea (South),"[Modality specificity and generality in the hierarchical levels of cognitive control]
The prefrontal cortex (PFC) is organized hierarchically along a posterior-to-anterior axis. This theoretical framework has been studied mostly using visual stimuli. Here, we investigated this functional organization of the PFC, particularly using multiple sensory information with multivoxel pattern analysis (MVPA) and functional connectivity (FC). We used two different sensory modalities—auditory cues and visual targets—to establish different levels of hierarchical processing. We found that the posterior-to-anterior pattern of activations along the precentral gyrus (PreCG), inferior frontal gyrus (IFG), and middle frontal gyrus (MFG) was observed as the level of hierarchy increased. Furthermore, MVPA results showed that the more anterior regions specifically encoded information for higher-level processing. More interestingly, sensory areas had stronger FC with the posterior region of the PFC than the anterior region. We suggest that the PFC has different functional associations with sensory modality depending on the levels of cognitive control.
[Different Brain Mechanisms of Time Estimation Depending on Situational Information]
Although we do not always measure time, we can estimate the passage of time based on our previous experience. However, it is unclear how the brain estimates the passage of time without explicit measures. We hypothesized that people use situational information to compensate for missing time. Using Bayesian hierarchical modeling and functional magnetic resonance imaging, we aimed to probe how our brain estimates time with/without situational information. As a result, the frontal lobe is actively involved in time estimation with situational information. The cerebellum and hippocampus were significantly activated in estimating time without situational information. We suggest that the frontal lobe plays a vital role in time estimation to control attentional modulation and time-based prospective memory with situational information. In contrast, the cerebellum and the hippocampus seem to act as an internal clock since these regions were involved in the relatively pure estimation of the time. ",,,https://www.semanticscholar.org/author/H.-Jeon/1945514
53,1377,Karim,Jerbi,karim.jerbi.udem@gmail.com,Université de Montréal,+1-438-969-2103,Montréal,Québec,H2V 2S9,Canada,"[Distinct trajectories in low-dimensional neural oscillation state space track dynamic decision-making in humans]
The brain evolved to govern behavior in a dynamic world, in which pertinent information about choices is often in flux. Thus, the commitment to an action choice must reflect a balance between monitoring that information and the necessity to act before opportunities are lost. Here, we investigate the mechanisms of dynamic decision-making in humans using low dimensional space representation of brain wide magnetoencephalography recordings. We show that the principal components (PCs) of alpha (9-13 Hz) and beta power (16-24 Hz) are involved in tracking sensory information evolving over time in the sensorimotor and visual cortex. We also found that alpha PCs reflect the commitment to a particular choice, while beta PCs reflect motor execution. Finally, higher frequency components in subcortical areas reflect the adjustment of speed-accuracy tradeoff policies. These results provide a new detailed characterization of the distributed oscillatory brain processes underlying dynamic decision-making in humans.

[Aperiodic brain activity and response to anesthesia vary in disorders of consciousness]
In the human electroencephalogram (EEG), oscillatory power peaks co-exist with non-oscillatory, aperiodic activity. Although EEG analysis has traditionally focused exclusively on oscillatory power, recent investigations have shown that the aperiodic EEG component can distinguish conscious wakefulness from sleep and anesthetic-induced unconsciousness (Lendner et al. 2020). This study investigates the aperiodic EEG component of individuals in a disorder of consciousness (DOC), and how it changes in response to exposure to anesthesia. High-density EEG was recorded from 43 individuals in a DOC. To measure the brain’s reaction to global perturbation, a subset of n = 16 were also exposed to a targeted infusion of propofol anesthesia. The aperiodic component was defined by the spectral slope and offset in the 1-45 Hz and 30-45 Hz range of the power spectral density. Brain network criticality and complexity were estimated using the pair correlation function (PCF) and Lempel-Ziv complexity, respectively. The level of responsiveness of all individuals in DOC was assessed using the Coma Recovery Scale-Revised (CRS-R). Recovery of consciousness was assessed three months post-EEG. At baseline, the EEG aperiodic component was more strongly correlated to the participants’ level of consciousness than the oscillatory component. Anesthesia caused a steepening of the spectral slope across participants. Importantly, the change in spectral slope positively correlated with the individual participant’s level of responsiveness. The spectral slope during exposure to anesthesia contained prognostic value for individuals with DOC. The anesthetic-induced change in aperiodic EEG was accompanied by loss of information-richness and a reduction in network criticality. The aperiodic EEG component in individuals with DOC has been historically neglected; this research highlights the importance of considering this measure for future research investigating brain mechanisms underlying consciousness.

[Linking human and artificial neural representations underlying face recognition: Insights from MEG and CNNs]
Mounting evidence suggests that biological and artificial neural networks trained on similar tasks can exhibit remarkable functional similarities. In particular, Convolutional Neural Networks (CNNs) trained on object recognition have been shown to learn representations that model the processing hierarchy in the human visual system. But what about the specific case of face recognition? Would CNNs trained on face recognition learn representations that capture the neural dynamics of the neural circuits that mediate face recognition? Here, we investigated the putative similarities between the representations learned by three CNN architectures trained on face recognition (FaceNet, ResNet50 and CORnet-S) and brain patterns assessed with magnetoencephalography (MEG) recorded during a face recognition tasks. We found that the neuromagnetic brain signals (especially in visual areas) was correlated with activations in multiple CNN layers. However, these correlations were not as strong as those observed in the more general task of object recognition. This may suggest that CNNs trained on face recognition capture only a small portion of the complexity of the real brain patterns associated with face recognition. Our results contribute to an emerging stream of research that seeks to probe face recognition mechanisms by joint exploration of the associated neural representations in both brains and machines.
",,,1714811
54,1194,Zuzanna,Kabulska,zuzanna.kabulska@ur.de,University of Regensburg,+49-152-5751-3214,Regensburg,Bayern,93057,Germany,"[Revealing the architecture underlying action representations]
In our work, we wanted to better understand how actions are represented in the brain. We selected 100 daily actions and investigated their representation on two levels: category- and feature-based levels. First, we performed several behavioral studies from which we derived two separate behavioral models, representing the category- and feature-based action organization. Next, we conducted an fMRI study and performed a representational similarity analysis (RSA) with the two behavioral models. Additionally, to account for semantic information as well as low-level visual features, we used in the analysis also a semantic model (created based on a language model BERT) and a low-level visual model (created from the outputs of an early layer of Resnet 50). The results showed the importance of occipitotemporal areas in high-level action understanding. Moreover, we show that different hemispheres show a preference for either category- or feature-based action information.

[Neural representation of action categories]
In this study, we were interested in how different action categories are represented in the brain. First, we were interested in whether specific brain regions show a preference for certain action categories. To this aim, we conducted univariate conjunction analysis and multivariate pattern analysis. In addition, we wanted to investigate whether it is possible to distinguish between different action categories based on connectivity patterns. For that purpose, we examined functional connectivity patterns between selected brain areas and compared these between different action categories.

[Visual imagery explained with a deep neural network]
In this project, we were interested in the neural basis of visual imagery. In an fMRI experiment, participants were asked to imagine two types of objects: animals or tools (16 separate types per category). Then, we used a deep neural network (VGG19), fed it with images of these animals and tools, and for each image, we got an output from each layer separately. From the outputs, we created representational dissimilarity matrices for each layer (size 32x32, equal to the number of pairwise comparisons between the images) and compared them with brain data.
","Kimberly Stachenfeld, Kelsey Allen, Jessica Hamrick, Deepak Pathak, Ida Momennejad, Evelina Fedorenko",Philipp Seidel,2135160470
55,1316,Ari,Kahn,arik@princeton.edu,Princeton University,925-285-9061,Jersey City,NJ,07302,United States,"Measuring Behavioral Arbitration of the Successor Representation
When faced with a multi-step decision problem, humans and animals must balance flexible and accurate decision making with computational complexity. One prominent approach takes advantage of temporal abstraction of fu- ture states: by learning to predict long-run future trajec- tory independently of rewards, the successor represen- tation (SR) can avoid the costs of full mental simulation, while retaining the ability to cheaply replan when goals change. Human behavior shows signatures of such tem- poral abstraction, but their trial-by-trial acquisition has not been elucidated and it remains an open question if reliance on such abstractions adapts to their usefulness, e.g. the predictability of long-run states. We developed a novel task to distinguish SR-based planning. Our results support findings that human behavior exhibits a mix of learning strategies, and crucially, we measure SR usage on a trial-by-trial basis. Further, by dynamically manipu- lating the task structure, we observe preliminary results suggesting that human reliance on temporal approxima- tions is arbitrated by future predictability.

Network structure influences the strength of learned neural representations
Human experience is built on sequences of discrete events. From those sequences, humans build impressively accurate models of their world. This process is commonly referred to as graph (or structure) learning because the mental model encodes the graph of event-to-event transition probabilities, typically in medial temporal cortex. Recent evidence suggests that some network structures are easier to learn than others, but the neural substrate for this effect remains unknown. Here we use fMRI to show that the network structure of a temporal sequence influences the fidelity of stimuli's neural representations. Healthy adult human participants learned a set of stimulus-motor associations following one of two graph structures. The design of our experiment allowed us to separate regional sensitivity to the structural, stimulus, and motor response components of the task. As expected, whereas the motor response could be decoded from neural representations in postcentral gyrus, the shape of the stimulus could be decoded from lateral occipital cortex. The structure of the graph impacted the nature of neural representations: when the graph was modular as opposed to lattice-like, BOLD representations in visual areas better predicted trial identity in a held-out run and displayed higher intrinsic dimensionality. Our results demonstrate that even over relatively short timescales, graph structure determines the fidelity of event representations as well as the dimensionality of the space in which those representations are encoded. More broadly, our study shows that network context influences the strength of learned neural representations, motivating future work in the design, optimization, and adaptation of network contexts for distinct types of learning over different timescales.

Abstract representations of events arise from mental errors in learning and memory
Humans are adept at uncovering abstract associations in the world around them, yet the underlying mechanisms remain poorly understood. Intuitively, learning the higher-order structure of statistical relationships should involve complex mental processes. Here we propose an alternative perspective: that higher-order associations instead arise from natural errors in learning and memory. Using the free energy principle, which bridges information theory and Bayesian inference, we derive a maximum entropy model of people’s internal representations of the transitions between stimuli. Importantly, our model (i) affords a concise analytic form, (ii) qualitatively explains the effects of transition network structure on human expectations, and (iii) quantitatively predicts human reaction times in probabilistic sequential motor tasks. Together, these results suggest that mental errors influence our abstract representations of the world in significant and predictable ways, with direct implications for the study and design of optimally learnable information sources.",,"Mark Ho, Sreejan Kumar, Sevan Harootonian",153396864
56,1236,Karthika,Kamath,karthika.kamath@fractal.ai,Fractal Analytics,+91-98-6917-5254,Mumbai,Maharashtra,400094,India,"[Pleasure Systems in the Brain] - Pleasure is mediated by well-developed mesocorticolimbic circuitry and serves adaptive functions. In affective disorders, anhedonia (lack of pleasure) or dysphoria (negative affect) can result from breakdowns of that hedonic system. Human neuroimaging studies indicate that surprisingly similar circuitry is activated by quite diverse pleasures, suggesting a common neural currency shared by all. Wanting for reward is generated by a large and distributed brain system. Liking, or pleasure itself, is generated by a smaller set of hedonic hot spots within limbic circuitry. Those hot spots also can be embedded in broader anatomical patterns of valence organization, such as in a keyboard pattern of nucleus accumbens generators for desire versus dread. In contrast, some of the best known textbook candidates for pleasure generators, including classic pleasure electrodes and the mesolimbic dopamine system, may not generate pleasure after all. These emerging insights into brain pleasure mechanisms may eventually facilitate better treatments for affective disorders.

[Model-based and model-free Pavlovian reward learning] - Revaluation, revision, and revelation] - Evidence supports at least two methods for learning about reward and punishment and making predictions for guiding actions. One method, called model-free, progressively acquires cached estimates of the long-run values of circum-stances and actions from retrospective experience. The other method, called model-based, uses representations of the environment, expectations, and prospective calculations to make cognitive predictions of future value. Extensive attention has been paid to both methods in computational analyses of instrumental learning. By contrast, although a full computational analysis has been lacking, Pavlovian learning and pre-diction has typically been presumed to be solely model-free. Here, we revise that presumption and review compelling evidence from Pavlovian revaluation experiments showing that Pavlovian predictions can involve their own form of model-based evaluation. In model-based Pavlovian evaluation, prevailing states of the body and brain influence value computations, and thereby produce powerful incentive motivations that can sometimes be quite new. We consider the consequences of this revised Pavlovian view for the computational landscape of prediction, response, and choice. We also revisit differences between Pavlovian and instrumental learning in the control of incentive motivation.

[Magnetoencephalographic Signals Identify Stages in Real-Life Decision Processes] - We used magnetoencephalography (MEG) to study the dynamics of neural responses in eight subjects engaged in shopping for day-today items from supermarket shelves. This behavior not only has personal and economic importance but also provides an example of an experience that is both personal and shared between individuals. The shopping experience enables the exploration of neural mechanisms underlying choice based on complex memories. Choosing among different brands of closely related products activated a robust sequence of signals within the first second after the presentation of the choice images. This sequence engaged first the visual cortex (80-100 ms), then as the images were analyzed, predominantly the left temporal regions (310-340 ms). At longer latency, characteristic neural activation was found in motor speech areas (500-520 ms) for images requiring low salience choices with respect to previous (brand) memory, and in right parietal cortex for high salience choices (850-920 ms). We argue that the neural processes associated with the particular brand choice stimulus can be separated into identifiable stages through observation of MEG responses and knowledge of functional anatomy.",,,
57,1061,Raphael,Kaplan,kaplan@uji.es,Universitat Jaume I,847-668-9844,Castellón de la Plana,Comunidad Valenciana,12071,Spain,"An immersive first-person navigation task for abstract knowledge acquisition

Advances in virtual reality (VR) technology have greatly benefited spatial navigation research. By presenting space in a controlled manner, changing aspects of the environment one at a time or manipulating the gain from different sensory inputs, the mechanisms underlying spatial behaviour can be investigated. In parallel, a growing body of evidence suggests that the processes involved in spatial navigation extend to non-spatial domains. Here, we leverage VR technology advances to test whether participants can navigate abstract knowledge. We designed a two-dimensional quantity space-presented using a head-mounted display-to test if participants can navigate abstract knowledge using a first-person perspective navigation paradigm. To investigate the effect of physical movement, we divided participants into two groups: one walking and rotating on a motion platform, the other group using a gamepad to move through the abstract space. We found that both groups learned to navigate using a first-person perspective and formed accurate representations of the abstract space. Interestingly, navigation in the quantity space resembled behavioural patterns observed in navigation studies using environments with natural visuospatial cues. Notably, both groups demonstrated similar patterns of learning. Taken together, these results imply that both self-movement and remote exploration can be used to learn the relational mapping between abstract stimuli.

Transforming Social Perspectives with Cognitive Maps

Growing evidence suggests that cognitive maps represent relations between social knowledge similar to how spatial locations are represented in an environment. Notably, the extant human medial temporal lobe literature assumes associations between social stimuli follow a linear associative mapping from an egocentric viewpoint to a cognitive map. Yet, this form of associative social memory doesn't account for a core phenomenon of social interactions in which social knowledge learned via comparisons to the self, other individuals, or social networks are assimilated within a single frame of reference. We argue that hippocampal-entorhinal coordinate transformations, known to integrate egocentric and allocentric spatial cues, inform social perspective switching between the self and others. We present evidence that the hippocampal formation helps inform social interactions by relating self versus other social attribute comparisons to society in general, which can afford rapid and flexible assimilation of knowledge about the relationship between the self and social networks of varying proximities. We conclude by discussing the ramifications of cognitive maps in aiding this social perspective transformation process in states of health and disease.

The Role of Cognitive Maps in Decision-Making

A growing body of work is investigating the use of cognitive maps during decision-making. Here we discuss how decision-making organizes experiences according to an internal model of the current task, thereby structuring memory. Likewise, we consider how the structure of cognitive maps contributes to decision-making.",,"psychophysics, computational vision",https://www.semanticscholar.org/author/Raphael-Kaplan/49333014
58,1238,Kendrick,Kay,kay@umn.edu,University of Minnesota,510-206-1059,St. Paul,MN,55114,United States,"[A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence] Extensive sampling of neural activity during rich cognitive phenomena is critical for robust understanding of brain function. Here we present the Natural Scenes Dataset (NSD), in which high-resolution functional magnetic resonance imaging responses to tens of thousands of richly annotated natural scenes were measured while participants performed a continuous recognition task. To optimize data quality, we developed and applied novel estimation and denoising techniques. Simple visual inspec- tions of the NSD data reveal clear representational transformations along the ventral visual pathway. Further exemplifying the inferential power of the dataset, we used NSD to build and train deep neural network models that predict brain activity more accurately than state-of-the-art models from computer vision. NSD also includes substantial resting-state and diffusion data, enabling network neuroscience perspectives to constrain and enhance models of perception and memory. Given its unprec- edented scale, quality and breadth, NSD opens new avenues of inquiry in cognitive neuroscience and artificial intelligence.

[Understanding multivariate brain activity: Evaluating the effect of voxelwise noise correlations on population codes in functional magnetic resonance imaging] Previous studies in neurophysiology have shown that neurons exhibit trial-by-trial correlated activity and that such noise correlations (NCs) greatly impact the accuracy of population codes. Meanwhile, multivariate pattern analysis (MVPA) has become a mainstream approach in functional magnetic resonance imaging (fMRI), but it remains unclear how NCs between voxels influence MVPA performance. Here, we tackle this issue by combining voxel-encoding modeling and MVPA. We focus on a well-established form of NC, tuning- compatible noise correlation (TCNC), whose sign and magnitude are systematically related to the tuning similarity between two units. We show that this form of voxelwise NCs can improve MVPA performance if NCs are sufficiently strong. We also confirm these results using standard information-theoretic analyses in computational neuroscience. In the same theoretical framework, we further demonstrate that the effects of noise correlations at both the neuronal level and the voxel level may manifest differently in typical fMRI data, and their effects are modulated by tuning heterogeneity. Our results provide a theoretical foundation to understand the effect of correlated activity on population codes in macroscopic fMRI data. Our results also suggest that future fMRI research could benefit from a closer exami- nation of the correlational structure of multivariate responses, which is not directly revealed by conventional MVPA approaches.

[Principles for models of neural information processing] The goal of cognitive neuroscience is to understand how mental operations are performed by the brain. Given the complexity of the brain, this is a challenging endeavor that requires the development of formal models. Here, I provide a perspective on models of neural information processing in cognitive neuroscience. I define what these models are, explain why they are useful, and specify criteria for evaluating models. I also highlight the difference between functional and mechanistic models, and call attention to the value that neuroanatomy has for understanding brain function. Based on the principles I propose, I proceed to evaluate the merit of recently touted deep neural network models. I contend that these models are promising, but substantial work is necessary (i) to clarify what type of explanation these models provide, (ii) to determine what specific effects they accurately explain, and (iii) to improve our understanding of how they work.",,"Jacob Prince, Colin Conwell, Talia Konkle, Thomas Naselaris, Ian Charest, Dawn Finzi, Niko Kriegeskorte",1912661
59,1170,Atlas,Kazemian,atlaskazemian@gmail.com,The Johns Hopkins University,+1-604-781-1035,Baltimore,Maryland,21218,United States,"[Do We Need Deep Learning? Towards High-Performance Encoding Models of Visual Cortex Using Modules of Canonical Computations] The field of computational neuroscience has witnessed a surge of interest in convolutional neural networks (CNNs) trained through deep learning, following the finding that they can recapitulate the representation of visual information along the ventral stream. This has led to the routine use of CNNs as standard encoding models of visual cortex, despite limitations such as large dependency on training data and low interpretability. Here, we propose an alternative approach that addresses such limitations without sacrificing performance. We introduce a family of hand-engineered models that combine the convolution operation of CNNs with the right set of canonical neural computations, resulting in a model that requires little to no training. We present one such architecture and compare its encoding performance to a standard CNN by linearly mapping each model’s features to fMRI responses. We show that, with no learning involved, the performance of the hand-engineered model competes with the trained CNN for predicting object-evoked fMRI responses in high-level visual cortex. Our approach opens up the possibility of designing high-performance encoding models without relying on deep learning, with the potential of revealing critical inductive biases and computational efficiencies of the visual cortex.","James Whittington, Kimberly Stachenfled, Surya Ganguli, Jane X. Wang, Ida Momennejad ","Mick Bonner, Leyla Isik, Paul Solous, Angira Shirahatti, Ray Chen",
60,1126,Jin,Ke,jinke@uchicago.edu,University of Chicago,312-998-5025,Chicago,Illinois,60637,United States,"1. A Connectome-based Predictive Model of Emotion during Naturalistic Viewing

Abstract: Our thoughts and actions are guided by our ongoing affective experience. Affective states are often measured using self-report ratings, which are labor intensive to collect and can also disrupt ongoing cognition if obtained while performing a task. In this study, we aim to 1) derive a continuous and non-intrusive measure of affective experience based on dynamic functional connectivity (FC), and 2) characterize the interaction between brain regions underlying changes in affective states. We trained a connectome-based predictive model to predict subjective ratings of valence, arousal and dominance from fMRI data of participants watching a TV episode. All three models achieved reasonable accuracy (valence: r = .486, p = .018; arousal: r = .519, p = .002; dominance: r = .602, p = .008). FC edges within and between multiple large-scale functional brain networks reliably contributed to model predictions, suggesting that affective states are encoded in the interactions between brain regions. Taken together, our work presents a promising approach to probe affective experience based on brain imaging data.

2. Affective Experience Predicts Narrative Engagement during Naturalistic Viewing

Abstract: People are more engaged in a narrative during emotional moments. However, we lack a fine-grained understanding of the type of emotions that drive narrative engagement. Here, we used a connectome-based predictive model to predict narrative engagement from dynamic functional connectivity as participants watched an episode of the TV show Friday Night Lights (FNL). Correlation between dynamic intersubject correlation (dynamic ISC) and engagement revealed that areas in the dorsal attention network, ventral attention network, and control network were more synchronized across participants during moments of high engagement. We then used a ridge regression model to investigate the relationship between narrative engagement and the affective experience of 16 emotions. Our results suggest that people are more engaged in a narrative when they are subjectively experiencing high arousal negative emotions.","Leyla Isik, Christopher Baldassano, Kenneth Norman, Luke Chang, Lila Davachi","Topics interested in: cognitive neuroscience, naturalistic viewing, emotion, connectome-based predictive model ",
61,1321,Anirudha,Kemtur,anirudha.kemtur@gmail.com,Université de Montréal,+1-438-808-1856,Montréal,Québec,H3T 1L5,Canada,"[Using deep reinforcement learning to reveal how the brain encodes abstract state-space representations in high-dimensional environments] Humans possess an exceptional aptitude to efficiently make decisions from high-dimensional sensory observations. However, it is unknown how the brain compactly represents the current state of the environment to guide this process. The deep Q-network (DQN) achieves this by capturing highly nonlinear mappings from multivariate inputs to the values of potential actions. We deployed DQN as a model of brain activity and behavior in participants playing three Atari video games during fMRI. Hidden layers of DQN exhibited a striking resemblance to voxel activity in a distributed sensorimotor network, extending throughout the dorsal visual pathway into posterior parietal cortex. Neural state-space representations emerged from nonlinear transformations of the pixel space bridging perception to action and reward. These transformations reshape axes to reflect relevant high-level features and strip away information about task-irrelevant sensory features. Our findings shed light on the neural encoding of task representations for decision-making in real-world situations. [Neuro-Nav: A Library for Neurally-Plausible Reinforcement Learning] In this work we propose Neuro-Nav, an open-source library for neurally plausible reinforcement learning (RL). RL is among the most common modeling frameworks for studying decision making, learning, and navigation in biological organisms. In utilizing RL, cognitive scientists often handcraft environments and agents to meet the needs of their particular studies. On the other hand, artificial intelligence researchers often struggle to find benchmarks for neurally and biologically plausible representation and behavior (e.g., in decision making or navigation). In order to streamline this process across both fields with transparency and reproducibility, Neuro-Nav offers a set of standardized environments and RL algorithms drawn from canonical behavioral and neural studies in rodents and humans. We demonstrate that the toolkit replicates relevant findings from a number of studies across both cognitive science and RL literatures. We furthermore describe ways in which the library can be extended with novel algorithms (including deep RL) and environments to address future research needs of the field. [Brains and algorithms partially converge in natural language processing] Deep learning algorithms trained to predict masked words from large amount of text have recently been shown to generate activations similar to those of the human brain. However, what drives this similarity remains currently unknown. Here, we systematically compare a variety of deep language models to identify the computational principles that lead them to generate brain-like representations of sentences. Specifically, we analyze the brain responses to 400 isolated sentences in a large cohort of 102 subjects, each recorded for two hours with functional magnetic resonance imaging (fMRI) and magnetoencephalography (MEG). We then test where and when each of these algorithms maps onto the brain responses. Finally, we estimate how the architecture, training, and performance of these models independently account for the generation of brain-like representations. Our analyses reveal two main findings. First, the similarity between the algorithms and the brain primarily depends on their ability to predict words from context. Second, this similarity reveals the rise and maintenance of perceptual, lexical, and compositional representations within each cortical region. Overall, this study shows that modern language algorithms partially converge towards brain-like solutions, and thus delineates a promising path to unravel the foundations of natural language processing.","""Arthur Julani, Ida Momennejad, Patrick Mineault, Jean-Remi King""",,
62,1175,Insub,Kim,insubkim@stanford.edu,Stanford University,650-788-9954,Stanford,California,94305,United States,"[Neural representations of perceptual color experience in the human ventral visual pathway]
Color is a perceptual construct that arises from neural processing in hierarchically organized cortical visual areas. Previous research, however, often failed to distinguish between neural responses driven by stimulus chromaticity versus perceptual color experience. An unsolved question is whether the neural responses at each stage of cortical processing represent a physical stimulus or a color we see. The present study dissociated the perceptual domain of color experience from the physical domain of chromatic stimulation at each stage of cortical processing by using a switch rivalry paradigm that caused the color percept to vary over time without changing the retinal stimulation. Using functional MRI (fMRI) and a model-based encoding approach, we found that neural representations in higher visual areas, such as V4 and VO1, corresponded to the perceived color, whereas responses in early visual areas V1 and V2 were modulated by the chromatic light stimulus rather than color perception. Our findings support a transition in the ascending human ventral visual pathway, from a representation of the chromatic stimulus at the retina in early visual areas to responses that correspond to perceptually experienced colors in higher visual areas.

[A New Computational Framework for Estimating Spatiotemporal Population Receptive Fields in Human Visual Cortex]
Visual cortex process stimulus information over space and time. Conventional population receptive field (pRF) method successfully mapped spatial receptive field sizes across the visual cortex. However, characteristics of temporal receptive field for each visual area at a voxel-level have not been fully investigated. Here, we created a compressive spatiotemporal (CST) pRF model that simultaneously estimates spatial and temporal receptive field properties across the human visual cortex. To ensure computational validity and reproducibility, we developed a simulation software that systematically tests the performance of the model. Through simulation, we found that the ground-truth spatiotemporal parameters were accurately recovered from the simulated BOLD timeseries. Using fMRI, we found that (i) the CST model had higher accuracy than the conventional pRF model in all tested visual areas (V1, V2, V3, and hV4). (ii) Across the ventral visual hierarchy, we found progressively decreasing contribution of sustained responses while the contribution of transient responses increased. (iii) We found larger temporal windows for the central eccentricities compared to the peripheral eccentricities. (iv) Analogous to the growth in the size of spatial receptive field, we found progressively larger temporal receptive field sizes in higher visual areas compared to earlier visual areas.

[Childhood maltreatment is associated with increased neural response to ambiguous threatening facial expressions in adulthood: Evidence from the late positive potential]
Childhood maltreatment increases lifetime vulnerability for psychopathology. One proposed mechanism for this association is that early maltreatment increases vigilance for and attention to subtle threat cues, persisting outside of the environment in which maltreatment occurs. To test this possibility, the present study examined neural responses to ambiguous and nonambiguous threatening facial expressions in a sample of 25 adults reporting a history of low-to-moderate levels of abuse in childhood and 46 reporting no or low levels of childhood maltreatment. The measure of neural response used was the late positive potential (LPP), a neural marker of sustained attention to motivationally salient information that is sensitive to subtle variation in emotional content. Participants passively viewed angry–neutral and fearful–neutral face blends and rated emotional intensity for each face. In the maltreated group, as fearful faces increased in emotional intensity, the LPP similarly increased, suggesting increased sensitivity to subtle variation in threatening content. Moreover, the LPP at each level of emotional intensity was not related to current symptoms of anxiety and depression. However, contrary to our hypotheses, adults with a history of abuse did not rate angry or fearful faces as more threatening, nor did they exhibit a larger LPP to angry faces, compared to controls. These findings suggest that childhood maltreatment may be associated with increased sensitivity to ambiguous threatening information in adulthood.
",,"Eline Kupers, Dawn Finzi",
63,1387,Joonhwa,Kim,joonhwa_kim@brown.edu,Brown University,315-632-8136,Providence,Rhode Island,02906,United States,"[Approximate Bayesian Inference captures differential effects of value confidence on obligatory and voluntary choices]

People weigh their options differently depending on (a) how confident they are in their valuation of each, and (b) whether they must select one option over another. To explore the interaction between the two, we used a paradigm in which following initial obligatory choices, participants made subsequent voluntary choices in which they could choose (or choose not to choose) more additional items from the set. We tested how the value a person assigned to each option interacted with their confidence in those values to shape initial and subsequent choices. When their confidence in the values was higher, participants’ initial choices were faster and more accurate and their subsequent choices were more sensitive to the options’ values. We show that these effects can be captured by a modified leaky competing accumulator model that approximates Bayesian inference and accounts for the obligatory vs. voluntary nature of the choice.","Laurence Hunt, Tom Griffiths, Matt Nassar, Yael Niv, Sam Gershman, Anne Collins","Romy Froemer, Jason Leng, Niloufar Razmi",
64,1311,Misun,Kim,mkim@cbs.mpg.de,Max Planck Institute for Human Cognitive and Brain Sciences,034199402487,Leipzig,,04103,Germany,"[Locally Euclidean cognitive maps for a spherical surface]
Humans can build cognitive maps of the world. Whether people can adapt and build a map of a novel environment in which they can no longer rely on the familiar Euclidean geometrical intuition is an intriguing question that can help us to understand the capacity and limitation of human cognition. Here, we conducted an object-location memory test and a path integration task on the spherical and planar surface with immersive virtual reality (VR). Participants could recall the object location well above the chance on both planar and sphere environment, but the direction error was particularly large for the sphere condition when the target was farther away. This result is in line with the locally planar maps hypothesis. Participants also showed systematic overturn bias on the sphere, as a consequence of following the Euclidean geometrical rule on the spherical surface. Our findings showed the capacity of building a cognitive map for a non-flat surface with a strong Euclidean prior.
[Adaptive cognitive maps for curved surfaces in the 3D world]
Terrains in a 3D world can be undulating. Yet, most prior research has exclusively investigated spatial representations on a flat surface, leaving a 2D cognitive map as the dominant model in the field. Here, we investigated whether humans represent a curved surface by building a dimension-reduced flattened 2D map or a full 3D map. Participants learned the location of objects positioned on a flat and curved surface in a virtual environment by driving on the concave side of the surface (Experiment 1), driving and looking vertically (Experiment 2), or flying (Experiment 3). Subsequently, they were asked to retrieve either the path distance or the 3D Euclidean distance between the objects. Path distance estimation was good overall, but we found a significant underestimation bias for the path distance on the curve, suggesting an influence of potential 3D shortcuts, even though participants were only driving on the surface. Euclidean distance estimation was better when participants were exposed more to the global 3D structure of the environment by looking and flying. These results suggest that the representation of the 2D manifold, embedded in a 3D world, is neither purely 2D nor 3D. Rather, it is flexible and dependent on the behavioral experience and demand.
[Three-dimensional space representations in the human brain]
Brain structures that support spatial cognition by encoding one’s position and direction have been extensively studied for decades. In the majority of studies, neural substrates have been investigated on a horizontal two-dimensional plane, whereas humans and other animals also move vertically in a three-dimensional (3D) world. In this thesis, I investigated how 3D spatial information is represented in the human brain using functional MRI experiments and custom-built 3D virtual environments. In the first experiment, participants moved on flat, tilted-up or tilted-down pathways in a 3D lattice structure. Multivoxel pattern analysis revealed that the anterior hippocampus expressed 3D location information that was similarly sensitive to the vertical and horizontal axes. The retrosplenial cortex and posterior hippocampus represented direction information that was only sensitive to the vertical axis. In the second experiment, participants moved in a virtual building with multiple levels and rooms. Using an fMRI repetition suppression analysis, I observed a hierarchical representation of this 3D space, with anterior hippocampus representing local information within a room, while retrosplenial cortex, parahippocampal cortex and posterior hippocampus represented room information within the wider building. As in the first experiment, vertical and horizontal location information was similarly encoded. In the last experiments, participants were placed into a virtual zero-gravity environment where they could move freely along all 3 axes. The thalamus and subiculum expressed horizontal heading information, whereas retrosplenial cortex showed dominant encoding of vertical heading. Using novel fMRI analyses, I also found preliminary evidence of a 3D grid code in the entorhinal cortex. Overall, these experiments demonstrate the capacity of the human brain to implement a flexible and efficient representation of 3D space. The work in this thesis will, I hope, serve as a stepping-stone in our understanding of how we navigate in the real – 3D – world.
","Moira Dillon, Kaushik Lakshminarasimhan, Dora Angelaki",Seongmin Park,
65,1329,Tamás,Kovács,kovacs.tamas.barnabas@gmail.com,Central European University,+36-30-256-9794,Budapest,,1051,Hungary,"[Cognitive Tomography Reveals Complex, Task-Independent Mental Representations]

Humans develop rich mental representations that guide their behavior in a variety of everyday tasks. However, it is unknown whether these representations, often formalized as priors in Bayesian inference, are specific for each task or subserve multiple tasks. Current approaches cannot distinguish between these two possibilities because they cannot extract comparable representations across different tasks. Here, we develop a novel method, termed cognitive tomography, that can extract complex, multidimensional priors across tasks. We apply this method to human judgments in two qualitatively different tasks, “familiarity” and “odd one out,” involving an ecologically relevant set of stimuli, human faces. We show that priors over faces are structurally complex and vary dramatically across subjects, but are invariant across the tasks within each subject. The priors we extract from each task allow us to predict with high precision the behavior of subjects for novel stimuli both in the same task as well as in the other task. Our results provide the first evidence for a single high-dimensional structured representation of a naturalistic stimulus set that guides behavior in multiple tasks. Moreover, the representations estimated by cognitive tomography can provide independent, behavior-based regressors for elucidating the neural correlates of complex naturalistic priors.

[Mind Reading by Machine Learning: A doubly Bayesian Method for Inferring Mental Representations]

A central challenge in cognitive science is to measure and quantify the mental representations humans develop in other words, to 'read' subject's minds. In order to eliminate potential biases in reporting mental contents due to verbal elaboration, subjects' responses in experiments are often limited to binary decisions or discrete choices that do not require conscious reflection upon their mental contents. However, it is unclear what such impoverished data can tell us about the potential richness and dynamics of subjects' mental representations. To address this problem, we used ideal observer models that formalise choice behaviour as (quasi-)Bayes-optimal, given subjects' representations in long-term memory, acquired through prior learning, and the stimuli currently available to them. Bayesian inversion of such ideal observer models allowed us to infer subjects' mental representation from their choice behaviour in a variety of psychophysical tasks. The inferred mental representations also allowed us to predict future choices of subjects with reasonable accuracy, even in tasks that were different from those in which the representations were estimated. These results demonstrate a significant potential in standard binary decision tasks to recover detailed information about subjects' mental representations.

[A Switching Observer for Human Perceptual Estimation]

Human perceptual inference has been fruitfully characterized as a normative Bayesian process in which sensory evidence and priors are multiplicatively combined to form posteriors from which sensory estimates can be optimally read out. We tested whether this basic Bayesian framework could explain human subjects’ behavior in two estimation tasks in which we varied the strength of sensory evidence (motion coherence or contrast) and priors (set of directions or orientations). We found that despite excellent agreement of estimates mean and variability with a Basic Bayesian observer model, the estimate distributions were bimodal with unpredicted modes near the prior and the likelihood. We developed a model that switched between prior and sensory evidence rather than integrating the two, which better explained the data than the Basic and several other Bayesian observers. Our data suggest that humans can approximate Bayesian optimality with a switching heuristic that forgoes multiplicative combination of priors and likelihoods.",,,
66,1400,Yash,Kshirsagar,ykshirsagar@ski.org,Smith-Kettlewell Eye Research Institute,415-345-2067,San Francisco,CA,94115,United States,"How does the auditory system categorize natural sounds? Here we apply multimodal neuroimaging to illustrate the progression from acoustic to semantically dominated representations. Combining magnetoencephalographic (MEG) and functional magnetic resonance imaging (fMRI) scans of observers listening to naturalistic sounds, we found superior temporal responses beginning ~55 ms post-stimulus onset, spreading to extratemporal cortices by ~100 ms. Early regions were distinguished less by onset/peak latency than by functional properties and overall temporal response profiles. Early acoustically-dominated representations trended systematically toward category dominance over time (after ~200ms) and space (beyond primary cortex). Semantic category representation was spatially specific: Vocalizations were preferentially distinguished in frontotemporal voice-selective regions and the fusiform; scenes and objects were distinguished in parahippocampal and medial place areas. Our results are consistent with real-world events coded via an extended auditory processing hierarchy, in which acoustic representations rapidly enter multiple streams specialized by category, including areas typically considered visual cortex.

We present a device that combines principles of ultrasonic echolocation and spatial hearing to provide human users with environmental cues that are i) not otherwise available to the human auditory system and ii) richer in object and spatial information than the more heavily processed sonar cues of other assistive devices. The device consists of a wearable headset with an ultrasonic emitter and stereo microphones with affixed artificial pinnae. The goal of this study is to describe the device and evaluate the utility of the echoic information it provides. Methods: The echoes of ultrasonic pulses were recorded and time-stretched to lower their frequencies into the human auditory range, then played back to the user. We tested performance among naive and experienced sighted volunteers using a set of localization experiments in which the locations of echo-reflective surfaces were judged using these time-stretched echoes. Results: Naive subjects were able to make laterality and distance judgments, suggesting that the echoes provide innately useful information without prior training. Naive subjects were generally unable to make elevation judgments from recorded echoes. However trained subjects demonstrated an ability to judge elevation as well. Conclusion: This suggests that the device can be used effectively to examine the environment and that the human auditory system can rapidly adapt to these artificial echolocation cues. Significance: Interpreting and interacting with the external world constitutes a major challenge for persons who are blind or visually impaired. This device has the potential to aid blind people in interacting with their environment.

Any cognitive function is mediated by a network of many cortical sites whose activity is orchestrated through complex temporal dynamics. To understand cognition, we need to identify brain responses simultaneously in space and time. Here we present a technique that does this by linking multivariate response patterns of the human brain recorded with functional magnetic resonance imaging (fMRI) and with magneto- or electroencephalography (M/EEG) based on representational similarity. We present the rationale and current applications of this non-invasive analysis technique, termed M/EEG-fMRI fusion, and discuss its pros and cons. We highlight its wide applicability in cognitive neuroscience and how its openness to further development and extension gives it strong potential for a deeper understanding of cognition in the future.",,,
67,1177,Kaustubh,Kulkarni,kaustubh.kulkarni@icahn.mssm.edu,Icahn School of Medicine at Mount Sinai,732-688-4990,New York,NY,10029,United States,"[A neuroeconomic signature of opioid craving: How fluctuations in craving bias drug-related and nondrug-related value] How does craving bias decisions to pursue drugs over other valuable, and healthier, alternatives in addiction? To address this question, we measured the in-the-moment economic decisions of people with opioid use disorder as they experienced craving, shortly after receiving their scheduled opioid maintenance medication and ~24 h later. We found that higher cravers had higher drug-related valuation, and that moments of higher craving within-person also led to higher drug-related valuation. When experiencing increased opioid craving, participants were willing to pay more for personalized consumer items and foods more closely related to their drug use, but not for alternative “nondrug-related” but equally desirable options. This selective increase in value with craving was greater when the drug-related options were offered in higher quantities and was separable from the effects of other fluctuating psychological states like negative mood. These findings suggest that craving narrows and focuses economic motivation toward the object of craving by selectively and multiplicatively amplifying perceived value along a “drug relatedness” dimension.

[Computational psychiatry as a bridge from neuroscience to clinical applications] Translating advances in neuroscience into benefits for patients with mental illness presents enormous challenges because it involves both the most complex organ, the brain, and its interaction with a similarly complex environment. Dealing with such complexities demands powerful techniques. Computational psychiatry combines multiple levels and types of computation with multiple types of data in an effort to improve understanding, prediction and treatment of mental illness. Computational psychiatry, broadly defined, encompasses two complementary approaches: data driven and theory driven. Data-driven approaches apply machine-learning methods to high-dimensional data to improve classification of disease, predict treatment outcomes or improve treatment selection. These approaches are generally agnostic as to the underlying mechanisms. Theory-driven approaches, in contrast, use models that instantiate prior knowledge of, or explicit hypotheses about, such mechanisms, possibly at multiple levels of analysis and abstraction. We review recent advances in both approaches, with an emphasis on clinical applications, and highlight the utility of combining them.

[Believing in dopamine] Midbrain dopamine signals are widely thought to report reward prediction errors that drive learning in the basal ganglia. However, dopamine has also been implicated in various probabilistic computations, such as encoding uncertainty and controlling exploration. Here, we show how these different facets of dopamine signalling can be brought together under a common reinforcement learning framework. The key idea is that multiple sources of uncertainty impinge on reinforcement learning computations: uncertainty about the state of the environment, the parameters of the value function and the optimal action policy. Each of these sources plays a distinct role in the prefrontal cortex–basal ganglia circuit for reinforcement learning and is ultimately reflected in dopamine activity. The view that dopamine plays a central role in the encoding and updating of beliefs brings the classical prediction error theory into alignment with more recent theories of Bayesian reinforcement learning.","Samuel Gershman, Anna Konova, Lucina Uddin, Aaron Bornstein, Tobias Hauser, Robb Rutledge",,153135811
68,1208,Sreejan,Kumar,sreejank@princeton.edu,Princeton University,240-654-7956,Princeton,New Jersey,08540,United States,"[Reconstructing the cascade of language processing in the brain using the internal computations of a transformer-based language model]
Piecing together the meaning of a narrative requires understanding not only the individual words but also the intricate relationships between them. How does the brain construct this kind of rich, contextual meaning from natural language? Recently, a new class of artificial neural networks—based on the Transformer architecture—has revolutionized the field of language modeling. Transformers integrate information across words via multiple layers of structured circuit computations, forming increasingly contextualized representations of linguistic content. In this paper, we deconstruct these circuit computations and analyze the associated “transformations” (alongside the more commonly studied “embeddings”) at each layer to provide a fine-grained window onto linguistic computations in the human brain. Using functional MRI data acquired while participants listened to naturalistic spoken stories, we find that these transformations capture a hierarchy of linguistic computations across cortex, with transformations at later layers in the model mapping onto higher-level language areas in the brain. We then decompose these transformations into individual, functionally-specialized “attention heads” and demonstrate that the emergent syntactic computations performed by individual heads correlate with predictions of brain activity in specific cortical regions. These heads fall along gradients corresponding to different layers, contextual distances, and syntactic dependencies in a low-dimensional cortical space. Our findings provide a new basis for using the internal structure of large language models to better capture the cascade of cortical computations that support natural language comprehension.

[Disentangling Abstraction from Statistical Pattern Matching in Human and Machine Learning]
The ability to acquire abstract knowledge is a hallmark of human intelligence and is believed by many to be one of the core differences between humans and neural network models. Agents can be endowed with an inductive bias towards abstraction through meta-learning, where they are trained on a distribution of tasks that share some abstract structure that can be learned and applied. However, because neural networks are hard to interpret, it can be difficult to tell whether agents have learned the underlying abstraction, or alternatively statistical patterns that are characteristic of that abstraction. In this work, we compare the performance of humans and agents in a meta-reinforcement learning paradigm in which tasks are generated from abstract rules. We define a novel methodology for building ""task metamers"" that closely match the statistics of the abstract tasks but use a different underlying generative process, and evaluate performance on both abstract and metamer tasks. In our first set of experiments, we found that humans perform better at abstract tasks than metamer tasks whereas a widely-used meta-reinforcement learning agent performs worse on the abstract tasks than the matched metamers. In a second set of experiments, we base the tasks on abstractions derived directly from empirically identified human priors. We utilize the same procedure to generate corresponding metamer tasks, and see the same double dissociation between humans and agents. This work provides a foundation for characterizing differences between humans and machine learning that can be used in future work towards developing machines with human-like behavior.

[Using Natural Language and Program Abstractions to Instill Human Inductive Biases in Machines]
Strong inductive biases are a key component of human intelligence, allowing people to quickly learn a variety of tasks. Although meta-learning has emerged as an approach for endowing neural networks with useful inductive biases, agents trained by meta-learning may acquire very different strategies from humans. We show that co-training these agents on predicting representations from natural language task descriptions and from programs induced to generate such tasks guides them toward human-like inductive biases. Human-generated language descriptions and program induction with library learning both result in more human-like behavior in downstream meta-reinforcement learning agents than less abstract controls (synthetic language descriptions, program induction without library learning), suggesting that the abstraction supported by these representations is key. ",,,
69,1071,Eline,Kupers,ekupers@stanford.edu,Stanford University,646-464-2272,Stanford,CA,94305,United States,"[A population receptive field model of the magnetoencephalography response]
Computational models which predict the neurophysiological response from experimental stimuli have played an important role in human neuroimaging. One type of computational model, the population receptive field (pRF), has been used to describe cortical responses at the millimeter scale using functional magnetic resonance imaging (fMRI) and electrocorticography (ECoG). However, pRF models are not widely used for non-invasive electromagnetic field measurements (EEG/MEG), because individual sensors pool responses originating from several centimeter of cortex, containing neural populations with widely varying spatial tuning. Here, we introduce a forward-modeling approach in which pRFs estimated from fMRI data are used to predict MEG sensor responses. Subjects viewed contrast-reversing bar stimuli sweeping across the visual field in separate fMRI and MEG sessions. Individual subject’s pRFs were modeled on the cortical surface at the millimeter scale using the fMRI data. We then predicted cortical time series and projected these predictions to MEG sensors using a biophysical MEG forward model, accounting for the pooling across cortex. We compared the predicted MEG responses to observed visually evoked steady-state responses measured in the MEG session. We found that pRF parameters estimated by fMRI could explain a substantial fraction of the variance in steady-state MEG sensor responses (up to 60% in individual sensors). Control analyses in which we artificially perturbed either pRF size or pRF position reduced MEG prediction accuracy, indicating that MEG data are sensitive to pRF properties derived from fMRI. Our model provides a quantitative approach to link fMRI and MEG measurements, thereby enabling advances in our understanding of spatiotemporal dynamics in human visual field maps.
   ",,,
70,1306,Kaushik,Lakshminarasimhan,jl5649@columbia.edu,Columbia University,832-571-3182,New York,NY,10029,United States,"[Eye movements reveal spatiotemporal dynamics of visually-informed planning in navigation]
Goal-oriented navigation is widely understood to depend upon internal maps. Although this may be the case in many settings, humans tend to rely on vision in complex, unfamiliar environments. To study the nature of gaze during visually-guided navigation, we tasked humans to navigate to transiently visible goals in virtual mazes of varying levels of difficulty, observing that they took near-optimal trajectories in all arenas. By analyzing participants' eye movements, we gained insights into how they performed visually-informed planning. The spatial distribution of gaze revealed that environmental complexity mediated a striking trade-off in the extent to which attention was directed towards two complimentary aspects of the world model: the reward location and task-relevant transitions. The temporal evolution of gaze revealed rapid, sequential prospection of the future path, evocative of neural replay. These findings suggest that the spatiotemporal characteristics of gaze during navigation are significantly shaped by the unique cognitive computations underlying real-world, sequential decision making.

[Dynamical Latent State Computation in the Posterior Parietal Cortex]
Success in many real-world tasks depends on our ability to dynamically track hidden states of the world. To understand the underlying neural computations, we recorded brain activity in posterior parietal cortex (PPC) of monkeys navigating by optic flow to a hidden target location within a virtual environment, without explicit position cues. In addition to sequential neural dynamics and strong interneuronal interactions, we found that the hidden state – monkey’s displacement from the goal – was encoded in single neurons, and could be dynamically decoded from population activity. The decoded estimates predicted navigation performance on individual trials. Task manipulations that perturbed the world model induced substantial changes in neural interactions, and modified the neural representation of the hidden state, while representations of sensory and motor variables remained stable. The findings were recapitulated by a task-optimized recurrent neural network model, suggesting that neural interactions in PPC embody the world model to consolidate information and track task-relevant hidden states.

[A Dynamic Bayesian Observer Model Reveals Origins of Bias in Visual Path Integration]
Path integration is a strategy by which animals track their position by integrating their self-motion velocity. To identify the computational origins of bias in visual path integration, we asked human subjects to navigate in a virtual environment using optic flow and found that they generally traveled beyond the goal location. Such a behavior could stem from leaky integration of unbiased self-motion velocity estimates or from a prior expectation favoring slower speeds that causes velocity underestimation. Testing both alternatives using a probabilistic framework that maximizes expected reward, we found that subjects' biases were better explained by a slow-speed prior than imperfect integration. When subjects integrate paths over long periods, this framework intriguingly predicts a distance-dependent bias reversal due to buildup of uncertainty, which we also confirmed experimentally. These results suggest that visual path integration in noisy environments is limited largely by biases in processing optic flow rather than by leaky integration.","David Redish, Jessica Hamrick, Malcolm MacIver, Tom Griffiths, Ida Momennejad, Constantin Rothkopf","Ruiyi Zhang, Xaq Pitkow, Dora Angelaki",
71,1187,Kwangjun,Lee,k.lee@uva.nl,Universiteit van Amsterdam,+31-6-2948-2707,Amsterdam,Noord Holland,1098 XH,Netherlands,"[Spiking Neural Networks for Predictive Coding with a Feedforward Gist Pathway]
How does the brain seamlessly perceive the world without having a direct access to the source of sensations in the physical world? Rather than passively relaying information that sensory organs pick up from the external world, it actively gathers statistical regularities from sensory inputs to represent (i.e., predict) the subject’s current situation. Predictive coding (PC) offers a biologically plausible scheme for such a generative model via hierarchical prediction error minimization. To advance the biological realism of previous PC models based on artificial neural networks, we developed a spiking neural network for predictive coding (SNN-PC), which introduces spiking neurons and proposes structural and algorithmic modifications required to overcome challenges from spiking dynamics. In addition, we present the feedforward gist pathway, which models a neural code for the gist of an image and provides a neurobiological alternative to arbitrary choices of a prior. After training with simple visual images, SNN-PC developed hierarchical latent representations and reconstructed input images. SNN-PC suggests biologically plausible mechanisms by which the brain performs perceptual inference and learning in an unsupervised manner.",,Matthias Brucklacher,
72,1326,Xiamin,Leng,xiamin_leng@brown.edu,Brown University,401-314-5106,Providence,Rhode Island,02906,United States,"[Leaving alternatives behind: A theoretical and experimental investigation of the role of mutual inhibition in shaping choice]
When studying value-based decision making, we typically focus on understanding how people choose one option from a set to the exclusion of the remaining options (e.g., a menu). Popular models of decision making likewise assume some form of competition between options to account for choice exclusivity. Studying choices that relax this exclusivity property (e.g., buffets) could provide a critical test of these models, as well as novel insights into the range of decisions in the real world. Here, we developed a novel task that compares exclusive to non-exclusive choices, and used this task to test predicted computational mechanisms for choice exclusivity. Across two studies, we found that exclusive and non-exclusive choices were similarly accurate and similarly influenced by the relative values of the options, but non-exclusive choices were overall faster and demonstrated a greater speeding effect with higher overall set values than exclusive choices. We showed that these behavioral patterns are predicted by a sequential sampling model in which evidence accumulation is less competitive for non-exclusive relative to exclusive choices. These findings demonstrate new approaches to tease apart the processes that make our choices better from those that make them (unnecessarily) hard.

[Dissociable influences of reward and punishment on adaptive cognitive control]
To invest effort into any cognitive task, people must be sufficiently motivated. Whereas prior research has focused primarily on how the cognitive control required to complete these tasks is motivated by the potential rewards for success, it is also known that control investment can be equally motivated by the potential negative consequence for failure. Previous theoretical and experimental work has yet to examine how positive and negative incentives differentially influence the manner and intensity with which people allocate control. Here, we develop and test a normative model of control allocation under conditions of varying positive and negative performance incentives. Our model predicts, and our empirical findings confirm, that rewards for success and punishment for failure should differentially influence adjustments to the evidence accumulation rate versus response threshold, respectively. This dissociation further enabled us to infer how motivated a given person was by the consequences of success versus failure.
",,"Romy Frömer, Joonhwa Kim, Harrison Ritz",
73,1127,Anna,Leshinskaya,anna.leshinskaya@gmail.com,"University of California, Davis",617-237-0260,Berkeley,CA,94708,United States,"[Integration of event experiences to build relational memory in the human brain]

How are experiences of events used to update knowledge of predictive relations in semantic memory? We examined the roles of anterior-lateral entorhinal cortex (alEC), important for encoding recently experienced temporal relations, and middle temporal gyrus (MTG), involved in familiar concepts of actions and events. Participants underwent fMRI during exposure to novel event sequences and a memory probe phase (Session 1) and the same process a week later (Session 2). Across distinct sequences, relations among similar events could either be Consistent, or the roles of the events could swap (Inconsistent). We examined the effect of Consistency on the strength of relational memory content at both timepoints. Areas that integrate across diverse experiences should be aided in the Consistent condition. We found that MTG performed this integrative role only in Session 2, subsequent to similar effects in alEC at Session 1. We suggest these areas work together to build relational knowledge.

[Models of Predictive Memory for Brain and Behavior]

The human mind spontaneously summarizes experience by encoding predictive regularities, but there is a variety of computations by which it may do so. In this talk, I share my work on the nature of these computations using behavior, computational modeling, and fMRI. In part 1, I discuss behavioral work testing whether participants’ memory for predictive regularities from an incidental encoding task reflects the conditional probability between events (as often assumed) or by a contingency principle, delta P. By this principle, derived from work on explicit learning and causal reasoning, the relationship between stimulus pair A and B is a function not only of how often A is followed by B, but also how often B occurs without A. We show that delta P captures what participants remember about events they had seen, not just how they judge these relationships. In our modeling work, we confirm that a standard Rescorla Wagner learning model cannot capture these results, but we found that a surprisingly simple modification to this model (divisive normalization) allows it to do so. In part 2, I describe a new in-progress fMRI study that asks whether neural encoding of relational memory is also guided by the delta P principle. Areas in the medial temporal lobe (hippocampus and entorhinal cortex in particular) are known to encode relationships among stimuli, but this is often assumed to be driven by simple co-occurence. I report very early but striking evidence suggesting that co-occurrence is not a good model of how these areas encode experience. I am excited to get feedback on our design and early findings. ",,,
74,1082,Jason,Li,jasli@mit.edu,Massachusetts Institute of Technology,919-473-6569,Milton,Massachusetts,02186,United States,"[Modeling Human Eye Movements with Neural Networks in a Maze-Solving Task] From smoothly pursuing moving objects to rapidly shifting gazes during visual search, humans employ a wide variety of eye movement strategies in different contexts. While eye movements provide a rich window into mental processes, building generative models of eye movements is notoriously difficult, and to date the computational objectives guiding eye movements remain largely a mystery. In this work, we tackled these problems in the context of a canonical spatial planning task, maze-solving. We collected eye movement data from human subjects and built deep generative models of eye movements using a novel differentiable architecture for gaze fixations and gaze shifts. We found that human eye movements are best predicted by a model that is optimized not to perform the task as efficiently as possible but instead to run an internal simulation of an object traversing the maze. This not only provides a generative model of eye movements in this task but also suggests a computational theory for how humans solve the task, namely that humans use mental simulation.","Ilker Yildirim, Nikolaus Kriegeskorte, Martin Schrimpf, Kim Stachenfeld, Tatiana Engel, Dileep George",Nicholas Watters,
75,1288,Jing-Jing,Li,jl3676@berkeley.edu,"University of California, Berkeley",607-342-8369,Berkeley,CA,94710,United States,"[Temporal and State Abstractions for Efficient Learning, Transfer, and Composition in Humans]
Humans use prior knowledge to efficiently solve novel tasks, but how they structure past knowledge during learning to enable such fast generalization is not well understood. We recently proposed that hierarchical state abstraction enabled generalization of simple one-step rules, by inferring context clusters for each rule. However, humans’ daily tasks are often temporally extended, and necessitate more complex multi-step, hierarchically structured strategies. The options framework in hierarchical reinforcement learning provides a theoretical framework for representing such transferable strategies. Options are abstract multi-step policies, assembled from simpler one-step actions or other options, that can represent meaningful reusable strategies as temporal abstractions. We developed a novel sequential decision-making protocol to test if humans learn and transfer multi-step options. In a series of four experiments, we found transfer effects at multiple hierarchical levels of abstraction that could not be explained by flat reinforcement learning models or hierarchical models lacking temporal abstractions. We extended the options framework to develop a quantitative model that blends temporal and state abstractions. Our model captures the transfer effects observed in human participants. Our results provide evidence that humans create and compose hierarchical options, and use them to explore in novel contexts, consequently transferring past knowledge and speeding up learning.

[Reasoning, Learning, and Creativity: Frontal Lobe Function and Human Decision-Making]
The frontal lobes subserve decision-making and executive control—that is, the selection and coordination of goal-directed behaviors. Current models of frontal executive function, however, do not explain human decision-making in everyday environments featuring uncertain, changing, and especially open-ended situations. Here, we propose a computational model of human executive function that clarifies this issue. Using behavioral experiments, we show that unlike others, the proposed model predicts human decisions and their variations across individuals in naturalistic situations. The model reveals that for driving action, the human frontal function monitors up to three/four concurrent behavioral strategies and infers online their ability to predict action outcomes: whenever one appears more reliable than unreliable, this strategy is chosen to guide the selection and learning of actions that maximize rewards. Otherwise, a new behavioral strategy is tentatively formed, partly from those stored in long-term memory, then probed, and if competitive confirmed to subsequently drive action. Thus, the human executive function has a monitoring capacity limited to three or four behavioral strategies. This limitation is compensated by the binary structure of executive control that in ambiguous and unknown situations promotes the exploration and creation of new behavioral strategies. The results support a model of human frontal function that integrates reasoning, learning, and creative abilities in the service of decision-making and adaptive behavior.

[Prefrontal cortex as a meta-reinforcement learning system]
Over the past 20 years, neuroscience research on reward-based learning has converged on a canonical model, under which the neurotransmitter dopamine 'stamps in' associations between situations, actions and rewards by modulating the strength of synaptic connections between neurons. However, a growing number of recent findings have placed this standard model under strain. We now draw on recent advances in artificial intelligence to introduce a new theory of reward-based learning. Here, the dopamine system trains another part of the brain, the prefrontal cortex, to operate as its own free-standing learning system. This new perspective accommodates the findings that motivated the standard model, but also deals gracefully with a wider range of observations, providing a fresh foundation for future research.",,,
76,1142,Ruolan,Li,liruolan.leslie@gmail.com,University of Maryland,585-363-8977,Riverdale Park,MD,20737,United States,"#1 Title: Modeling rhythm in speech as in music: Towards a unified cognitive representation
#1 Abstract: In acquiring language, differences in input can greatly affect learning outcomes, but which aspects of language learning are most sensitive to input variations, and which are robust, remains debated. A recent modeling study successfully reproduced a phenomenon empirically observed in early phonetic learning—learning about the sounds of the native language in the first year of life—despite using input that differed in quantity and speaker composition from what a typical infant would hear. In this paper, we carry out a direct test of that model’s robustness to input variations. We find that, despite what the original result suggested, the learning outcomes are sensitive to properties of the input and that more plausible input leads to a better fit with empirical observations. This has implications for understanding early phonetic learning in infants and underscores the importance of using realistic input in models of language acquisition.

#2 Title: Modeling Rhythm in Speech as in Music: Towards a Unified Cognitive Representation
#2 Abstract: Rhythm plays an important role in language perception and learning, with infants perceiving rhythmic differences across languages at birth. While the mechanisms underlying rhythm perception in speech remain unclear, one interesting possibility is that  hese mechanisms are similar to those involved in the perception of musical rhythm. In this work, we adopt a model originally designed for musical rhythm to simulate speech rhythm perception. We show that this model replicates the behavioral results of language discrimination in newborns, and outperforms an existing model of infant language discrimination. We also find that percussives — fast-changing components in the acoustics — are necessary for distinguishing languages of different rhythms, which suggests that percussives are essential for rhythm perception. Our music-inspired model of speech rhythm may be seen as a first step towards a unified theory of how rhythm is represented in speech and music.
",,,
77,1133,Wenliang,Li,kevinwli@outlook.com,University College London,+44-7531-143-552,London,,W1T 4JG,United Kingdom,"[Models that employ latent variables to capture structure in observed data lie at the heart of many current unsupervised learning algorithms, but exact maximum-likelihood learning for powerful and flexible latent-variable models is almost always intractable. Thus, state-of-the-art approaches either abandon the maximum-likelihood framework entirely, or else rely on a variety of variational approximations to the posterior distribution over the latents. Here, we propose an alternative approach that we call amortised learning. Rather than computing an approximation to the posterior over latents, we use a wake-sleep Monte-Carlo strategy to learn a function that directly estimates the maximum-likelihood parameter updates. Amortised learning is possible whenever samples of latents and observations can be simulated from the generative model, treating the model as a “black box”. We demonstrate its effectiveness on a wide range of complex models, including those with latents that are discrete or supported on non-Euclidean spaces.]
[Humans and other animals are frequently near-optimal in their ability to integrate noisy and ambiguous sensory data to form robust percepts---which are informed both by sensory evidence and by prior expectations about the structure of the environment. It is suggested that the brain does so using the statistical structure provided by an internal model of how latent, causal factors produce the observed patterns. In dynamic environments, such integration often takes the form of\emph {postdiction}, wherein later sensory evidence affects inferences about earlier percepts. As the brain must operate in current time, without the luxury of acausal propagation of information, how does such postdictive inference come about? Here, we propose a general framework for neural probabilistic inference in dynamic models based on the distributed distributional code (DDC) representation of uncertainty, naturally extending the underlying encoding to incorporate implicit probabilistic beliefs about both present and past. We show that, as in other uses of the DDC, an inferential model can be learnt efficiently using samples from an internal model of the world. Applied to stimuli used in the context of psychophysics experiments, the framework provides an online and plausible mechanism for inference, including postdictive effects.]
[Understanding visual perceptual learning (VPL) has become increasingly more challenging as new phenomena are discovered with novel stimuli and training paradigms. Although existing models aid our knowledge of critical aspects of VPL, the connections shown by these models between behavioral learning and plasticity across different brain areas are typically superficial. Most models explain VPL as readout from simple perceptual representations to decision areas and are not easily adaptable to explain new findings. Here, we show that a well -known instance of deep neural network (DNN), whereas not designed specifically for VPL, provides a computational model of VPL with enough complexity to be studied at many levels of analyses. After learning a Gabor orientation discrimination task, the DNN model reproduced key behavioral results, including increasing specificity with higher task precision, and also suggested that learning precise discriminations could transfer asymmetrically to coarse discriminations when the stimulus conditions varied. Consistent with the behavioral findings, the distribution of plasticity moved toward lower layers when task precision increased and this distribution was also modulated by tasks with different stimulus types. Furthermore, learning in the network units demonstrated close resemblance to extant electrophysiological recordings in monkey visual areas. Altogether, the DNN fulfilled predictions of existing theories regarding specificity and plasticity and reproduced findings of tuning changes in neurons of the primate visual areas. Although the comparisons were mostly qualitative, the DNN provides a new method of studying VPL, can serve as a test bed for theories, and assists in generating predictions for physiological investigations.]","Eero Simoncelli,Surya Ganguli",,
78,1383,Yuxuan,Li,liyuxuan@stanford.edu,Stanford University,650-788-9157,Stanford,CA,94305,United States,"[A weighted constraint satisfaction approach to human goal-directed decision making]
When we plan for long-range goals, proximal information cannot be exploited in a blindly myopic way, as relevant future information must also be considered. But when a subgoal must be resolved first, irrelevant future information should not interfere with the processing of more proximal, subgoal-relevant information. We explore the idea that decision making in both situations relies on the flexible modulation of the degree to which different pieces of information under consideration are weighted, rather than explicitly decomposing a problem into smaller parts and solving each part independently. We asked participants to find the shortest goal-reaching paths in mazes and modeled their initial path choices as a noisy, weighted information integration process. In a base task where choosing the optimal initial path required weighting starting-point and goal-proximal factors equally, participants did take both constraints into account, with participants who made more accurate choices tending to exhibit more balanced weighting. The base task was then embedded as an initial subtask in a larger maze, where the same two factors constrained the optimal path to a subgoal, and the final goal position was irrelevant to the initial path choice. In this more complex task, participants’ choices reflected predominant consideration of the subgoal-relevant constraints, but also some influence of the initially-irrelevant final goal. More accurate participants placed much less weight on the optimality-irrelevant goal and again tended to weight the two initially-relevant constraints more equally. These findings suggest that humans may rely on a graded, task-sensitive weighting of multiple constraints to generate approximately optimal decision outcomes in both hierarchical and non-hierarchical goal-directed tasks.

[Emergent structured decision processes in Transformers]
Transformer-based neural networks have proven to be simple yet scalable models in both natural language processing and machine vision.  Recent work suggested that language models can acquire sensitivity to latent hierarchical linguistic structures when trained only for word prediction under self-supervision.  In this work, we study transformer-based models in a decision making context, another domain rich of hierarchical structures.  We explore the idea that structured decision making may also emerge from these general-purpose sequence-to-sequence models through only self-supervision.  We trained a causal transformer model to perform a set of sequence manipulation tasks that can each be described by a different decision rule.  Using generalization tests and ablation studies, as well as assessing self-attention patterns and analyzing latent representations, our work seeks to parse how the decision rules become implicitly acquired in these models over the course of learning to solve a single task or when learning multiple tasks.  The ability to flexibly contextualize the given input through self-attention in a task-conditioned manner may support not only successful multi-task learning across different task settings, but also the development of structured decision processes.","Mark Ho, Jessica Hamrick, Kimberly Stachenfeld",,
79,1072,Rex,Liu,rex_liu@brown.edu,Brown University,401-678-8010,Providence,RI,02906,United States,"A hallmark of human intelligence, but challenging for reinforcement learning (RL) agents, is the ability to compositionally generalise, that is, to recompose familiar knowledge components in novel ways to solve new problems. For instance, when navigating in a city, one needs to know the location of the destination and how to operate a vehicle to get there, whether it be pedalling a bike or operating a car. In RL, these correspond to the reward function and transition function, respectively. To compositionally generalize, these two components need to be transferable independently of each other: multiple modes of transport can reach the same goal, and any given mode can be used to reach multiple destinations. Yet there are also instances where it can be helpful to learn and transfer entire structures, jointly representing goals and transitions, particularly whenever these recur in natural tasks (e.g., given a suggestion to get ice cream, one might prefer to bike, even in new towns). Prior theoretical work has explored how, in model-based RL, agents can learn and generalize task components (transition and reward functions). But a satisfactory account for how a single agent can simultaneously satisfy the two competing demands is still lacking. Here, we propose a hierarchical RL agent that learns and transfers individual task components as well as entire structures (particular compositions of components) by inferring both through a non-parametric Bayesian model of the task. It maintains a factorised representation of task components through a hierarchical Dirichlet process, but it also represents different possible covariances between these components through a standard Dirichlet process. We validate our approach on a variety of navigation tasks covering a wide range of statistical correlations between task components and show that it can also improve generalisation and transfer in more complex, hierarchical tasks with goal/subgoal structures. Finally, we end with a discussion of our work including how this clustering algorithm could conceivably be implemented by cortico-striatal gating circuits in the brain.","Jane Wang, Chelsea Finn, Ida Mommenejad, Matt Botvinick, Maria Eckstein, Anne Collins",,
80,1424,Xueqing,Liu,lxqlxq21@gmail.com,Waymo,4048636968,Mountain View,CA,94041,United States,TBD,,,
81,1149,Javier,Lopez-Randulfe,lopez.randulfe@tum.de,Technical University of Munich,+34-69-726-7154,München,Bayern,85748,Germany,"[Philosophy of the Spike: Rate-Based vs. Spike-Based Theories of the Brain]

Does the brain use a firing rate code or a spike timing code? Considering this controversial question from an epistemological perspective, I argue that progress has been hampered by its problematic phrasing. It takes the perspective of an external observer looking at whether those two observables vary with stimuli, and thereby misses the relevant question: which one has a causal role in neural activity? When rephrased in a more meaningful way, the rate-based view appears as an ad hoc methodological postulate, one that is practical but with virtually no empirical or theoretical support.


[Optimized spiking neurons can classify images with high accuracy through temporal coding with two spikes]

Spike-based neuromorphic hardware promises to reduce the energy consumption of image classification and other deep-learning applications, particularly on mobile phones and other edge devices. However, direct training of deep spiking neural networks is difficult, and previous methods for converting trained artificial neural networks to spiking neurons were inefficient because the neurons had to emit too many spikes. We show that a substantially more efficient conversion arises when one optimizes the spiking neuron model for that purpose, so that it not only matters for information transmission how many spikes a neuron emits, but also when it emits those spikes. This advances the accuracy that can be achieved for image classification with spiking neurons, and the resulting networks need on average just two spikes per neuron for classifying an image. In addition, our new conversion method improves latency and throughput of the resulting spiking networks.


[Comparison of Artificial and Spiking Neural Networks on Digital Hardware]

Despite the success of Deep Neural Networks—a type of Artificial Neural Network (ANN)—in problem domains such as image recognition and speech processing, the energy and processing demands during both training and deployment are growing at an unsustainable rate in the push for greater accuracy. There is a temptation to look for radical new approaches to these applications, and one such approach is the notion that replacing the abstract neuron used in most deep networks with a more biologically-plausible spiking neuron might lead to savings in both energy and resource cost. The most common spiking networks use rate-coded neurons for which a simple translation from a pre-trained ANN to an equivalent spike-based network (SNN) is readily achievable. But does the spike-based network offer an improvement of energy efficiency over the original deep network? In this work, we consider the digital implementations of the core steps in an ANN and the equivalent steps in a rate-coded spiking neural network. We establish a simple method of assessing the relative advantages of rate-based spike encoding over a conventional ANN model. Assuming identical underlying silicon technology we show that most rate-coded spiking network implementations will not be more energy or resource efficient than the original ANN, concluding that more imaginative uses of spikes are required to displace conventional ANNs as the dominant computing framework for neural computation.",,,1450863775
82,1165,Zitong,Lu,lu.2637@osu.edu,The Ohio State University,614-620-1247,Columbus,OH,43212,United States,"[NeuroRA: A Python Toolbox of Representational Analysis From Multi-Modal Neural Data]
In studies of cognitive neuroscience, multivariate pattern analysis (MVPA) is widely used as it offers richer information than traditional univariate analysis. Representational similarity analysis (RSA), as one method of MVPA, has become an effective decoding method based on neural data by calculating the similarity between different representations in the brain under different conditions. Moreover, RSA is suitable for researchers to compare data from different modalities and even bridge data from different species. However, previous toolboxes have been made to fit specific datasets. Here, we develop NeuroRA, a novel and easy-to-use toolbox for representational analysis. Our toolbox aims at conducting cross-modal data analysis from multi-modal neural data (e.g., EEG, MEG, fNIRS, fMRI, and other sources of neruroelectrophysiological data), behavioral data, and computer-simulated data. Compared with previous software packages, our toolbox is more comprehensive and powerful. Using NeuroRA, users can not only calculate the representational dissimilarity matrix (RDM), which reflects the representational similarity among different task conditions and conduct a representational analysis among different RDMs to achieve a cross-modal comparison. Besides, users can calculate neural pattern similarity (NPS), spatiotemporal pattern similarity (STPS), and inter-subject correlation (ISC) with this toolbox. NeuroRA also provides users with functions performing statistical analysis, storage, and visualization of results. We introduce the structure, modules, features, and algorithms of NeuroRA in this paper, as well as examples applying the toolbox in published datasets.

[Representation comparisons between human brain and hierarchical deep convolutional neural network in face perception reveal a fatigue mechanism of repetition suppression]
Repetition suppression (RS) for faces have long been studied, yet, the underlying primary neural mechanisms of RS remains debated, for example, fatigue or sharpening of neuronal activities. The present study used a hierarchical deep convolutional neural network (DCNN), which achieve the performance of face recognition at human level, as a tool to simulate the neural mechanism of facial repetition suppression as fatigue or sharpening of neurons and compared representations between human brains and DCNNs. First, pre-trained but not randomly-weighted DCNN was better at distinguishing between different types of faces at human level than randomly-weighted DCNN. Afterwards, we constructed two RS models, fatigue model and sharpening model, to modify the activations of DCNNs and conducted cross-modal representational similarity analysis (RSA) comparisons between dynamic processing in human EEG signals and layers in modified DCNNs. We found that representations of human brains were more similar to those in fatigue-modified DCNN, compared with sharpening modified DCNN. Our results indicated that the facial RS effect in face perception was more likely caused by the fatigue mechanism suggesting that the activation of neurons with stronger responses to face stimulus would be attenuated more. Also, human brains showed stronger and longer similarities with the fatigue-modified DCNN as the layer increased. Therefore, the current study supports the fatigue mechanism as a more plausible neural mechanism of facial RS. The comparison between representations in the human brain and hierarchical DCNN provides a promising tool to simulate and infer the brain mechanism underlying human behaviors.

[Dynamic saccade context triggers more stable object-location binding]
Despite receiving visual inputs based on eye-centered (retinotopic) coordinates, we are able to perceive the world-centered (spatiotopic) locations of objects. A long-standing debate has been how object representations are transferred from retinotopic to spatiotopic coordinates to achieve stable visual perception across eye movements. Many studies have found retinotopic effects even for higher level visual processes, like object-location binding. However, these studies often rely on fairly static contexts (prolonged fixation on one location, followed by a single saccade). Might spatiotopic object-location binding be triggered selectively in dynamic saccade contexts? To test this hypothesis, we modified the behavioral ‘spatial congruency bias’ (SCB) paradigm. Human participants had to judge if two objects presented sequentially were the same or different. We conducted four experiments to investigate retinotopic vs spatiotopic object-location binding in different states. Critically, we found both strong spatiotopic and retinotopic SCBs in a dynamic saccade context with continuous eye-movements (Expt 1), but only retinotopic in the static context with only one saccade (Expt 2). Expts 3 and 4 isolated different dynamic saccade factors and showed that repeated saccades could decrease the retinotopic bias, and having the stimulus present before, during, and after the eye movement could increase the spatiotopic bias. Thus, using a consistent experimental design, we isolated factors that directly change object-location binding from retinotopic to spatiotopic. Based on these results, we propose an object location mapping theory with a dynamic state and gating model, which provides a possible interpretation to help us intuitively understand how our brains represent object location across saccades. Overall, our results provide strong evidence that dynamic saccade context can trigger an integrative and dynamic brain state to form more stable object-location binding, which is crucial to improved understanding of how the brain achieves visual stability in the dynamic world.",,,Zitong Lu
83,1227,April,Luo,cdluo@ucdavis.edu,"University of California, Davis",530-760-6185,Davis,California,95618,United States,"[lapsing model in Schizophrenia patients]
[sEEG and speech BCI]
[computational models and EEG decoding]",,,
84,1036,Tzuhsuan,Ma,mat@janelia.hhmi.org,HHMI Janelia Research Campus,512-660-9390,Ashburn,VA,20147,United States,"[Optimal abstraction for highly efficient dynamic inference] The brain must make efficient use of limited resources to guide behavior in the face of uncertainty and change. Although the efficient coding hypothesis (ECH) provides a normative framework for understanding how incoming sensory signals should be compressed to optimally transmit information, it provides little guidance for understanding how to compress inference processes within which signals are transformed, held, and integrated to guide behavior. In contrast to the stationary signals that are often explored within ECH, such inferences are dynamic and change over time as the environment changes. The principles by which a system should compress these dynamics, and how this compression impacts behavior, are not known (Fig 1a). Here, we formalize this problem by considering an agent that infers a changing state of the world from ambiguous sensory measurements. We assume that the agent need only make a coarse prediction of these states to guide behavior. To optimally make this prediction, the agent could update a fully detailed internal model of continuous beliefs about each world state. Instead, we consider how the agent could build an abstracted internal model to reliably interact with the world. We show that such abstracted models can be extremely compact and exhibit qualitatively different features than the full model. For instance, an abstraction that reuses a local inference circuit in two distant parts of the world can save resources while still enabling better-than-chance behavior. As such, these models are “wrong but effective”. To extract the essential features that constitute effective compression, we investigate commonalities across all optimal abstractions in the form of construction principles that can be used to build new abstractions. Armed with these construction principles, one can begin to understand how a system can exploit hidden task structures to compress the temporally-evolving signals that underlie dynamic inference.

[Optimal small programs for computationally-constrained inferences that maximize behavioral utility] To guide effective behavioral policies in the face of uncertainty and change, the brain is thought to make inferences about ambiguous surroundings to inform future actions. In such partially-observable settings, optimal strategies can be derived via two separate approaches: Bayesian inference for updating beliefs about unobservable properties of the environment, and value iteration for specifying optimal actions derived from those beliefs (Fig 1a). However, maintaining such internal beliefs requires in?nite precision. Given limited computational resources, the brain must develop ef?cient strategies for determining not only how best to act, but whether, when, and how to devote resources to making inferences. We formalize the interplay between limited resources and behavioral utility in partially-observable, nonstationary settings. We enumerate small programs that guide future actions based on past observations (Fig 1b) and identify optimal programs that maximize different utilities given a ?nite number of internal states M. We illustrate the value of this approach with a dynamic foraging task in which the type and probability of rewards—controlled by unobserved world states—changes over time (Fig 1c). To our surprise, the optimal M-state agent is not simply a discretized Bayesian agent; rather, it uses a fundamentally different program to achieve much higher utility in a two-port task (Fig 2). We further demonstrate this difference in a task that requires tradeoffs between two action types (Fig 3). While a Bayesian agent uses the second action type as soon it delivers a slightly higher payoff, a small agent must balance the cost of reallocating resources and implementing relevant inferences, resulting in a strategy that requires a bigger payoff to warrant the second action. Together, this work shows how resource limitations necessitate a tight relationship between inference and action, and it informs the design of adaptive, modular agents that use small programs to exploit environmental structure.

[The brain reuses the same place cells to learn different things without forgetting] Equally fascinating, place cells encode animal’s locations in a very different way than grid cells. When a researcher put a rat in a new room, its place cells do not appear to encode anything beyond simply being activated randomly. But over the course of just 10 minutes, these neurons learn the environment in such a detail that individual neurons activate only at their specialized locations. So unlike a single grid cell’s ambiguous coding, one can decode animal’s location just by finding which neuron is currently activated. Even more remarkably, the same population of place cells can learn a second completely different room without forgetting the first one. This unique ability place cells possess is called “learning without catastrophic forgetting,” which is a long-standing mystery not just in neuroscience but in AI research. Without overcoming such a challenge, one cannot unleash the full potential of an artificial neural network, and AI cannot learn like an animal in an online and continual fashion. Knowing this, I was determined to uncover the mechanism for place cells’ mysterious ability of continual learning. From my exploration of different types of neural network, I discovered a special way to wire up the network (for which I termed balanced initialization) so that the forgetting problem can be avoided. Although this special wiring gives a neural network such an amazing learning capability, it comes with an unforeseen consequence that was once thought to be avoided by conventional wisdom. The consequence is to use a fuzzy and high-dimensional neural representation to encode a precise and low-dimensional variable like animal’s location. Traditionally, an experimental field like neuroscience (or even just science in general) had the tendency to construct a minimal model directly from our human experiences. While this preference can save scientists many trials and errors, it also gets them stuck in a lot of programs. In the case of place cells, it was the intuitive (but wrong) thinking about how the brain should construct a low-dimensional representation to encode a low-dimensional variable got previous researchers stuck. For the brain in the end builds this un-imaginable fuzzy representation into place cells, it can ever overcome the catastrophic forgetting. As for the implication of my work to the AI research, the learning mechanism I uncovered can potentially be translated into an appropriate learning algorithm for a recurrent neural network (RNN) which also suffers from the catastrophic forgetting issue that prevents these system to learn in real time and has to be pre-trained before deployment.",Weiji Ma,,
85,1174,Xiangyu,Ma,xmaal@connect.ust.hk,Hong Kong University of Science and Technology,+852-5395-1814,Hong Kong,,,China,"[Multisensory integration is crucial to the survival of animals. In the dorsal medial superior temporal area and the ventral intraparietal area, experimental data revealed the existence of congruent neurons. Neurons in these areas with congruent preferred stimuli are found to be reciprocally connected. Recent work suggested that the congruent neurons can be utilized for multisensory information integration. In the Bayesian framework, the multisensory integration of the visual and vestibular stimuli generated from a fully correlated prior has been analyzed, but in reality, it is more relevant to consider environments described by composite priors with both correlated and independent components. We find that the probabilistic model can be formulated in two steps, the first integrates the cues from the two modalities to infer the posterior by using the correlated prior component, and the second combines the correlated component with a further input from the independent cue to yield the integrated inference. Thus, we propose that the corresponding network architecture consists of two groups of neurons in each area, the first group is congruently connected with its counterpart in the other area, and the second group receives inputs from the first one as well as the direct cue. This approach helps us to build up connections between the Bayesian framework and neural systems, and provides important insights on well characterized brain functions. Responses from both group of neurons are useful for downstream neural circuits responsible for causal inference. Furthermore, in the framework of probabilistic population coding, we find that the accuracy of the Bayesian prediction can be remarkably improved if the noise in the neuronal dynamics is determined by the amplitude of the population vector, which originates from the collective nature of the noise and is determined by the neuronal responses themselves.]",,,
86,1094,Manasi,Malik,mmalik16@jhu.edu,The Johns Hopkins University,667-910-2511,Baltimore,Maryland,21218,United States,"[Social Inference from Relational Visual Information: An Investigation with Graph Neural Network Models] Humans effortlessly recognize social interactions from visual input, such as distinguishing helping versus hindering interactions. Attempts to model this ability typically rely on generative inverse planning models, which make predictions based on simulations of agents' inferred goals. However, these models are computationally expensive and intractable on natural videos. Further, evidence suggests that recognizing social interactions is largely a visual process, separate from complex mental simulation. Yet, bottom-up visual models have not been able to reproduce human behavior. We hypothesize that humans rely on \underline{\emph{relational}} visual information in particular, which is lacking from standard neural network models, to recognize social interactions. We propose a graph neural network model, SocialGNN, that uses relational visual information to recognize social interactions between agents. We find that SocialGNN aligns with human interaction judgments significantly better than a matched neural network model without graph structure and, unlike inverse planning models, can operate on both animated and natural videos. These results show that adding relevant inductive biases to artificial vision systems allows them to make human-like social judgements without incurring high computational costs. Our findings further show that humans can make complex social interaction judgements based on visual information alone, and may rely on structured, graph-like representations.  [Network analysis of neuro-cognitive processes: studying mcgurk effect using EEG data] Understanding multisensory perception is an important area of brain research as it relates to perception and behavior. In this project, the focus of our study is the interaction between auditory and visual perception. We study this interaction through McGurk effect, using two sets of EEG, data acquired from two different sets of subjects at National Brain Research Center (NBRC), Manesar. In this project, we focus on studying the McGurk effect through properties of the brain networks. The approach has been to look at global network measures, area-wise network measures, variation of modularity across time and hub identification using node-wise network measures.",,"Paul Soulos, Emalie McMahon, Angira Shirahatti",
87,1423,Eshed,Margalit,eshed.margalit@gmail.com,Stanford University,5103861924,Mountain View,CA,94041,United States,"[Topographic deep neural networks predict the functional organization of the primate ventral visual pathway] The primate ventral visual pathway is organized into functional maps, including pinwheel-like arrangements of orientation-tuned neurons in primary visual cortex (V1) and patches of category-selective neurons in higher visual cortex. While deep convolutional neural networks (DCNNs) trained for object recognition accurately predict neural representations throughout the ventral pathway, they have no spatial layout for features at a given retinotopic location and are thus unable to predict the rich topographic organization of visual cortex. Here, we close this gap by first assigning each DCNN unit a position in a 2D cortical sheet, then training the network to minimize a cost function with two components: one encouraging accurate object recognition, and another favoring correlated responses among nearby units in each model layer. We find that training with this composite spatial loss produces brain-like topographic maps in both early and later model layers . Early layers contain smooth orientation preference maps with pinwheels, clusters of units preferring the same spatial frequency, and color-preference domains resembling V1 “blobs”. In a later layer of the same model, we observe clusters of category-selective units, e.g., face patches, whose spatial organization largely matches that found in primate higher visual cortex. Our model thus leverages local response correlations, which have been linked to theories of wire-length minimization, to accurately predict neuron responses and functional organization throughout the ventral visual pathway. In support of the wire-length minimization hypothesis, we find that our topographic DCNN would require shorter connections than a standard DCNN to support connections between similarly-tuned neurons within early (38% reduction) and later (31% reduction) model layers. These results suggest that the functional organization of visual cortex can be explained by two constraints: the need to perform object recognition and pressure for local populations of neurons to have correlated responses.

[Correlation-based spatial layout of deep neural network features generates ventral stream topography] The primate visual system is organized into functional maps, including pinwheel-like arrangements of orientation tuned neurons in primary visual cortex (V1) and patches of category-selective neurons in higher visual cortex. Recent work has demonstrated that deep convolutional neural networks (DCNNs) trained for object recognition are good descriptors of neural representations throughout the ventral pathway, with early, intermediate, and late cortical brain areas best predicted by corresponding layers of the DCNN. Despite this success, DCNNs have no inherent spatial layout for features at a given retinotopic location, and thus, make no predictions regarding many of the characteristic topographic phenomena observed in the brain beyond retinotopy itself, e.g., pinwheels and patches. Cortical map formation has been modeled using self-organizing maps that leverage principles of wiring-length minimization and local correlations of unit responses to produce topographic structure. However, these methods rely on simplified feature parameterizations that limit their ability to accommodate more realistic descriptions of neuron response properties, especially in higher visual areas. Here, we augment DCNNs by assigning model units spatial positions in a 2D “cortical sheet” and introduce a novel algorithm to arrange units so that local response correlations are maximized. Applying this algorithm to a categorization-optimized DCNN, we find that layouts generated from earlier layers recapitulate core features of V1 orientation, spatial frequency, and color preference maps, while those generated from later layers naturally exhibit category-selective clusters. Because this wide range of apparently disparate phenomenology is produced by the same underlying principle, our results suggest that the functional architecture of the visual system can be explained by two fundamental constraints: the need to perform visual tasks and the pressure to minimize biophysical costs such as wiring length. Our framework for spatially mapping DCNNs integrates biophysical and representational phenomenology, allowing a more unified understanding of the visual system’s functional architecture.

[Ultra-high-resolution fMRI of Human Ventral Temporal Cortex Reveals Differential Representation of Categories and Domains] Human ventral temporal cortex (VTC) is critical for visual recognition. It is thought that this ability is supported by large-scale patterns of activity across VTC that contain information about visual categories. However, it is unknown how category representations in VTC are organized at the submillimeter scale and across cortical depths. To fill this gap in knowledge, we measured BOLD responses in medial and lateral VTC to images spanning 10 categories from five domains (written characters, bodies, faces, places, and objects) at an ultra-high spatial resolution of 0.8 mm using 7 Tesla fMRI in both male and female participants. Representations in lateral VTC were organized most strongly at the general level of domains (e.g., places), whereas medial VTC was also organized at the level of specific categories (e.g., corridors and houses within the domain of places). In both lateral and medial VTC, domain-level and category-level structure decreased with cortical depth, and downsampling our data to standard resolution (2.4 mm) did not reverse differences in representations between lateral and medial VTC. The functional diversity of representations across VTC partitions may allow downstream regions to read out information in a flexible manner according to task demands. These results bridge an important gap between electrophysiological recordings in single neurons at the micron scale in nonhuman primates and standard-resolution fMRI in humans by elucidating distributed responses at the submillimeter scale with ultra-high-resolution fMRI in humans.",,"Dawn Finzi, Martin Schrimpf, Insub Kim, Kalanit Grill-Spector, Jim DiCarlo, Ko Kar, Kendrick Kay",
88,1410,Suhail,Matar,sm6832@nyu.edu,New York University,6462729243,New York,New York,10003,United States,"[How the brain segments a continuous speech stream is among the main puzzles in language processing, and it is typically studied at the phoneme or word level. Recent work suggests the brain also segments words at the intermediate level of morphemes, which are the smallest meaningful units in language. However, the nature of this mechanism remains unclear. We contrast three hierarchically-nested models: (i) a morphologically naïve model with only acoustic, lexical and phonetic predictors; (ii) a morphologically passive model, also sensitive to morpheme boundary/onset, and (iii) a morphologically predictive model, sensitive also to predictive segmentation and morphological surprisal and uncertainty. We recorded magnetoencephalography (MEG) data as participants listened to single Arabic words consisting of one of two kinds of verb stems and a direct object pronoun (‘jarraba ni’=‘(He) tested me.’): morphologically ambiguous stems were pairs of short and long stems that had the same onset (‘jarraba’  vs. ‘jarra’= ‘(he) dragged’), creating temporary, early uncertainty about whether a stem would be short or long; morphologically unambiguous stems (‘qayyama’=‘(he) evaluated’) were all long but without shorter counterparts (‘qayya’ does not exist). All stems had the same uniqueness point (at which a unique stem is congruent with the input); all long stems followed the same Arabic vocalic template. We expected better fitting models to account for more of the neural signal in a temporal response function (TRF) analysis; we found that models with predictive morphological features explained more variability in bilateral superior and middle temporal lobes. We also compared different predictive morphological parsing strategies and found differences between ambiguous and unambiguous stems, before and after stem uniqueness points, in bilateral superior temporal cortex and in left inferior frontal cortex. This provides evidence that, rather than passively segment the input stream into morphemes, the brain proactively does the segmentation by predicting a morpheme’s identity before it is uniquely identifiable.]

[During language comprehension, the brain processes not only word meanings, but also the grammatical structure—the “syntax”—that strings words into phrases and sentences. Yet the neural basis of syntax remains contentious, partly due to the elusiveness of experimental designs that vary structure independently of meaning-related variables. Here, we exploit Arabic’s grammatical properties, which enable such a design. We collected magnetoencephalography (MEG) data while participants read the same noun-adjective expressions with zero, one, or two contiguously-written definite articles (e.g., ‘chair purple’; ‘the-chair purple’; ‘the-chair the-purple’), representing equivalent concepts, but with different levels of syntactic complexity (respectively, indefinite phrases: ‘a purple chair’; sentences: ‘The chair is purple.’; definite phrases: ‘the purple chair’). We expected regions processing syntax to respond differently to simple versus complex structures. Single-word controls (‘chair’/‘purple’) addressed definiteness-based accounts. In noun-adjective expressions, syntactic complexity only modulated activity in the left posterior temporal lobe (LPTL),?~?300 ms after each word’s onset: indefinite phrases induced more MEG-measured positive activity. The effects disappeared in single-word tokens, ruling out non-syntactic interpretations. In contrast, left anterior temporal lobe (LATL) activation was driven by meaning. Overall, the results support models implicating the LPTL in structure building and the LATL in early stages of conceptual combination.]

[Though recent years have seen a growth in research on predictive processes in language comprehension, their scope and mechanisms remain partially elusive. While mechanisms involved in predicting specific words are relatively well understood, those underlying syntactic prediction are still unclear. In part, this is because of the difficulty in designing experiments that manipulate syntactic predictability while controlling other variables. In this MEG study, we achieved this with a manipulation of syntactic category predictability within fully well-formed expressions of Standard Arabic. Participants read sentences beginning with a subject-adjective context, in which the presence of at least one of two possible cues (gender-incongruity and/or an intervening relative pronoun) was sufficient for predicting a target word's syntactic category. Absence of both cues (i.e., congruent subject-adjective context with no relative pronoun) increased uncertainty about the target's syntactic category. Using source analysis, we compared activity evoked by targets with predictable and unpredictable categories in the occipital lobe. We found an interaction effect consistent with previous findings: in the primary visual cortex, an early evoked component (visual M100) is enhanced only when the syntactic category was unpredictable. We also compared responses to pre-target predictive and unpredictive contexts across five bilateral frontal and temporal regions. In the right-hemispheric frontal region, we found a temporal cluster (~230?ms after adjective onset), where unpredictive contexts elicited more activation than predictive contexts. By hypothesis elimination, we conclude that the most likely variable driving this effect is syntactic entropy. Our results show that predictive mechanisms recruited during reading also involve predicting upcoming syntactic categories, implicating at least two cortical regions: the left visual cortex and the right frontal cortex.]","Alona Fyshe, Samuel Nastase, Alexander Huth, Xue Gong, Davide Turco, Cathy Chen",,
89,1237,Emalie,McMahon,emaliemcmahon@jhu.edu,The Johns Hopkins Univeristy,865-964-0986,Baltimore,Maryland,21218,United States,"[Naturalistic two-person social perception in the brain]
In a bustling social event, like the VSS Tiki Bar, we quickly and effortlessly perceive who is interacting with whom and details of their interactions such as whether our colleagues are engaged in a friendly or adversarial debate. Extracting these social details is crucial for deciding how we want to act. While we do this with ease, little is understood about how this is solved in the mind and brain. Although recent research has shown that a region in the posterior superior temporal sulcus (pSTS) is visually selective for social interactions, which features of a social interaction this and other regions of the brain represent is unknown. To answer this question, we showed participants 250 3-second video clips of naturalistic two-person interactions in the fMRI experiment. The stimulus set was curated to limit low-level confounds such that early features from an ImageNet-trained AlexNet were minimally correlated with social dimensions. The videos varied in sociality, social dimensions (e.g., valence, arousal, and cooperativity), and visual dimensions (e.g., the distance of the agents and the spatial expanse of the scene). Each participant separately completed functional localizers to define category-selective regions such as scene, social interaction, and theory of mind regions. We used an encoding model approach to investigate where social and visual dimensions are represented in the brain. After controlling for low-level information and motion energy, we validated that scene information such as indoor/outdoor and the spatial expanse of the scenes were represented in scene regions (PPA and OPA). Crucially, we found that the presence of a social interaction is represented in the pSTS, replicating prior findings in a curated, naturalistic dataset. We will use multivariate, whole-brain analysis to investigate where high-level features of social interactions are represented in the brain.","Christine Tseng, Tianjiao Zhang, Benjamin Peters","Talia Konkle, Dan Janini, Colin Comwell, Liuba Papeo",145042934
90,1091,Anja,Meunier,anja.meunier@univie.ac.at,University of Vienna,+43-681-811-01986,Vienna,,1080,Austria,"[Towards Deep Learning in BCI: Automatic labeling of large natural data sets]
An abundance of data in our digitalized world, together with ever-increasing computational capacities, has brought back neural networks from the AI winter, and led to sharp increases in performance in many decade-old problems. In the BCI field, data is still scarce due to traditional set-ups with randomized individual trials, which severely restricts the number of data points obtained per session and thus limits the power of complex deep learning architectures. One remedy for this data scarcity is moving to continuous natural real-world stimuli, which in turn poses the challenge of efficient data labeling. Herff et al. have used a traditional speech recognition pipeline for this task. In our approach, we use the end-to-end deep neural network DeepSpeech to automatically label large amounts of EEG data during perception of natural speech, resulting in almost 400.000 labeled data points per subject.

[A mathematical framework for bridging Marr’s levels]
The increasing success of deep neural networks (DNNs) in solving complex tasks on par with human-level performance raises the question of whether artificial and biological neural networks perform specific tasks similarly. While current methods of comparison have identified similarities in their respective intermediate representations, a rigorous theory of algorithms and their physical implementations is indispensable for understanding the computational processes biological neural networks implement. This work proposes mathematical definitions of the terms algorithm and implementation and demonstrates on a toy model how to empirically test whether a physical system implements a specific algorithm. Our conceptual framework thus contributes to the efforts in cognitive computational neuroscience to develop rigorous theories that can link the computational, algorithmic, and implementational levels.
","Umut Guclu, Marcel van Gerven, Katja Seeliger",Moritz Grosse-Wentrup,
91,1060,Florent,Meyniel,florent.meyniel@gmail.com,NeuroSpin CEA,+33-1-69-08-95-01,Gif-sur-Yvette,,F-91191,France,"[Brainstem fMRI signaling of surprise across different types of deviant stimuli]
The ability to detect deviant stimuli is crucial to adapt our behavior. Previous work showed that infrequent (hence deviant) stimuli elicit phasic activation of the brainstem locus coeruleus (LC), which releases noradrenaline and controls central arousal. However, it is unclear whether deviance detection selectively recruits the LC or also other neuromodulatory systems related to dopamine, acetylcholine, and serotonin. It is also unclear whether deviant-related responses in those systems only track infrequent stimuli in a sequence (which can be computed with bottom-up processes), or also violations of the sequence structure (which requires higher-order processings). Here, we combined human fMRI recordings optimized for brainstem imaging with pupillometry (a peripheral marker of central arousal) to perform a mapping of deviant-related responses in subcortical structures. Participants were exposed to a “local-global” paradigm that distinguishes between deviance based on the stimulus probability and the sequence structure. fMRI responses to deviant stimuli were quite distributed, detected in the LC but also other subcortical nuclei and many cortical areas. Both types of deviance elicited responses in the pupil, LC and other neuromodulatory systems. Our results reveal that deviance detection in humans recruits several subcortical systems and generalizes across computationally different types of deviance. 

[A characterization of the neural representation of confidence during probabilistic learning]
Learning in a stochastic and changing environment is a difficult task. Models of learning typically postulate that observations that deviate from the learned predictions are surprising and used to update those predictions. Bayesian accounts further posit the existence of a confidence-weighting mechanism: learning should be modulated by the confidence level that accompanies those predictions. However, the neural bases of this confidence are much less known than the ones of surprise. Here, we used a dynamic probability learning task and high-field MRI to identify putative cortical regions involved in the representation of confidence about predictions during human learning. We devised a stringent test based on the conjunction of four criteria. We localized several regions in parietal and frontal cortices whose activity is sensitive to the confidence of an ideal observer, specifically so with respect to potential confounds (surprise and predictability), and in a way that is invariant to which item is predicted. We also tested for functionality in two ways. First, we localized regions whose activity patterns at the subject level showed an effect of both confidence and surprise in qualitative agreement with the confidence-weighting principle. Second, we found neural representations of ideal confidence that also accounted for subjective confidence. Taken together, those results identify a set of cortical regions potentially implicated in the confidence-weighting of learning.

[A characterization of the neural representation of confidence during probabilistic learning]
From decision making to perception to language, predicting what is coming next is crucial. It is also challenging in stochastic, changing, and structured environments; yet the brain makes accurate predictions in many situations. What computational architecture could enable this feat? Bayesian inference makes optimal predictions but is prohibitively difficult to compute. Here, we show that a specific recurrent neural network architecture enables simple and accurate solutions in several environments. It requires a set of three mechanisms: gating, lateral connections, and recurrent weight tuning. Like the human brain, such networks develop internal representations of their changing environment (including estimates of the environment’s latent variables and the precision of these estimates), leverage multiple levels of latent structure, and adapt their effective learning rate to changes without changing their connection weights. Being ubiquitous in the brain, gated recurrence could therefore serve as a generic building block to predict in real-life environments.",,,
92,1217,Leon,Möhring,l.moehring@hotmail.com,University Medical Center Hamburg-Eppendorf,+49-151-2528-7904,Hamburg,,20251,Germany,"[Identity prediction errors in the human midbrain update reward-identity expectations in the orbitofrontal cortex]
There is general consensus that dopaminergic midbrain neurons signal reward prediction errors, computed as the difference between expected and received reward value. However, recent work in rodents shows that these neurons also respond to errors related to inferred value and sensory features, indicating an expanded role for dopamine beyond learning cached values. Here we utilize a transreinforcer reversal learning task and functional magnetic resonance imaging (fMRI) to test whether prediction error signals in the human midbrain are evoked when the expected identity of an appetitive food odor reward is violated, while leaving value matched. We found that midbrain fMRI responses to identity and value errors are correlated, suggesting a common neural origin for these error signals. Moreover, changes in reward-identity expectations, encoded in the orbitofrontal cortex (OFC), are directly related to midbrain activity, demonstrating that identity-based error signals in the midbrain support the formation of outcome identity expectations in OFC.

[States versus rewards: dissociable neural prediction error signals underlying model-based and model-free reinforcement learning]
Reinforcement learning (RL) uses sequential experience with situations (""states"") and outcomes to assess actions. Whereas model-free RL uses this experience directly, in the form of a reward prediction error (RPE), model-based RL uses it indirectly, building a model of the state transition and outcome structure of the environment, and evaluating actions by searching this model. A state prediction error (SPE) plays a central role, reporting discrepancies between the current model and the observed state transitions. Using functional magnetic resonance imaging in humans solving a probabilistic Markov decision task, we found the neural signature of an SPE in the intraparietal sulcus and lateral prefrontal cortex, in addition to the previously well-characterized RPE in the ventral striatum. This finding supports the existence of two unique forms of learning signal in humans, which may form the basis of distinct computational strategies for guiding behavior.","Malcolm Maclver, David Redish, Ida Mommenejad, Arthur Juliani",,
93,1270,Robert,Mok,robmmok@gmail.com,University of Cambridge,N/A,Cambridge,Cambridgeshire,CB2 7EF,United Kingdom,"[A multi-level account of hippocampal function from behaviour to neurons]

A complete neuroscience requires multi-level theories that address phenomena ranging from higher-level cognitive behaviors to activities within a cell. A levels-of-mechanism approach that decomposes a higher-level model of cognition and behavior into component mechanisms provides a coherent and richer understanding of the system than any level alone. Toward this end, we decomposed a cognitive model into neuron-like units using a neural flocking approach that parallels recurrent hippocampal activity. Neural flocking coordinates units that collectively form higher-level mental constructs. The decomposed model suggested how brain-scale neural populations coordinate to form assemblies encoding concept and spatial representations, and why so many neurons are needed for robust performance at the cognitive level. This multi-level explanation provides a way to understand how cognition and symbol-like representations are supported by coordinated neural populations (assemblies) formed through learning.

[Abstract Neural Representations of Category Membership beyond Information Coding Stimulus or Response]

For decades, researchers have debated whether mental repre- sentations are symbolic or grounded in sensory inputs and motor programs. Certainly, aspects of mental representations are grounded. However, does the brain also contain abstract concept representations that mediate between perception and action in a flexible manner not tied to the details of sensory inputs and motor programs? Such conceptual pointers would be useful when concepts remain constant despite changes in appearance and associated actions. We evaluated whether human participants acquire such representations using fMRI. Participants completed a probabilistic concept learning task in which sensory, motor, and category variables were not perfectly coupled or entirely independent, making it possible to observe evidence for abstract representations or purely grounded representations. To assess how the learned concept structure is represented in the brain, we exam- ined brain regions implicated in flexible cognition (e.g., pFC and parietal cortex) that are most likely to encode an abstract represen- tation removed from sensory–motor details. We also examined sensory–motor regions that might encode grounded sensory– motor-based representations tuned for categorization. Using a cognitive model to estimate participants’ category rule and multi- variate pattern analysis of fMRI data, we found the left pFC and MT coded for category in the absence of information coding for stim- ulus or response. Because category was based on the stimulus, finding an abstract representation of category was not inevitable. Our results suggest that certain brain areas support categorization behavior by constructing concept representations in a format akin to a symbol that differs from stimulus–motor codes.

[Transfer of abstract structural knowledge across distinct stimulus domains aids learning of novel concepts]

Recent work in structural learning and meta-learning claim that humans can transfer abstract structural knowledge to aid learning across domains. This may also apply for learning new concepts, where learning a new concept structurally similar to an existing concept is easier than learning an entirely novel concept. However, analogical reasoning studies suggest that humans typically fail to do this unless they are given explicit pointers to the relationship, or sufficient perceptual similarity highlighting the parallel across tasks.
	
To test if people can apply abstract, structural knowledge of existing concepts to aid the learning of new concepts, we tested participants (N=152) on two concept-learning tasks that either shared (experimental group) or did not share abstract category structure (control), and provided no hints. The tasks involved learning object-based or room-based concepts, avoiding any perceptual link across tasks. The experimental group completed a concept-learning task with a rule-plus-exception structure, then completed another concept-learning task with the same structure. The control group performed a concept-learning task with an exclusive-or (XOR) structure then a rule-plus-exception structure. We found that the experimental group discovered the concept structure in the first task and applied this knowledge to speed up learning in the second task (d=0.51). The speed up in learning was greater than the control group (d=0.38). 

To understand how this works, we explored whether a neural network model could provide mechanistic insight into such abstract transfer. We found that a three-layer neural network trained on multiple tasks was able to show similar transfer effects by applying the representations learnt in the first concept-learning task to the second task, thereby speeding up learning. 

Our results show that humans can extract the abstract category structure from perceptual features of existing concepts and apply it to aid the learning new concepts across entirely different stimulus domains to the common underlying structure. Our neural network modelling results suggests that this might be implemented in the brain by re-using previously learned neural representations to apply to new tasks, even without any overlap in inputs.
","Mark Ho, Kohitij Kar, Wei Ji Ma, Jane X. Wang, Anne Collins, Tom Griffiths","Nikolaus Kriegeskorte, Ian Charest, Ida Momennejad, Kim Stachenfeld, Jascha Achterberg",4536648
94,1038,Gaia,Molinaro,gaiamolinaro@berkeley.edu,"University of California, Berkeley",510-504-9859,Albany,CA,94706,United States,"[Humans imperfectly recruit reward systems as they learn to achieve novel goals]
Transient goals are key motivators in human learning. Extending the classic reinforcement learning framework to include a flexible mapping of outcomes to rewards according to current goals accounts for goals as intrinsic reinforcers. However, learning by encoding transient goals as rewards is slower than learning with familiar rewards. Here, we test whether this effect is due to occasional lapses in goal encoding and the subsequent value updating. We tasked human participants with learning from both familiar rewards (the ""Points"" condition) and abstract novel ""goal"" images (the ""Goals"" condition). To detect lapses in goal encoding, we asked participants to report all positive outcomes they received.  Behavioral results replicated our previous finding that people learn less efficiently when they encode goals as rewards than directly receiving familiar rewards. However, our modeling analysis suggested that lapses could not fully explain this behavioral discrepancy. This finding challenges the hypothesis that lapses are the primary cause of slower goal-driven learning, providing insights into the complex cognitive mechanisms of how humans learn from flexible, goal-dependent value assignments.",,,
95,1162,Paul,Muhle-Karbe,paul.muhlekarbe@gmail.com,"University of Oxford /
University of Birmingham",+44-7458-902-210,Oxford,Oxfordshire,OX2 0EU,United Kingdom,"[A context-dependent representation of allocentric space in the human hippocampus]
The ability to guide decisions based on context is a key building block of intelligent behavior. Here, we study the neural mechanisms underlying sequential decisions (plans) that are made in distinct task contexts. We asked participants in the fMRI scanner to navigate an avatar through a grid world to find rewards in two of four potential goal locations (rooms). Participants learned to perform the task in two distinct contexts, each of which defined the location of the second reward contingent on the first. Using multivariate analyses of BOLD signals, we asked how the neural geometry of spatial coding varied with navigational goals. In the hippocampus, we found that the representation of space depended on context, with space being “compressed” so that context-specific goals were embedded together in neural space, and this compression predicted performance. A model in which place fields represent both current and prospective locations can account for these results.


[A Hierarchy of Functional States in Working Memory]
Extensive research has examined how information is maintained in working memory (WM), but it remains unknown how WM is used to guide behavior. We addressed this question by combining human electrophysiology (50 subjects, male and female) with pattern analyses, cognitive modeling, and a task requiring the prolonged maintenance of two WM items and priority shifts between them. This enabled us to discern neural states coding for memories that were selected to guide the next decision from states coding for concurrently held memories that were maintained for later use, and to examine how these states contribute to WM-based decisions. Selected memories were encoded in a functionally active state. This state was reflected in spontaneous brain activity during the delay period, closely tracked moment-to-moment fluctuations in the quality of evidence integration, and also predicted when memories would interfere with each other. In contrast, concurrently held memories were encoded in a functionally latent state. This state was reflected only in stimulus-evoked brain activity, tracked memory precision at longer timescales, but did not engage with ongoing decision dynamics. Intriguingly, the two functional states were highly flexible, as priority could be dynamically shifted back and forth between memories without degrading their precision. These results delineate a hierarchy of functional states, whereby latent memories supporting general maintenance are transformed into active decision circuits to guide flexible behavior.


[Causal Evidence for Learning-Dependent Frontal Lobe Contributions to Cognitive Control]
The lateral prefrontal cortex (LPFC) plays a central role in the prioritization of sensory input based on task relevance. Such top-down control of perception is of fundamental importance in goal-directed behavior, but can also be costly when deployed excessively, necessitating a mechanism that regulates control engagement to align it with changing environmental demands. We have recently introduced the “flexible control model” (FCM), which explains this regulation as resulting from a self-adjusting reinforcement-learning mechanism that infers latent statistical structure in dynamic task environments to predict forthcoming states. From this perspective, LPFC-based control is engaged as a function of anticipated cognitive demand, a notion for which we previously obtained correlative neuroimaging evidence. Here, we put this hypothesis to a rigorous, causal test by combining the FCM with a transcranial magnetic stimulation (TMS) intervention that transiently perturbed the LPFC. Human participants (male and female) completed a nonstationary version of the Stroop task with dynamically changing probabilities of conflict between task-relevant and task-irrelevant stimulus features. TMS was given on each trial before stimulus onset either over the LPFC or over a control site. In the control condition, we observed adaptive performance fluctuations consistent with demand predictions that were inferred from recent and remote trial history and effectively captured by our model. Critically, TMS over the LPFC eliminated these fluctuations while leaving basic cognitive and motor functions intact. These results provide causal evidence for a learning-based account of cognitive control and delineate the nature of the signals that regulate top-down biases over stimulus processing.","Erie Boorman, Anna Shapiro, Michael Mack, David Badre, Charan Ranganath, Stefano Fusi",,
96,1259,Dyana,Muller,dmuller@berkeley.edu,"University of California, Berkeley",602-410-6672,Berkeley,CA,94710,United States,"[A model of concept learning with generalizability]

Humans have a very rich understanding of concepts: our mental representation of an idea compactly encodes a multitude of information, which broadly aids in cognitive abilities such as reasoning, communication, prediction, and more. Importantly, our conceptual understanding of ideas allows us to extrapolate and generalize about information that may not be available. Current computational models of concept representations generalize poorly about information that is not explicitly given during training. This project explores the generalizability of a model trained via simultaneous gradient descent across a suite of tasks. Preliminary data indicates this may be a promising method of training representations that are able to generalize in ways that current models cannot.",,"Jing-Jing Li
Eduardo Sandoval",
97,1030,David,Murphy,dfm794@gmail.com,Independent,510-798-8299,Fremont,CA,94536,United States,"[A Proposal for Intelligent Agents with Episodic Memory]
In the future we can expect that artificial intelligent agents, once deployed, will be required to learn continually from their experience during their operational lifetime. Such agents will also need to communicate with humans and other agents regarding the content of their experience, in the context of passing along their learnings, for the purpose of explaining their actions in specific circumstances or simply to relate more naturally to humans concerning experiences the agent acquires that are not necessarily related to their assigned tasks. We argue that to support these goals, an agent would benefit from an episodic memory; that is, a memory that encodes the agent's experience in such a way that the agent can relive the experience, communicate about it and use its past experience, inclusive of the agents own past actions, to learn more effective models and policies. In this short paper, we propose one potential approach to provide an AI agent with such capabilities. We draw upon the ever-growing body of work examining the function and operation of the Medial Temporal Lobe (MTL) in mammals to guide us in adding an episodic memory capability to an AI agent composed of artificial neural networks (ANNs). Based on that, we highlight important aspects to be considered in the memory organization and we propose an architecture combining ANNs and standard Computer Science techniques for supporting storage and retrieval of episodic memories. Despite being initial work, we hope this short paper can spark discussions around the creation of intelligent agents with memory or, at least, provide a different point of view on the subject.",,,
98,1110,Sabine,Muzellec,sabine.muzellec@cnrs.fr,"CerCo, Brown University",+33-5-62-74-61-57,Toulouse,,31300,France,xx,,,
99,1130,Federico,Nardi,f.nardi21@imperial.ac.uk,Imperial College London,+44-7713-998-436,London,,SW7 2AZ,United Kingdom,"[Embodied virtual reality for the study of real-world motor learning] Motor-learning literature focuses on simple laboratory-tasks due to their controlled manner and the ease to apply manipulations to induce learning and adaptation. Recently, we introduced a billiards paradigm and demonstrated the feasibility of real-world-neuroscience using wearables for naturalistic full-body motion-tracking and mobile-brain-imaging. Here we developed an embodied virtual-reality (VR) environment to our real-world billiards paradigm, which allows to control the visual feedback for this complex real-world task, while maintaining sense of embodiment. The setup was validated by comparing real-world ball trajectories with the trajectories of the virtual balls, calculated by the physics engine. We then ran our short-term motor learning protocol in the embodied VR. Subjects played billiard shots when they held the physical cue and hit a physical ball on the table while seeing it all in VR. We found comparable short-term motor learning trends in the embodied VR to those we previously reported in the physical real-world task. Embodied VR can be used for learning real-world tasks in a highly controlled environment which enables applying visual manipulations, common in laboratory-tasks and rehabilitation, to a real-world full-body task. Embodied VR enables to manipulate feedback and apply perturbations to isolate and assess interactions between specific motor-learning components, thus enabling addressing the current questions of motor-learning in real-world tasks. Such a setup can potentially be used for rehabilitation, where VR is gaining popularity but the transfer to the real-world is currently limited, presumably, due to the lack of embodiment.

[Brain Activity Reveals Multiple Motor-Learning Mechanisms in a Real-World Task] Many recent studies found signatures of motor learning in neural beta oscillations (13–30 Hz), and specifically in the post-movement beta rebound (PMBR). All these studies were in controlled laboratory-tasks in which the task designed to induce the studied learning mechanism. Interestingly, these studies reported opposing dynamics of the PMBR magnitude over learning for the error-based and reward-based tasks (increase vs. decrease, respectively). Here, we explored the PMBR dynamics during real-world motor-skill-learning in a billiards task using mobile-brain-imaging. Our EEG recordings highlight the opposing dynamics of PMBR magnitudes (increase vs. decrease) between different subjects performing the same task. The groups of subjects, defined by their neural dynamics, also showed behavioral differences expected for different learning mechanisms. Our results suggest that when faced with the complexity of the real-world different subjects might use different learning mechanisms for the same complex task. We speculate that all subjects combine multi-modal mechanisms of learning, but different subjects have different predominant learning mechanisms.


[Implicit sensorimotor adaptation is preserved in Parkinson’s Disease] Our ability to enact successful goal-directed actions involves multiple learning processes. Among these processes, implicit motor adaptation ensures that the sensorimotor system remains finely tuned in response to changes in the body and environment. Whether Parkinson’s Disease (PD) impacts implicit motor adaptation remains a contentious area of research: whereas multiple reports show impaired performance in this population, many others show intact performance. While there are a range of methodological differences across studies, one critical issue is that performance in many of the studies may reflect a combination of implicit adaptation and strategic re-aiming. Here, we revisited this controversy using a visuomotor task designed to isolate implicit adaptation. In two experiments, we found that adaptation in response to a wide range of visual perturbations (3° - 45°) was similar in PD and matched control participants. Moreover, in a meta-analysis of previously published work, we found that the mean effect size contrasting PD and controls across 16 experiments was not significant. Together, these analyses indicate that implicit adaptation is preserved in PD, offering a fresh perspective on the role of the basal ganglia in sensorimotor learning.

","Richard Ivry, Jonathan Tsay",,
100,1226,Thomas,Naselaris,nase0005@umn.edu,University of Minnesota,612-554-7116,Minneapolis,MN,55419,United States,"[Brain-optimized neural networks learn non-hierarchical models of representation in human visual cortex] Deep neural networks (DNNs) trained to perform visual tasks learn representations that align with the hierarchy of visual areas in the primate brain. This finding has been taken to imply that the primate visual system forms representations by passing them through a hierarchical sequence of brain areas, just as DNNs form representations by passing them through a hierarchical sequence of layers. To test the validity of this assumption, we optimized DNNs not to perform visual tasks but to directly predict brain activity in human visual areas V1--V4. Using a massive sampling of human brain activity, we constructed brain-optimized networks that predict brain activity even more accurately than task-optimized networks. We show that brain-optimized networks can learn representations that diverge from those formed in a strict hierarchy. Brain-optimized networks do not need to align representations in V1--V4 with layer depth; moreover, they are able to accurately model anterior brain areas (e.g., V4) without computing intermediary representations associated with posterior brain areas (e.g., V1). Our results challenge the view that human visual areas V1--V4 act---like the early layers of a DNN---as a serial pre-processing sequence for higher areas, and suggest they may subserve their own independent functions.

[Generative Feedback Explains Distinct Brain Activity Codes for Seen and Mental Images] The relationship between mental imagery and vision is a long-standing problem in neuroscience. Currently, it is not known whether differences between the activity evoked during vision and reinstated during imagery reflect different codes for seen and mental images. To address this problem, we modeled mental imagery in the human brain as feedback in a hierarchical generative network. Such networks synthesize images by feeding abstract representations from higher to lower levels of the network hierarchy. When higher processing levels are less sensitive to stimulus variation than lower processing levels, as in the human brain, activity in low-level visual areas should encode variation in mental images with less precision than seen images. To test this prediction, we conducted an fMRI experiment in which subjects imagined and then viewed hundreds of spatially varying naturalistic stimuli. To analyze these data, we developed imagery-encoding models. These models accurately predicted brain responses to imagined stimuli and enabled accurate decoding of their position and content. They also allowed us to compare, for every voxel, tuning to seen and imagined spatial frequencies, as well as the location and size of receptive fields in visual and imagined space. We confirmed our prediction, showing that, in low-level visual areas, imagined spatial frequencies in individual voxels are reduced relative to seen spatial frequencies and that receptive fields in imagined space are larger than in visual space. These findings reveal distinct codes for seen and mental images and link mental imagery to the computational abilities of generative networks.

[The Kanerva Machine: A Generative Distributed Memory] We present an end-to-end trained memory system that quickly adapts to new data and generates samples like them. Inspired by Kanerva's sparse distributed memory, it has a robust distributed reading and writing mechanism. The memory is analytically tractable, which enables optimal on-line compression via a Bayesian update-rule. We formulate it as a hierarchical conditional generative model, where memory provides a rich data-dependent prior distribution. Consequently, the top-down memory and bottom-up perception are combined to produce the code representing an observation. Empirically, we demonstrate that the adaptive memory significantly improves generative models trained on both the Omniglot and CIFAR datasets. Compared with the Differentiable Neural Computer (DNC) and its variants, our memory model has greater capacity and is significantly easier to train.",,"Ghislain St-Yves, Tiasha Saha Roy, Jesse Breedlove, Kendrick Kay",2689972
101,1339,Samuel,Nastase,sam.nastase@gmail.com,Princeton University,6032770904,Princeton,NJ,08542,United States,"[Reconstructing the cascade of language processing in the brain using the internal computations of a transformer-based language model]
Piecing together the meaning of a narrative requires understanding not only the individual words but also the intricate relationships between them. How does the brain construct this kind of rich, contextual meaning from natural language? Recently, a new class of artificial neural networks—based on the Transformer architecture—has revolutionized the field of language modeling. Transformers integrate information across words via multiple layers of structured circuit computations, forming increasingly contextualized representations of linguistic content. In this paper, we deconstruct these circuit computations and analyze the associated “transformations” (alongside the more commonly studied “embeddings”) at each layer to provide a fine-grained window onto linguistic computations in the human brain. Using functional MRI data acquired while participants listened to naturalistic spoken stories, we find that these transformations capture a hierarchy of linguistic computations across cortex, with transformations at later layers in the model mapping onto higher-level language areas in the brain. We then decompose these transformations into individual, functionally-specialized “attention heads” and demonstrate that the emergent syntactic computations performed by individual heads correlate with predictions of brain activity in specific cortical regions. These heads fall along gradients corresponding to different layers, contextual distances, and syntactic dependencies in a low-dimensional cortical space. Our findings provide a new basis for using the internal structure of large language models to better capture the cascade of cortical computations that support natural language comprehension.

[The “Narratives” fMRI dataset for evaluating models of naturalistic language comprehension]
The “Narratives” collection aggregates a variety of functional MRI datasets collected while human subjects listened to naturalistic spoken stories. The current release includes 345 subjects, 891 functional scans, and 27 diverse stories of varying duration totaling ~4.6?hours of unique stimuli (~43,000 words). This data collection is well-suited for naturalistic neuroimaging analysis, and is intended to serve as a benchmark for models of language and narrative comprehension. We provide standardized MRI data accompanied by rich metadata, preprocessed versions of the data ready for immediate use, and the spoken story stimuli with time-stamped phoneme- and word-level transcripts. All code and data are publicly available with full provenance in keeping with current best practices in transparent and reproducible neuroimaging.

[Direct Fit to Nature: An Evolutionary Perspective on Biological and Artificial Neural Networks]
Evolution is a blind fitting process by which organisms become adapted to their environment. Does the brain use similar brute-force fitting processes to learn how to perceive and act upon the world? Recent advances in artificial neural networks have exposed the power of optimizing millions of synaptic weights over millions of observations to operate robustly in real-world contexts. These models do not learn simple, human-interpretable rules or representations of the world; rather, they use local computations to interpolate over task-relevant manifolds in a high-dimensional parameter space. Counterintuitively, similar to evolutionary processes, over-parameterized models can be simple and parsimonious, as they provide a versatile, robust solution for learning a diverse set of functions. This new family of direct-fit models present a radical challenge to many of the theoretical assumptions in psychology and neuroscience. At the same time, this shift in perspective establishes unexpected links with developmental and ecological psychology.","Tony Zador, Blake Richards, Ev Fedorenko, Dan Goodman, Jonny Saunders",,2646546
102,1339,Samuel,Nastase,sam.nastase@gmail.com,Princeton University,6032770904,Princeton,NJ,08542,United States,"[Reconstructing the cascade of language processing in the brain using the internal computations of a transformer-based language model]
Piecing together the meaning of a narrative requires understanding not only the individual words but also the intricate relationships between them. How does the brain construct this kind of rich, contextual meaning from natural language? Recently, a new class of artificial neural networks—based on the Transformer architecture—has revolutionized the field of language modeling. Transformers integrate information across words via multiple layers of structured circuit computations, forming increasingly contextualized representations of linguistic content. In this paper, we deconstruct these circuit computations and analyze the associated “transformations” (alongside the more commonly studied “embeddings”) at each layer to provide a fine-grained window onto linguistic computations in the human brain. Using functional MRI data acquired while participants listened to naturalistic spoken stories, we find that these transformations capture a hierarchy of linguistic computations across cortex, with transformations at later layers in the model mapping onto higher-level language areas in the brain. We then decompose these transformations into individual, functionally-specialized “attention heads” and demonstrate that the emergent syntactic computations performed by individual heads correlate with predictions of brain activity in specific cortical regions. These heads fall along gradients corresponding to different layers, contextual distances, and syntactic dependencies in a low-dimensional cortical space. Our findings provide a new basis for using the internal structure of large language models to better capture the cascade of cortical computations that support natural language comprehension.

[The “Narratives” fMRI dataset for evaluating models of naturalistic language comprehension]
The “Narratives” collection aggregates a variety of functional MRI datasets collected while human subjects listened to naturalistic spoken stories. The current release includes 345 subjects, 891 functional scans, and 27 diverse stories of varying duration totaling ~4.6?hours of unique stimuli (~43,000 words). This data collection is well-suited for naturalistic neuroimaging analysis, and is intended to serve as a benchmark for models of language and narrative comprehension. We provide standardized MRI data accompanied by rich metadata, preprocessed versions of the data ready for immediate use, and the spoken story stimuli with time-stamped phoneme- and word-level transcripts. All code and data are publicly available with full provenance in keeping with current best practices in transparent and reproducible neuroimaging.

[Direct Fit to Nature: An Evolutionary Perspective on Biological and Artificial Neural Networks]
Evolution is a blind fitting process by which organisms become adapted to their environment. Does the brain use similar brute-force fitting processes to learn how to perceive and act upon the world? Recent advances in artificial neural networks have exposed the power of optimizing millions of synaptic weights over millions of observations to operate robustly in real-world contexts. These models do not learn simple, human-interpretable rules or representations of the world; rather, they use local computations to interpolate over task-relevant manifolds in a high-dimensional parameter space. Counterintuitively, similar to evolutionary processes, over-parameterized models can be simple and parsimonious, as they provide a versatile, robust solution for learning a diverse set of functions. This new family of direct-fit models present a radical challenge to many of the theoretical assumptions in psychology and neuroscience. At the same time, this shift in perspective establishes unexpected links with developmental and ecological psychology.","Tony Zador, Blake Richards, Ev Fedorenko, Dan Goodman, Jonny Saunders",,2646546
103,1228,Ryosuke,Negi,negi15175@icloud.com,Tsukuba Univesity,+81-90-7892-5201,Tsukuba,Ibaraki,305-0821,Japan,"Paste in plain text the title and abstract of up to 3 works representative of your research
interests. Put the title of each work in brackets []

BCI
Computational neuroscience
Robotics","Chelsea Finn, Anne Collins, David Quiroga-Martinez, Suhail Matar,Yoonjung Lee, Alan
Bush",,
104,1066,Lukas,Neugebauer,l.neugebauer@uke.de,University Medical Center Hamburg-Eppendorf,+49-176-3952-4790,Hamburg,,20246,Germany,"[Explaining Compound Generalization in Associative and Causal Learning Through Rational Principles of Dimensional Generalization]
How do we apply learning from one situation to a similar, but not identical, situation? The
principles governing the extent to which animals and humans generalize what they have learned
about certain stimuli to novel compounds containing those stimuli vary depending on a number of
factors. Perhaps the best studied among these factors is the type of stimuli used to generate
compounds. One prominent hypothesis is that different generalization principles apply depending
on whether the stimuli in a compound are similar or dissimilar to each other. However, the results
of many experiments cannot be explained by this hypothesis. Here we propose a rational Bayesian
theory of compound generalization that uses the notion of consequential regions, first developed in
the context of rational theories of multidimensional generalization, to explain the effects of
stimulus factors on compound generalization. The model explains a large number of results from
the compound generalization literature, including the influence of stimulus modality and spatial
contiguity on the summation effect, the lack of influence of stimulus factors on summation with a
recovered inhibitor, the effect of spatial position of stimuli on the blocking effect, the
asymmetrical generalization decrement in overshadowing and external inhibition, and the
conditions leading to a reliable external inhibition effect. By integrating rational theories of
compound and dimensional generalization, our model provides the first comprehensive
computational account of the effects of stimulus factors on compound generalization, including
spatial and temporal contiguity between components, which have posed longstanding problems for
rational theories of associative and causal learning.

[Generalization guides human exploration in vast decision spaces]
From foraging for food to learning complex games, many aspects of human behaviour can be framed as a search problem with a vast space of possible actions. Under finite search horizons, optimal solutions are generally unobtainable. Yet, how do humans navigate vast problem spaces, which require intelligent exploration of unobserved actions? Using various bandit tasks with up to 121 arms, we study how humans search for rewards under limited search horizons, in which the spatial correlation of rewards (in both generated and natural environments) provides traction for generalization. Across various different probabilistic and heuristic models, we find evidence that Gaussian process function learning—combined with an optimistic upper confidence bound sampling strategy—provides a robust account of how people use generalization to guide search. Our modelling results and parameter estimates are recoverable and can be used to simulate human-like performance, providing insights about human behaviour in complex environments.

[Neural computations underlying causal structure learning]
Behavioral evidence suggests that beliefs about causal structure constrain associative learning, determining which stimuli can enter into association, as well as the functional form of that association. Bayesian learning theory provides one mechanism by which structural beliefs can be acquired from experience, but the neural basis of this mechanism is poorly understood. We studied this question with a combination of behavioral, computational, and neuroimaging techniques. Male and female human subjects learned to predict an outcome based on cue and context stimuli while being scanned using fMRI. Using a model-based analysis of the fMRI data, we show that structure learning signals are encoded in posterior parietal cortex, lateral prefrontal cortex, and the frontal pole. These structure learning signals are distinct from associative learning signals. Moreover, representational similarity analysis and information mapping revealed that the multivariate patterns of activity in posterior parietal cortex and anterior insula encode the full posterior distribution over causal structures. Variability in the encoding of the posterior across subjects predicted variability in their subsequent behavioral performance. These results provide evidence for a neural architecture in which structure learning guides the formation of associations.","Fabian Soto, Charley Wu, Eric Schulz, Josh Tenenbaum",,
105,1137,Cliona,O'Doherty,odoherc1@tcd.ie,Trinity College Dublin,+353-86-036-2121,Dublin,,2,Ireland,"[Objects or Context? Learning From Temporal Regularities in Continuous Visual Experience With an Infant-inspired DNN] Current deep neural network (DNN) models of human vision are focused on static, unnaturalistic supervised learning mechanisms, which are not present in human infants and which ignore the dynamics of naturalistic experience. Here, we implement an infant-inspired learning mechanism into a self-supervised DNN, by using contrastive learning to find commonalities in naturalistic video over various timescales. We hypothesised that commonalities across longer timescales (e.g., one minute) would reflect scene context, which changes relatively slowly. We assessed learned representations with test images in which objects or backgrounds were changed. We found that the temporal contrastive learn- ing approach led to representations that reflected scene context more than a baseline supervised network, which learned an object-centric embedding. However, at longer (5 min) timescales, object and context knowledge could both be learned. This illustrates that temporal structure in naturalistic visual inputs can be a powerful resource for learning, and demonstrates the importance of embrac- ing dynamic training signals when implementing more human-like DNN models.

[SemanticCMC: Contrastive Learning of Meaningful Object Associations from Temporal Co-occurrence Patterns in Naturalistic Movies] Deep convolutional neural networks continue to prevail at the forefront of innovation in computer vision. New, contrastive methods of self-supervised learning are lessening our reliance on curated datasets and hold potential for more robust and generalisable learning. While much of the focus in computer vision is on classification accuracy and if a network is performing well, it is equally useful to understand how these models are learning. By probing the representational underpinnings of DNN behaviour, we can better understand the foundations of success in computer vision. Here, we present an application of representational similarity analysis to investigate the patterns of activations within the self-supervised network Contrastive Multiview Coding (CMC). We illustrate that, despite enabling high ImageNet classification accuracy, purely perceptual tasks prevent CMC from capturing more high-level semantic structure that would easily be learned by a human. Building from this, we present SemanticCMC. Trained on a naturalistic movie dataset with meaningful temporal co-occurrence patterns, we illustrate that this alternate task improves coding of concept semantics despite attenuated classification accuracy. This preliminary analysis on a single self-supervised network highlights that reliance on object-level decoding does not always indicate meaningful concepts have been captured. By investigating the nature of the content learned by DNNs, we can improve our understanding of their similarities and differences to human vision and progress towards naturalistic visual machine learning in the real world.

[Semantic relationships emerge from visual temporal co-occurrences; a statistical analysis of a learning mechanism in early infancy] Infants learn to recognise the objects they see around them through experience. Investigating a signal which may facilitate this process, we quantified the temporal co-occurrence patterns in a naturalistic movie dataset and the semantic quality of object clusters. It was found using a statistical analysis that temporal correlations of objects could be clustered into semantically meaningful categories. Learning across a time interval which was neither too short nor too long (1 min) allowed superordinate category clusters to be learned by a deep neural network learning model. Thus, the results presented here provide a possible learning signal which could be tested in future infant studies. ","Bradley Love, Ida Momennejad, Daniel Yamins, Blake Richards, Kim Stachenfeld, Jane Wang ","Rhodri Cusack, Anna Truzzi",
106,1419,Sarah,Oh,sarahoh@berkeley.edu,UC Berkeley,2137189713,Berkeley,CA,94703,United States,"[Hierarchically organized behavior and its neural foundations: a reinforcement learning perspective] Research on human and animal behavior has long emphasized its hierarchical structure-the divisibility of ongoing behavior into discrete tasks, which are comprised of subtask sequences, which in turn are built of simple actions. The hierarchical structure of behavior has also been of enduring interest within neuroscience, where it has been widely considered to reflect prefrontal cortical functions. In this paper, we reexamine behavioral hierarchy and its neural substrates from the point of view of recent developments in computational reinforcement learning. Specifically, we consider a set of approaches known collectively as hierarchical reinforcement learning, which extend the reinforcement learning paradigm by allowing the learning agent to aggregate actions into reusable subroutines or skills. A close look at the components of hierarchical reinforcement learning suggests how they might map onto neural structures, in particular regions within the dorsolateral and orbital prefrontal cortex. It also suggests specific ways in which hierarchical reinforcement learning might provide a complement to existing psychological models of hierarchically structured behavior. A particularly important question that hierarchical reinforcement learning brings to the fore is that of how learning identifies new action routines that are likely to provide useful building blocks in solving a wide range of future problems. Here and at many other points, hierarchical reinforcement learning offers an appealing framework for investigating the computational and neural underpinnings of hierarchically structured behavior.


[Actions, Action Sequences and Habits: Evidence That Goal-Directed and Habitual Action Control Are Hierarchically Organized] 
Behavioral evidence suggests that instrumental conditioning is governed by two forms of action control: a goal-directed and a habit learning process. Model-based reinforcement learning (RL) has been argued to underlie the goal-directed process; however, the way in which it interacts with habits and the structure of the habitual process has remained unclear. According to a flat architecture, the habitual process corresponds to model-free RL, and its interaction with the goal-directed process is coordinated by an external arbitration mechanism. Alternatively, the interaction between these systems has recently been argued to be hierarchical, such that the formation of action sequences underlies habit learning and a goal-directed process selects between goal-directed actions and habitual sequences of actions to reach the goal. Here we used a two-stage decision-making task to test predictions from these accounts. The hierarchical account predicts that, because they are tied to each other as an action sequence, selecting a habitual action in the first stage will be followed by a habitual action in the second stage, whereas the flat account predicts that the statuses of the first and second stage actions are independent of each other. We found, based on subjects' choices and reaction times, that human subjects combined single actions to build action sequences and that the formation of such action sequences was sufficient to explain habitual actions. Furthermore, based on Bayesian model comparison, a family of hierarchical RL models, assuming a hierarchical interaction between habit and goal-directed processes, provided a better fit of the subjects' behavior than a family of flat models. Although these findings do not rule out all possible model-free accounts of instrumental conditioning, they do show such accounts are not necessary to explain habitual actions and provide a new basis for understanding how goal-directed and habitual action control interact.


[A Survey of Reinforcement Learning Informed by Natural Language] To be successful in real-world tasks, Reinforcement Learning (RL) needs to exploit the compositional, relational, and hierarchical structure of the world, and learn to transfer it to the task at hand. Recent ad- vances in representation learning for language make it possible to build models that acquire world knowl- edge from text corpora and integrate this knowledge into downstream decision making problems. We thus argue that the time is right to investigate a tight integration of natural language understanding into RL in particular. We survey the state of the field, including work on instruction following, text games, and learning from textual domain knowledge. Fi- nally, we call for the development of new environ- ments as well as further investigation into the po- tential uses of recent Natural Language Processing (NLP) techniques for such tasks.",,,
107,1336,Kevin,O'Neill,kevin.oneill@duke.edu,Duke University,919-660-3028,Durham,North Carolina,27708,United States,"[Measuring and Modeling Confidence in Human Causal Judgment]
The human capacity for causal judgment has long been thought to depend on an ability to consider counterfactual alternatives: the lightning strike caused the forest fire because, had it not struck, the forest fire would not have ensued. To accommodate psychological effects on causal judgment, a range of recent accounts have proposed that people probabilistically sample counterfactual alternatives from which they compute a graded index of causal strength. While such models have had success in describing the influence of probability on causal judgments, we show that they make further untested predictions about how probability should influence people's metacognitive confidence in their causal judgments. In a large ($N=3020$) sample of participants in a causal judgment task, we found evidence that probability indeed influences people's confidence in their causal judgments and that these influences were predicted by a counterfactual sampling model.

[Confidence and Gradation in Causal Judgment]
When comparing the roles of the lightning strike and the dry climate in causing the forest fire, one might think that the lightning strike is more of a cause than the dry climate, or one might think that the lightning strike completely caused the fire while the dry conditions did not cause it at all. Psychologists and philosophers have long debated whether such causal judgments are graded; that is, whether people treat some causes as stronger than others. To address this debate, we first reanalyzed data from four recent studies. We found that causal judgments were actually multimodal: although most causal judgments made on a continuous scale were categorical, there was also some gradation. We then tested two competing explanations for this gradation: the confidence explanation, which states that people make graded causal judgments because they have varying degrees of belief in causal relations, and the strength explanation, which states that people make graded causal judgments because they believe that causation itself is graded. Experiment 1 tested the confidence explanation and showed that gradation in causal judgments was indeed moderated by confidence: people tended to make graded causal judgments when they were unconfident, but they tended to make more categorical causal judgments when they were confident. Experiment 2 tested the causal strength explanation and showed that although confidence still explained variation in causal judgments, it did not explain away the effects of normality, causal structure, or the number of candidate causes. Overall, we found that causal judgments were multimodal and that people make graded judgments both when they think a cause is weak and when they are uncertain about its causal role.

[Eye-tracking mental simulation during retrospective causal reasoning]
There are conflicting theories about how people reason through cause and effect. A key distinction between two prominent accounts pertains to whether, in judging an event’s causal relevance, people preferentially consider what actually happened (as predicted by process theories) or whether they also consider what could have happened under different conditions (as predicted by counterfactual theories). Toward adjudicating between these theories, the current work used eye tracking and Gaussian Process modeling to investigate how people form causal judgments retrospectively and in the absence of ongoing visual input. Participants played a virtual ball-shooting game: after choosing to move left or right, they encoded a video of the actual outcome and then were prompted to mentally simulate either (a) what actually happened, (b) what could have happened, or (c) what caused the outcome to happen while looking at a blank screen. During causal judgment, we found evidence that participants visually mentally simulated counterfactual possibilities: they moved their eyes in similar patterns as when they imagined a counterfactual alternative. Altogether, these results favor counterfactual theories of causal reasoning, demonstrate how visual mental simulation can support this reasoning, and provide a novel methodological approach for using eye movements to investigate causal reasoning and counterfactual thinking more broadly.",,,
108,1261,Daniel,Pacheco,daniel.pacheco@upf.edu,Ruhr University Bochum,+34-610-842-344,Barcelona,,08026,Spain,"[Recurrence is required to capture the representational dynamics of the human visual system]
The human visual system is an intricate network of brain regions that enables us to recognize the world around us. Despite its abundant lateral and feedback connections, object processing is commonly viewed and studied as a feedforward process. Here, we measure and model the rapid representational dynamics across multiple stages of the human ventral stream using time-resolved brain imaging and deep learning. We observe substantial representational transformations during the first 300 ms of processing within and across ventral-stream regions. Categorical divisions emerge in sequence, cascading forward and in reverse across regions, and Granger causality analysis suggests bidirectional information flow between regions. Finally, recurrent deep neural network models clearly outperform parameter-matched feedforward models in terms of their ability to capture the multiregion cortical dynamics. Targeted virtual cooling experiments on the recurrent deep network models further substantiate the importance of their lateral and top-down connections. These results establish that recurrent models are required to understand information processing in the human ventral stream.

[Using deep reinforcement learning to reveal how the brain encodes abstract state-space representations in high-dimensional environments]
Humans possess an exceptional aptitude to efficiently make decisions from high-dimensional sensory observations. However, it is unknown how the brain compactly represents the current state of the environment to guide this process. The deep Q-network (DQN) achieves this by capturing highly nonlinear mappings from multivariate inputs to the values of potential actions. We deployed DQN as a model of brain activity and behavior in participants playing three Atari video games during fMRI. Hidden layers of DQN exhibited a striking resemblance to voxel activity in a distributed sensorimotor network, extending throughout the dorsal visual pathway into posterior parietal cortex. Neural state-space representations emerged from nonlinear transformations of the pixel space bridging perception to action and reward. These transformations reshape axes to reflect relevant high-level features and strip away information about task-irrelevant sensory features. Our findings shed light on the neural encoding of task representations for decision-making in real-world situations.

[Stable maintenance of multiple representational formats in human visual short-term memory]
Visual short-term memory (VSTM) enables humans to form a stable and coherent representation of the external world. However, the nature and temporal dynamics of the neural representations in VSTM that support this stability are barely understood. Here we combined human intracranial electroencephalography (iEEG) recordings with analyses using deep neural networks and semantic models to probe the representational format and temporal dynamics of information in VSTM. We found clear evidence that VSTM maintenance occurred in two distinct representational formats which originated from different encoding periods. The first format derived from an early encoding period (250 to 770 ms) corresponded to higher-order visual representations. The second format originated from a late encoding period (1,000 to 1,980 ms) and contained abstract semantic representations. These representational formats were overall stable during maintenance, with no consistent transformation across time. Nevertheless, maintenance of both representational formats showed substantial arrhythmic fluctuations, i.e., waxing and waning in irregular intervals. The increases of the maintained representational formats were specific to the phases of hippocampal low-frequency activity. Our results demonstrate that human VSTM simultaneously maintains representations at different levels of processing, from higher-order visual information to abstract semantic representations, which are stably maintained via coupling to hippocampal low-frequency activity.","Lynn Sörensen
Logan Cross",,
109,1055,Jungtak,Park,jungtak05@dgist.ac.kr,Daegu Gyeongbuk Institute of Science and Technology (DGIST),+82-10-8432-2486,Daegu,,42988,Korea (South),"[Different Brain Mechanisms of Time Estimation Depending on Situational Information]

Although we do not always measure time, we can estimate the passage of time based on our previous experience. However, it is unclear how the brain estimates the passage of time without explicit measures. We hypothesized that people use situational information to compensate for missing time. Using Bayesian hierarchical modeling and functional magnetic resonance imaging, we aimed to probe how our brain estimates time with/without situational information. As a result, the frontal lobe is actively involved in time estimation with situational information. The cerebellum and hippocampus were significantly activated in estimating time without situational information. We suggest that the frontal lobe plays a vital role in time estimation to control attentional modulation and time-based prospective memory with situational information. In contrast, the cerebellum and the hippocampus seem to act as an internal clock since these regions were involved in the relatively pure estimation of the time. ",,,
110,1035,Seongmin,Park,seongmin.a.park@gmail.com,"University of California, Davis",530-903-7094,Davis,California,95618,United States,"[Map making: Constructing, combining, and inferring on abstract cognitive maps]
Cognitive maps enable efficient inferences from limited experience that can guide novel decisions. We tested whether the hippocampus (HC), entorhinal cortex (EC), and ventromedial prefrontal cortex (vmPFC)/medial orbitofrontal cortex (mOFC) organize abstract and discrete relational information into a cognitive map to guide novel inferences. Subjects learned the status of people in two unseen 2D social hierarchies, with each dimension learned on a separate day. Although one dimension was behaviorally relevant, multivariate activity patterns in HC, EC, and vmPFC/mOFC were linearly related to the Euclidean distance between people in the mentally reconstructed 2D space. Hubs created unique comparisons between the hierarchies, enabling inferences between novel pairs. We found that both behavior and neural activity in EC and vmPFC/mOFC reflected the Euclidean distance to the retrieved hub, which was reinstated in HC. These findings reveal how abstract and discrete relational structures are represented, are combined, and enable novel inferences in the human brain.

[Inferences on a multidimensional social hierarchy use a grid-like code]
Generalizing experiences to guide decision-making in novel situations is a hallmark of flexible behavior. Cognitive maps of an environment or task can theoretically afford such flexibility, but direct evidence has proven elusive. In this study, we found that discretely sampled abstract relationships between entities in an unseen two-dimensional social hierarchy are reconstructed into a unitary two-dimensional cognitive map in the hippocampus and entorhinal cortex. We further show that humans use a grid-like code in entorhinal cortex and medial prefrontal cortex for inferred direct trajectories between entities in the reconstructed abstract space during discrete decisions. These grid-like representations in the entorhinal cortex are associated with decision value computations in the medial prefrontal cortex and temporoparietal junction. Collectively, these findings show that grid-like representations are used by the human brain to infer novel solutions, even in abstract and discrete problems, and suggest a general mechanism underpinning flexible decision-making and generalization.

[Neural computations underlying strategic social decision-making in groups]
When making decisions in groups, the outcome of one’s decision often depends on the decisions of others, and there is a tradeoff between short-term incentives for an individual and long-term incentives for the groups. Yet, little is known about the neurocomputational mechanisms at play when weighing different utilities during repeated social interactions. Here, using model-based fMRI and Public-good-games, we find that the ventromedial prefrontal cortex encodes immediate expected rewards as individual utility while the lateral frontopolar cortex encodes group utility (i.e., pending rewards of alternative strategies beneficial for the group). When it is required to change one’s strategy, these brain regions exhibited changes in functional interactions with brain regions engaged in switching strategies. Moreover, the anterior cingulate cortex and the temporoparietal junction updated beliefs about the decision of others during interactions. Together, our findings provide a neurocomputational account of how the brain dynamically computes effective strategies to make adaptive collective decisions.","Ila Fiete, Will Dabney, Brenden Lake, Sebastian Musslick, Ben Sorscher, David Abel",,
111,1416,Benjamin,Peters,benjamin.peters@columbia.edu,Columbia University,9174421239,New York,NY,10027,United States,"[A neural network walks into a lab: towards using deep nets as models for human behavior]
What might sound like the beginning of a joke has become an attractive prospect for many cognitive scientists: the use of deep neural network models (DNNs) as models of human behavior in perceptual and cognitive tasks. Although DNNs have taken over machine learning, attempts to use them as models of human behavior are still in the early stages. Can they become a versatile model class in the cognitive scientist's toolbox? We first argue why DNNs have the potential to be interesting models of human behavior. We then discuss how that potential can be more fully realized. On the one hand, we argue that the cycle of training, testing, and revising DNNs needs to be revisited through the lens of the cognitive scientist's goals. Specifically, we argue that methods for assessing the goodness of fit between DNN models and human behavior have to date been impoverished. On the other hand, cognitive science might have to start using more complex tasks (including richer stimulus spaces), but doing so might be beneficial for DNN-independent reasons as well. Finally, we highlight avenues where traditional cognitive process models and DNNs may show productive synergy.

[Capturing the objects of vision with neural networks]
Human visual perception carves a scene at its physical joints, decomposing the world into objects, which are selectively attended, tracked, and predicted as we engage our surroundings. Object representations emancipate perception from the sensory input, enabling us to keep in mind that which is out of sight and to use perceptual content as a basis for action and symbolic cognition. Human behavioral studies have documented how object representations emerge through grouping, amodal completion, proto-objects, and object files. Deep neural network (DNN) models of visual object recognition, by contrast, remain largely tethered to the sensory input, despite achieving human-level performance at labeling objects. Here, we review related work in both fields and examine how these fields can help each other. The cognitive literature provides a starting point for the development of new experimental tasks that reveal mechanisms of human object perception and serve as benchmarks driving development of deep neural network models that will put the object into object recognition.

[A neural network family for systematic analysis of RF size and computational-path-length distribution as determinants of neural predictivity and behavioral performance]
Deep feedforward convolutional neural network models (FCNNs) explain aspects of the representational transformations in the visual hierarchy. However, particular models implement idiosyncratic combinations of architectural hyperparameters, which limits theoretical progress. In particular, the size of receptive fields (RFs) and the distribution of computational path lengths (CPL; number of nonlinearities encountered) leading up to a representational stage are confounded across layers of the same architecture (deeper layers have larger RFs) and depend on idiosyncratic choices (kernel sizes, depth, skipping connections) across architectures. Here we introduce HBox, a family of architectures designed to break the confoundation of RF size and CPL. Like conventional FCNNs, an HBox model contains a feedforward hierarchy of convolutional feature maps. Unlike FCNNs, each map has a predefined RF size that can result from shorter or longer computational paths or any combination thereof (through skipping connections). We implemented a large sample of HBox models and investigated how RF size and CPL jointly account for neural predictivity and behavioral performance. The model set also provides insights into the joint contribution of deep and broad pathways which achieve complexity, respectively, through long or numerous computational paths. When controlling for the number of parameters, we find that visual tasks with higher complexity (CIFAR10, Imagenet) and occlusion (Digitclutter; Spoerer et al., 2017) show peak performance in models that trade off breadth to achieve higher depth (average CPL). The opposite holds for a simpler task (MNIST). We further disentangle the contribution of CPL, and RF size to the match of brain and model representation by assessing the ability of HBox models to predict visual representations in regions-of-interests in a large-scale fMRI benchmark (natural scenes dataset; Allen et al., 2021). The HBox architecture family illustrates how high-parametric task-performing vision models can be used systematically to gain theoretical insights into the neural mechanisms of vision.
","Constantin Rothkopf, Ida Momennejad, Arthur Juliani, Chelsea Finn, Tomer Ullman, Tom Griffiths, Kevin Smith, Judith Fan, Jessica Hamrick","Tal Golan, Nikolaus Kriegeskorte, Minni Sun, JohnMark Taylor, Heiko Schütt, Celia Durkin, Veronica Bossio, Sam Lippl, Wenxuan Guo, Paul Linton, Patrick Stinson",
112,1281,Leandro,Pires de Lima Jacob,lpljacob@bu.edu,Boston University,413-310-4416,Boston,MA,02215,United States,"[Deep Learning Reveals Non-linear Relationships between EEG and fMRI Dynamics]
fMRI enables non-invasive neuroimaging with high spatial resolution, but analyzing fMRI’s hemodynamic data is challenging due to its complex relationship to the underlying neural activity. Deep learning’s power to find non-linear relationships makes it particularly suitable for fMRI analyses, with many successful applications in classification of resting-state data. However, few have explored deep learning’s potential to generate continuous cross-modal predictions. If deep learning can translate fMRI data to neurophysiological EEG, it could become a promising method for uncovering relationships between hemodynamic changes and neural activity. Here, we demonstrate a proof-of-concept that deep recurrent neural networks can predict sleep EEG delta power from resting-state fMRI on a datapoint-to-datapoint basis, even for out-of-sample subjects, with predictions primarily driven by cortical fMRI dynamics. Supporting the idea that these predictions leverage non-linear information, a cross-correlation analysis revealed that our model outperformed simple linear methods. These results highlight the potential of deep learning to identify complex relationships between hemodynamic fMRI and EEG neural activity that cannot be detected with traditional linear analyses.",,,
113,1272,Carlos,Ponce,carlos@hms.harvard.edu,Harvard Medical School,617-682-0111,Boston,Massachusetts,02115,United States,"[Tuning landscapes of the ventral stream]
A fundamental goal in neuroscience is to define how cortical neurons respond to arbitrary natural images, such as those we encounter in daily life. Most studies of visual neurons rely on tuning curves, plots showing the dependence of evoked activity on image changes along one or two continuously changing parameters of simpler artificial stimuli. While it would be ideal to relate classic tuning curves to naturalistic images, but this has been difficult because naturalistic images are complex, containing many textures and objects, there is no agreement on how to describe them systematically using fixed a priori variables. In this study, we test the idea that all classic tuning curves can be viewed as slices of a higher-dimensional tuning landscape, i.e., the neuronal response as a function over the entire natural image space. We approximated the natural image space with a 4096-dimensional manifold, parametrized by a deep-learning image generator, and we investigated the tuning landscapes of ventral stream neurons on it. For each neuron we targeted, we used a closed-loop evolutionary approach to search for activation maximizing stimulus; using this stimulus (“prototype”) as a landmark, we sampled images around it and mapped the neuronal tuning. To probe the global distribution of the landmarks, we also searched for activation maximizing stimuli in a randomly chosen 50D subspace. By moving away from the prototype in diverse ways, we found that neurons showed smooth bell-shaped tuning consistent with radial basis functions. These tuning functions spanned a larger range of response and a wider extent in generator space, compared to the tuning measured in univariate image sets (Gabor, curvature). The width of tuning in generator space became narrower from V1 to IT, accompanied by a longer search convergence time and a larger decrease in response when we constrained the search to 50d random subspace. These trends pointed to a systematic difference in the landscape geometry throughout the ventral stream. One model that could explain all these trends is one where neurons in higher visual cortices have higher intrinsic dimensionality, i.e., more feature axes than neurons in early visual neurons. Overall, our results indicate that visual neurons should not be viewed as signaling parameter values in special tuning axes, but rather, they should be viewed as similarity (kernel) machines or radial basis functions, signaling distance to prototypes on the image manifold.

[Visual prototypes in the ventral stream are attuned to complexity and gaze behavior]
Early theories of efficient coding suggested the visual system could compress the world by learning to represent features where information was concentrated, such as contours. This view was validated by the discovery that neurons in posterior visual cortex respond to edges and curvature.  Still, it remains unclear what other information-rich features are encoded by neurons in more anterior cortical regions (e.g., inferotemporal cortex). Here, we use a generative deep neural network to synthesize images guided by neuronal responses from across the visuocortical hierarchy, using floating microelectrode arrays in areas V1, V4 and inferotemporal cortex of two macaque monkeys. We hypothesize these images (“prototypes”) represent such predicted information-rich features. Prototypes vary across areas, show moderate complexity, and resemble salient visual attributes and semantic content of natural images, as indicated by the animals’ gaze behavior. This suggests the code for object recognition represents compressed features of behavioral relevance, an underexplored aspect of efficient coding. 
","Wei Ji Ma, Thomas Serre",Nikolaus Kriegeskorte,
114,1409,Lindsey,Powell,ljpowell@ucsd.edu,UC San Diego,6176469048,La Jolla,CA,92093,United States,"[Adopted utility calculus: Origins of a concept of social affiliation]
To successfully navigate their social world, humans need to understand and map enduring relationships between people: We need a concept of social affiliation. Here I propose that the initial concept of social affiliation, available in infancy, is based on the extent to which one individual consistently takes on the goals and needs of another. This proposal grounds affiliation in intuitive psychology, as formalized in the naive-utility-calculus model. A concept of affiliation based on interpersonal utility adoption can account for findings from studies of infants’ reasoning about imitation, similarity, helpful and fair individuals, “ritual” behaviors, and social groups without the need for additional innate mechanisms such as a coalitional psychology, moral sense, or general preference for similar others. I identify further tests of this proposal and also discuss how it is likely to be relevant to social reasoning and learning across the life span.
[Neural signatures of distinct motives for infant looking]
Patterns of infant looking are used to test fundamental theories of the origins of
knowledge and to understand infants’ motives as active learners and social partners (1-5). Here
we find that early developing functional divisions in prefrontal cortex can serve as dissociable
neural signatures of two distinct motives for infant looking: social value and information value.
We used functional near-infrared spectroscopy (fNIRS) to measure prefrontal activation while 6-
to 12-month-old infants observed speakers who varied either in the positivity of their social cues
or in the statistical structure of their speech pattern. In left medial prefrontal cortex we observed
higher activation in response to the positive, high social value speaker, which predicted infants’
subsequent preferential looking to that speaker. In contrast, responses in left ventrolateral
prefrontal cortex to speakers’ statistical speech patterns predicted infants’ subsequent looking to
new speech that violated the established pattern. In the future, these dissociable responses in
prefrontal cortex could be used to parse the cognitive processes and motivations that underlie
human infants’ pursuit of learning and affiliation.
[Joint reasoning about social affiliation and emotion]
Social relationships powerfully influence human emotions. Understanding how relationships impact emotions allows people to make important social inferences, such as what will delight or upset someone else and which people are allies or enemies. Here we bring together research that has separately addressed reasoning about emotion and reasoning about affiliation, and propose a framework for how people engage in joint reasoning across these two domains. People expect others’ emotions to reflect their appraisals of a situation relative to their desires. People also expect others to value the welfare of friends, family, and group members. Because of this socially motivated concern, people’s appraisals–and therefore their emotions–ought to be affected by not only their own state of affairs but also the states of their friends and adversaries. An intuitive theory representing this connection between affiliation and emotion would allow observers to use relationships to predict others’ emotional responses and to infer relationships from observed emotional responses to social partners. We review evidence that infants are able to make emotional and affiliation inferences separately, and propose future work exploring infants’ ability to reason jointly about both domains. The early presence of this social cognitive framework could support learning from and about others.",,,34484675
115,1257,Aaditya,Prasad,aprasad@salk.edu,Salk Institute for Biological Studies,559-400-9927,San Diego,CA,92121,United States,"Natural Image Statistics and Modelling Neural Representations

Biological visual systems have evolved around the efficient coding of natural image statistics in order to support recognition of complex patterns. While it has been shown that deep neural networks exhibit biologically similar representations of images, it is unknown whether these representations emerge as a result of coding natural statistics. Here, we train self-supervised neural network models of the mouse visual cortex on various domains which exhibit different image properties. We find that networks trained on different domains tend to learn domain-specific features that are important for distinguishing objects into separate categories. As a result, we find that, in the earlier convolutional layers, artificial neural representations diverge from biological ones as a function of naturalness. Our results provide evidence for the idea that the visual cortex is adapted to features within natural domains and that the input stimuli is an important component to modelling the visual system.","Konrad Kording, Kanaka Rajan, Wei Ji Ma, Ida Momennejad, Daniel Yamins, James Dicarlo",,
116,1333,Arthur,Prat-Carrabin,arthur.p@columbia.edu,Columbia University,646-334-6962,New York City,NY,10027,United States,"[Bias and variance of the Bayesian-mean decoder]
Perception, in theoretical neuroscience, has been modeled as the encoding of external stimuli into internal signals, which are then decoded. The Bayesian mean is an important decoder, as it is optimal for purposes of both estimation and discrimination. We present widely-applicable approximations to the bias and to the variance of the Bayesian mean, obtained under the minimal and biologically-relevant assumption that the encoding results from a series of independent, though not necessarily identically-distributed, signals. Simulations substantiate the accuracy of our approximations in the small-noise regime. The bias of the Bayesian mean comprises two components: one driven by the prior, and one driven by the precision of the encoding. If the encoding is 'efficient', the two components have opposite effects; their relative strengths are determined by the objective that the encoding optimizes. The experimental literature on perception reports both 'Bayesian' biases directed towards prior expectations, and opposite, 'anti-Bayesian' biases. We show that different tasks are indeed predicted to yield such contradictory biases, under a consistently-optimal encoding-decoding model. Moreover, we recover Wei and Stocker's ""law of human perception"", a relation between the bias of the Bayesian mean and the derivative of its variance, and show how the coefficient of proportionality in this law depends on the task at hand. Our results provide a parsimonious theory of optimal perception under constraints, in which encoding and decoding are adapted both to the prior and to the task faced by the observer.

[Imprecise Probabilistic Inference from Sequential Data]
Although the Bayesian paradigm is an important benchmark in studies of human inference, the extent to which it provides a useful framework to account for human behavior remains debated. We document systematic departures from the predictions of Bayesian inference, even on average, in the estimates by experimental subjects of the probability of a binary event following observations of successive realizations of the event. In particular we find under-reaction of subjects' probability estimates to the presented evidence (""conservatism"") after only a few observations, and at the same time over-reaction to the evidence after a longer sequence of observations, which is not explained by an incorrect prior. We uncover the autocorrelation in subjects' estimates, which suggests that they carry imprecise representations of the decision situations, with noise in beliefs propagating over successive trials. But even taking into account these internal imprecisions, we find that the subjects' updates are inconsistent with the rules of Bayesian inference. We show how subjects instead considerably economize on the attention that they pay to the information relevant to decision, and on the degree of control that they exert over their precise response, while giving responses fairly adapted to the experimental task. A ""noisy counting"" model of probability estimation reproduces the several patterns we exhibit in subjects' behavior. In sum, human subjects in our task perform reasonably well while greatly minimizing the amount of information that they pay attention to. Our results emphasize that investigating this frugality in attention is crucial in understanding human decisions.

[Efficient coding of numbers explains decision bias and noise]
Humans differentially weight different stimuli in averaging tasks, which has been interpreted as reflecting encoding bias. We examine the alternative hypothesis that stimuli are encoded with noise and then optimally decoded. Under a model of efficient coding, the amount of noise should vary across stimuli and depend on statistics of the stimuli. We investigate these predictions through a task in which the participants are asked to compare the averages of two series of numbers, each sampled from a prior distribution that varies across blocks of trials. The participants encode numbers with a bias and a noise that both depend on the number. Infrequently occurring numbers are encoded with more noise. We show how an efficient-coding, Bayesian-decoding model accounts for these patterns and best captures the participants’ behaviour. Finally, our results suggest that Wei and Stocker’s “law of human perception”, which relates the bias and variability of sensory estimates, also applies to number cognition.","Samuel J. Gershman, Wei Ji Ma, Todd Gureckis, Catherine Hartley, Thomas Griffiths, Talia Konkle",,1418892691
117,1152,David Ricardo,Quiroga-Martinez,dquiroga@berkeley.edu,"University of California, Berkeley",+13656529,Berkeley,CA,94703,United States,"[neural dynamics of listened and imagined musical sound sequences]

Imagine a song you know by heart. With low effort you could play it vividly in your mind. However, little is known about how the brain represents and holds in mind such musical “thoughts”. Here, we leverage time-generalized decoding from MEG brain source activations to show that listened and imagined melodies are represented in auditory cortex, thalamus, middle cingulate cortex and precuneus. Accuracy patterns reveal that during listening and imagining sounds are represented as a melodic group, while during listening they are also represented individually. Opposite brain activation patterns distinguish between melodies during listening compared to imagining. Furthermore, encoding, imagining and retrieving melodies enhances delta and theta power in frontopolar regions, and suppresses alpha and beta power in sensorimotor and auditory regions. Our work sheds light on the neural dynamics of listened and imagined musical sound sequences.

",,Leyao Yu,1943769630
118,1392,Reza,Ramezan,rramezan@uwaterloo.ca,University of Waterloo,"+1-519-888-4567, ext. 48078",Waterloo,ON,N2L 3G1,Canada,"[A MULTIVARIATE MODEL FOR THE ANALYSIS OF NEURAL SPIKE TRAINS]
The current state-of-the-art technology in collecting neurophysiological data allows for simultaneous recording from hundreds
of neurons. A proper modelling approach for such data is the point process framework. However, the existing point process
models for simultaneous neural spike trains are computationally infeasible. In this work, we introduce a multi-neuron version
of our previous work on Skellam process with resetting (SPR). Unlike other models, the multivariate SPR is flexible in
capturing the correlation structure among spike trains, it is computationally efficient, and is also biologically justified due to
mimicking a neural integration process. Through both simulations and real-data analyses we highlight the strengths and
weaknesses of this model.

[Bridging the Gap : Statistics, Neuroscience, and Spike Trains]
Abstract:
A scientific model is a conceptual/physical representation of a phenomenon while a statistical model is a probabilistic object which is usually derived based on the collected data from a phenomenon. While the adage “all models are wrong but some are useful’’ applies, it is still possible to combine the two approaches to improve model performance and model justification. This talk explains why it is important to do so, and introduces the Skellam Process with Resetting (SPR): a simple yet powerful and flexible model for neural spike trains which is biologically justified. Within a point-process framework, the SPR model is motivated by neural integration: a biological process through which nerve cells combine the information they receive from other nerve cells. It will be shown that certain parametrizations of the SPR result in popular models for inter-spike intervals (inter-arrival times) such as Gamma, Inverse Gaussian, etc. Most importantly, SPR has a flexible multivariate generalization for the analysis of simultaneously recorded spike trains and population coding. The model strengths and weaknesses will be highlighted through real data analysis and simulations.

[Computational Neuroscience: A Romance of Many Disciplines]
Computational neuroscience studies the brain from an information processing point of view. A strong body of literature suggests that the nervous system has fundamental stochasticity, making statistical models very attractive in computational neuroscience. This being said, the adage ""all models are wrong, but some are useful"" applies. Among the useful is a collection of data-driven models tailored to understand neurophysiological phenomena and the process of information coding in the brain. This talk reviews, briefly, some of the common statistical methods and recent advancements in the analysis of neural spike trains. Data visualization, computational challenges, biological justification, and performance of these models will also be discussed in addition to exciting opportunities for future research.
","David Redish, ",,
119,1292,A. David,Redish,redish@umn.edu,University of Minnesota,612-626-3738,Minneapolis,MN,55455,United States,"[Computational Validity: Using Computation to translate behaviors across species].  
We propose a new conceptual framework (computational validity) for translation across species and populations based on the computational similarity between the information processing underlying parallel tasks. Translating between species depends not on the superficial similarity of the tasks presented, but rather on the computational similarity of the strategies and mechanisms that underlie those behaviours. Computational validity goes beyond construct validity by directly addressing questions of information processing. Computational validity interacts with circuit validity as computation depends on circuits, but similar computations could be accomplished by different circuits. Because different individuals may use different computations to accomplish a given task, computational validity suggests that behaviour should be understood through the subject's point of view; thus, behaviour should be characterized on an individual level rather than a task level. Tasks can constrain the computational algorithms available to a subject and the observed subtleties of that behaviour can provide information about the computations used by each individual. Computational validity has especially high relevance for the study of psychiatric disorders, given the new views of psychiatry as identifying and mediating information processing dysfunctions that may show high inter-individual variability, as well as for animal models investigating aspects of human psychiatric disorders.

[Avoid-approach conflict behaviors differentially affected by anxiolytics: implications for a computational model of risky decision-making] 
Whether fear or anxiety is expressed is thought to depend on an animal’s proximity to threat. In general, fear is elicited when threat is
proximal, while anxiety is a response to threat that is distal and uncertain. This threat gradient model suggests that fear and anxiety
involve non-overlapping neural circuitry, yet few behavioral paradigms exist that elicit both states. We studied avoid-approach
conflict in rats that were behaving in a predator-inhabited foraging arena task that involved tangible threat and reward incentives.
In the task, rats exhibited a variety of both fearful and anxious behaviors corresponding to proximal and distal threat, respectively.We
then administered ethanol or diazepam to the rats in order to study how anxiolytics affected these fear and anxiety behaviors. We
discovered that both ethanol and diazepam attenuated proximal-threat fear-like behaviors. Furthermore, we found that diazepam, but
not ethanol, increased distal-threat anxiety-like behavior but also made rats less risk-averse. Finally, we describe how decisional
conflict can be modeled as a partially observable Markov decision process and characterize a potential relationship between anxious
behavior, diazepam’s ability to suppress hippocampal theta oscillations, and hippocampal representations of the future.

[Information Processing in Decision-Making Systems]
Decisions result from an interaction between multiple functional systems acting in parallel to process information in very different ways, each with strengths and weaknesses. In this review, the authors address three action-selection components of decision-making: The Pavlovian system releases an action from a limited repertoire of potential actions, such as approaching learned stimuli. Like the Pavlovian system, the habit system is computationally fast but, unlike the Pavlovian system permits arbitrary stimulus-action pairings. These associations are a “forward’’ mechanism; when a situation is recognized, the action is released. In contrast, the deliberative system is flexible but takes time to process. The deliberative system uses knowledge of the causal structure of the world to search into the future, planning actions to maximize expected rewards. Deliberation depends on the ability to imagine future possibilities, including novel situations, and it allows decisions to be taken without having previously experienced the options. Various anatomical structures have been identified that carry out the information processing of each of these systems: hippocampus constitutes a map of the world that can be used for searching/imagining the future; dorsal striatal neurons represent situation-action associations; and ventral striatum maintains value representations for all three systems. Each system presents vulnerabilities to pathologies that can manifest as psychiatric disorders. Understanding these systems and their relation to neuroanatomy opens up a deeper way to treat the structural problems underlying various disorders.",,,
120,1150,Nico,Reeb,nico.reeb@tum.de,Technical University of Munich,+49-151-6410-1575,München,Bayern,80937,Germany,"[Probabilistic brains: knowns and unknowns]
There is strong behavioral and physiological evidence that the brain both represents probability distributions and performs probabilistic inference. Computational neuroscientists have started to shed light on how these probabilistic representations and computations might be implemented in neural circuits. One particularly appealing aspect of these theories is their generality: they can be used to model a wide range of tasks, from sensory processing to high-level cognition. To date, however, these theories have only been applied to very simple tasks. Here we discuss the challenges that will emerge as researchers start focusing their efforts on real-life computations, with a focus on probabilistic learning, structural learning and approximate inference.

[Synaptic plasticity as Bayesian inference]
Learning, especially rapid learning, is critical for survival. However, learning is hard; a large number of synaptic weights must be set based on noisy, often ambiguous, sensory information. In such a high-noise regime, keeping track of probability distributions over weights is the optimal strategy. Here we hypothesize that synapses take that strategy; in essence, when they estimate weights, they include error bars. They then use that uncertainty to adjust their learning rates, with more uncertain weights having higher learning rates. We also make a second, independent, hypothesis: synapses communicate their uncertainty by linking it to variability in postsynaptic potential size, with more uncertainty leading to more variability. These two hypotheses cast synaptic plasticity as a problem of Bayesian inference, and thus provide a normative view of learning. They generalize known learning rules, offer an explanation for the large variability in the size of postsynaptic potentials and make falsifiable experimental predictions.

[Spatio-temporal Representations of Uncertainty in Spiking Neural Networks]
It has been long argued that, because of inherent ambiguity and noise, the brain needs to represent uncertainty in the form of probability distributions. The neural encoding of such distributions remains however highly controversial. Here we
present a novel circuit model for representing multidimensional real-valued distributions using a spike based spatio-temporal code. Our model combines the computational advantages of the currently competing models for probabilistic codes
and exhibits realistic neural responses along a variety of classic measures. Furthermore, the model highlights the challenges associated with interpreting neural
activity in relation to behavioral uncertainty and points to alternative population level approaches for the experimental validation of distributed representations.

",,,
121,1191,Fabian,Renz,renz@mpib-berlin.mpg.de,Max Planck Institute for Human Development,+49-152-2850-3257,Berlin,,14195,Germany,"[Dynamics of fMRI patterns reflect sub-second activation sequences and reveal replay in human visual cortex]
Neural computations are often fast and anatomically localized. Yet, investigating such computations in humans is challenging because non-invasive methods have either high temporal or spatial resolution, but not both. Of particular relevance, fast neural replay is known to occur throughout the brain in a coordinated fashion about which little is known. We develop a multivariate analysis method for functional magnetic resonance imaging that makes it possible to study sequentially activated neural patterns separated by less than 100 ms with precise spatial resolution. Human participants viewed five images individually and sequentially with speeds up to 32ms between items. Probabilistic pattern classifiers were trained on activation patterns in visual and ventrotemporal cortex during individual image trials. Applied to sequence trials, probabilistic classifier time courses allow the detection of neural representations and their order. Order detection remains possible at speeds up to 32 ms between items (plus 100 ms per item). The frequency spectrum of the sequentiality metric distinguishes between sub- versus supra-second sequences. Importantly, applied to resting-state data our method reveals fast replay of task-related stimuli in visual cortex. This indicates that non-hippocampal replay occurs even after tasks without memory requirements and shows that our method can be used to detect such spontaneously occurring replay.

[Experience replay is associated with efficient non-local learning]
To make effective decisions we need to consider the relationship between actions and outcomes. These are often separated by time and space. The mechanisms spanning these gaps remain unknown. One promising hypothesis involves neural replay of non-local experience. Using a task segregating direct from indirect value learning, combined with magnetoencephalography, we examined the role of neural replay in human non-local learning. Following reward receipt, we found significant backward replay of non-local experience, with a 160 msec state-to-state time lag, this was linked to efficient learning of action values. Backward replay, and also behavioural evidence of non-local learning, was more pronounced for experiences of greater benefit for future behaviour. These findings support non-local replay as a neural mechanism for solving complex credit assignment problems during learning.

[Human Representation Learning]
The central theme of this review is the dynamic interaction between information selection and learning. We pose a fundamental question about this interaction: How do we learn what features of our experiences are worth learning about? In humans, this process depends on attention and memory, two cognitive functions that together constrain representations of the world to features that are relevant for goal attainment. Recent evidence suggests that the representations shaped by attention and memory are themselves inferred from experience with each task. We review this evidence and place it in the context of work that has explicitly characterized representation learning as statistical inference. We discuss how inference can be scaled to real-world decisions by approximating beliefs based on a small number of experiences. Finally, we highlight some implications of this inference process for human decision-making in social environments.
","Tobias Gerstenberg, Ida Mommenejad, Kimberly Stachenfeld, Nathaniel D. Daw, Erie Boorman, Mark Ho",,
122,1254,Ivan Felipe,Rodriguez,ivan_felipe_rodriguez@brown.edu,Brown University,+1-787-506-8722,Providence,Rhode Island,02906,United States,"[Deep neural networks face a fundamental trade-off to explain human vision]The many successes of deep neural networks (DNNs) over the past decade have largely been driven by computational scale rather than insights from biological intelligence. Here, we explore if these computational trends have brought concomitant improvements in explaining the visual strategies that humans use to recognize objects. We compare two related but distinct properties of visual strategies in humans and DNNs: \textit{where} they believe important visual features are in images and \textit{how} they use those features to categorize objects. We find a trade-off between the categorization accuracy of 85 different DNNs and the alignment of their visual strategies with humans. \textit{DNNs have progressively become \textit{less aligned} with humans as they have become more accurate at object classification}. We rectify this growing issue with our novel neural harmonizer, a general-purpose training routine that aligns DNN and human visual strategies while also improving object classification accuracy. Our work represents the first systematic demonstration that the scaling laws that have guided the development of DNNs have also produced worse models of human vision.",,,
123,1334,Constantin,Rothkopf,constantin.rothkopf@tu-darmstadt.de,Technische Universität Darmstadt,+49-6151-162-3367,Darmstadt,Hesse,64283,Germany,"[Inverse Optimal Control Adapted to the Noise Characteristics of the Human Sensorimotor System]
Computational level explanations based on optimal feedback control with signal-dependent noise have been able to account for a vast array of phenomena in human sensorimotor behavior. However, commonly a cost function needs to be assumed for a task and the optimality of human behavior is evaluated by comparing observed and predicted trajectories. Here, we introduce inverse optimal control with signal-dependent noise, which allows inferring the cost function from observed behavior. To do so, we formalize the problem as a partially observable Markov decision process and distinguish between the agent’s and the experimenter’s inference problems. Specifically, we derive a probabilistic formulation of the evolution of states and belief states and an approximation to the propagation equation in the linear-quadratic Gaussian problem with signal-dependent noise. We extend the model to the case of partial observability of state variables from the point of view of the experimenter. We show the feasibility of the approach through validation on synthetic data and application to experimental data. Our approach enables recovering the costs and benefits implicit in human sequential sensorimotor behavior, thereby reconciling normative and descriptive approaches in a computational framework.

[Multi-step planning of eye movements in visual search]
The capability of directing gaze to relevant parts in the environment is crucial for our survival. Computational models have proposed quantitative accounts of human gaze selection in a range of visual search tasks. Initially, models suggested that gaze is directed to the locations in a visual scene at which some criterion such as the probability of target location, the reduction of uncertainty or the maximization of reward appear to be maximal. But subsequent studies established, that in some tasks humans instead direct their gaze to locations, such that after the single next look the criterion is expected to become maximal. However, in tasks going beyond a single action, the entire action sequence may determine future rewards thereby necessitating planning beyond a single next gaze shift. While previous empirical studies have suggested that human gaze sequences are planned, quantitative evidence for whether the human visual system is capable of finding optimal eye movement sequences according to probabilistic planning is missing. Here we employ a series of computational models to investigate whether humans are capable of looking ahead more than the next single eye movement. We found clear evidence that subjects’ behavior was better explained by the model of a planning observer compared to a myopic, greedy observer, which selects only a single saccade at a time. In particular, the location of our subjects’ first fixation differed depending on the stimulus and the time available for the search, which was well predicted quantitatively by a probabilistic planning model. Overall, our results are the first evidence that the human visual system’s gaze selection agrees with optimal planning under uncertainty.

[Learning rational temporal eye movement strategies]
During active behavior humans redirect their gaze several times every second within the visual environment. Where we look within static images is highly efficient, as quantified by computational models of human gaze shifts in visual search and face recognition tasks. However, when we shift gaze is mostly unknown despite its fundamental importance for survival in a dynamic world. It has been suggested that during naturalistic visuomotor behavior gaze deployment is coordinated with task-relevant events, often predictive of future events, and studies in sportsmen suggest that timing of eye movements is learned. Here we establish that humans efficiently learn to adjust the timing of eye movements in response to environmental regularities when monitoring locations in the visual scene to detect probabilistically occurring events. To detect the events humans adopt strategies that can be understood through a computational model that includes perceptual and acting uncertainties, a minimal processing time, and, crucially, the intrinsic costs of gaze behavior. Thus, subjects traded off event detection rate with behavioral costs of carrying out eye movements. Remarkably, based on this rational bounded actor model the time course of learning the gaze strategies is fully explained by an optimal Bayesian learner with humans’ characteristic uncertainty in time estimation, the well-known scalar law of biological timing. Taken together, these findings establish that the human visual system is highly efficient in learning temporal regularities in the environment and that it can use these regularities to control the timing of eye movements to detect behaviorally relevant events.",,,
124,1389,Srihita,Rudraraju,srrudrar@ucsd.edu,"University of California,
San Diego",858-729-8493,La Jolla,CA,92092,United States,"[Substractive prediction error is encoded in the auditory human midbrain]
[Predictive Coding Dynamics Improve Noise Robustness in A Deep Neural Network of the Human Auditory System]",,,47573340
125,1179,Maria,Ruesseler,maria.ruesseler@gmail.com,Oxford University,+49-157-3794-0665,Lauterbach - Wernges,,36341,Germany,"[Adaptive circuit dynamics across human cortex during evidence accumulation in changing environments. ] 
Many decisions under uncertainty entail the temporal accumulation of evidence that informs about the state of the environ-ment. When environments are subject to hidden changes in their state, maximizing accuracy and reward requires non-linear accumulation of evidence. How this adaptive, non-linear computation is realized in the brain is unknown. We analyzed human behavior and cortical population activity (measured with magnetoencephalography) recorded during visual evidence accumu-lation in a changing environment. Behavior and decision-related activity in cortical regions involved in action planning exhibited hallmarks of adaptive evidence accumulation, which could also be implemented by a recurrent cortical microcircuit. Decision dynamics in action-encoding parietal and frontal regions were mirrored in a frequency-specific modulation of the state of the visual cortex that depended on pupil-linked arousal and the expected probability of change. These findings link normative deci-sion computations to recurrent cortical circuit dynamics and highlight the adaptive nature of decision-related feedback to the sensory cortex.

[Neurocomputational mechanisms of prior-informed perceptual decision-making in humans]
To interact successfully with diverse sensory environments, we must adapt our decision processes to account for time constraints and prior probabilities. The full set of decision-process parameters that undergo such flexible adaptation has proven to be difficult to establish using simplified models that are based on behaviour alone. Here, we utilize well-characterized human neurophysiological signatures of decision formation to construct and constrain a build-to-threshold decision model with multiple build-up (evidence accumulation and urgency) and delay components (pre- and post-decisional). The model indicates that all of these components were adapted in distinct ways and, in several instances, fundamentally differ from the conclusions of conventional diffusion modelling. The neurally informed model outcomes were corroborated by independent neural decision signal observations that were not used in the model’s construction. These findings highlight the breadth of decision-process parameters that are amenable to strategic adjustment and the value in leveraging neurophysiological measurements to quantify these adjustments.",,Laurence Hunt,
126,1403,Hansol,Ryu,hansolxryu@gmail.com,University of Calgary,4038070461,Calgary,AB,T2N 3E5,Canada,"[Is co-contraction good or bad for movement control? A simulation study using a single model for various movements] <INTRO> Co-contraction is a simultaneous activation of agonist-antagonist muscles, and in general believed to improve stability. Such co-contraction implies muscles contracting more than minimally needed to achieve the same motion and external forces. Therefore, higher level of co-contraction is often associated with increased metabolic cost, and thus seems to have a trade-off between its benefit and increased cost (Falisse A, et al. 2019). In contrast, some studies suggest that co-contraction may reduce metabolic cost when there is noise and delay in the system (Koelewijn AD, Van Den Bogert AJ, 2022). Previous studies often focused on the effect of the co-contraction level to a single type of a movement. Here, we simulated the effects of co-contraction and demonstrated its optimality during catching, balancing, and reaching tasks using a single model, by changing the cost functions, constraints, or types of disturbances it experiences during the simulation. We tested how the model incorporates co-contraction to achieve better precision, stability, or efficiency, and qualitatively compared it to observations of human movements in these tasks. <MODEL> Our simulation model consists of a one degree of freedom rotating rigid body, and two agonist-antagonist muscles that has identical muscle properties controlling the rigid body. We optimized muscle activation patterns and feedback gains of the controller, such that the resulting movement minimizes the cost function, which combines energetic cost and a task performance measure. Using the model, we reproduced experimental observations on the effect of co-contraction in various tasks. <SIMULATIONS> The catching tasks was formulated as a position control task in the presence of a sudden velocity and mass disturbance, and the simulation produced pre-activation and counter movement before the impact as observed in experiments. The reaching task was formulated as a control task where initial state and final state are specified. We tested the effect of a target size on the co-contraction level during reaching tasks by changing the error tolerance in the final position, and as observed in experiments, the simulation found a solution with more co-contraction for a smaller target. To model balancing tasks subject to unexpected disturbances, we ran the optimization algorithm multiple times with altered conditions and looked for a solution that performed overall the best. Our simulation model produced a solution with more co-contraction when there were more uncertainties, as seen in experimental observations. Moreover, when a large disturbance was applied, more co-contraction up to a certain level was more energetically efficient than not using co-contraction, as seen in some previous studies. <DISCUSSION> The framework here can help design new experiments to examine how nervous systems incorporate various objectives and constraints in control of movements in the presence of uncertainty.

[An optimality principle for locomotor central pattern generators] 
Two types of neural circuits contribute to legged locomotion: central pattern generators (CPGs) that produce rhythmic motor commands (even in the absence of feedback, termed “fictive locomotion”), and reflex circuits driven by sensory feedback. Each circuit alone serves a clear purpose, and the two together are understood to cooperate during normal locomotion. The difficulty is in explaining their relative balance objectively within a control model, as there are infinite combinations that could produce the same nominal motor pattern. Here we propose that optimization in the presence of uncertainty can explain how the circuits should best be combined for locomotion. The key is to re-interpret the CPG in the context of state estimator-based control: an internal model of the limbs that predicts their state, using sensory feedback to optimally balance competing effects of environmental and sensory uncertainties. We demonstrate use of optimally predicted state to drive a simple model of bipedal, dynamic walking, which thus yields minimal energetic cost of transport and best stability. The internal model may be implemented with neural circuitry compatible with classic CPG models, except with neural parameters determined by optimal estimation principles. Fictive locomotion also emerges, but as a side effect of estimator dynamics rather than an explicit internal rhythm. Uncertainty could be key to shaping CPG behavior and governing optimal use of feedback.
",,,
127,1411,Tiasha,Saha Roy,tsaharoy@umn.edu,University of Minnesota,(612) 449-7039,Minneapolis,Minnesota,55414,United States,"[Quantitative comparison of imagery and perception] 
Brain activity during mental imagery is often characterized as a reactivation of sensory activity that is especially robust in high-level visual areas and less so in low-level visual areas. However, brain areas vary considerably in their response to visual stimuli, and so it is possible that each area's engagement by imagery could depend upon the content of imagery itself. To investigate this possibility, we tested if the activity profile across the visual cortex remains stable when subjects imagine two different kinds of stimuli: simple oriented bars and crosses, and complex natural scenes. Specifically, we measured and mapped signal-to-noise ratios (SNR) during imagery and vision across the human visual system using 7T fMRI. We find that SNR profiles are highly stable across stimulus types: imagery SNR is identical to visual SNR in high-level visual areas, but dramatically lower than visual SNR in lower visual areas. These findings indicate that engagement of cortical visual areas during voluntary mental imagery is highly stereotyped, and varies much more across visual areas than it does across imagery content.

[Neural Timeline and Effect of Task Difficulty on Contextual Guidance of Visual Search in Natural Scenes]
Visual search is extremely critical in the real world and is often modulated by contextual guidance. The neural timeline underlying contextual guidance of visual search in natural scenes remains largely unexplored. We used multivariate pattern classifiers to predict coarse contextual locations from single trial electroencephalogram (EEG) signals during visual search in natural scenes in absence of targets. Participants searched for heterogeneous targets in natural images, and the target-absent images carrying information for possible target location provided by scene context were considered for analysis. Our results demonstrate that task consistent contextual locations in natural scenes can be predicted reliably pointing to the role of contextual information guided early deployment of spatial attention in visual search. Multivariate pattern analysis (MVPA) failed to predict the expected location using the same stimuli in a separate control EEG study when contextual information was made inconsequential. Using a source localization analysis of the event related potential (ERP), we could further explore the spatiotemporal dynamics to understand the neural timeline of contextual guidance. Finally, we demonstrated that for easy tasks, contextual guidance facilitation starts as early as 170 ms post stimulus onset  whereas, for difficult targets, the contextual effect was relevant at a later stage alluding to the role of task difficulty in mediating the neural timeline of contextual guidance.

[How Our Perception and Confidence is Altered Using Decision Cues]
Understanding how individuals utilize social information while making perceptual decisions and how it affects their decision confidence is crucial in a society. Till date, very little is known about perceptual decision making in humans and the associated neural mediators under social influence. The present study provides empirical evidence of how individuals get manipulated by others' decision while performing a face/car identification task. Subjects were significantly influenced by what they perceived as decisions of other subjects while the cues in reality were manipulated independently from the stimulus. Subjects in general tend to increase their decision confidence when their individual decision and cues coincide, while their confidence decreases when cues conflict with their individual judgments often leading to reversal of decision. Using a novel statistical model, it was possible to rank subjects based on their propensity to be influenced by cues. This was subsequently corroborated by analysis of their neural data. Neural time series analysis revealed no significant difference in decision making using social cues in the early stages unlike neural expectation studies with predictive cues. Multivariate pattern analysis of neural data alludes to a potential role of frontal cortex in the later stages of visual processing which appeared to code the effect of cues on perceptual decision making. Specifically medial frontal cortex seems to play a role in facilitating perceptual decision preceded by conflicting cues.
",,,
128,1099,Verena,Sarrazin,verena.sarrazin@psych.ox.ac.uk,University of Oxford,+44-7759-903-366,Oxford,Oxfordshire,OX3 7JX,United Kingdom,"[The effect of bifrontal transcranial direct current stimulation on reward learning in low mood]

Learning and decision-making processes are perturbed in several mental health conditions. Individuals with depression, for example, tend to show an affective bias, prioritizing the processing of negative over positive outcomes. This bias is thought to causally maintain low mood. During learning, outcomes which are informative (e.g. that occur in a volatile rather than stable context) exert a greater influence on beliefs. Patients' estimates of the information content of positive and negative outcomes may therefore underlie negative biases and may present a novel intervention target to remediate symptoms of depression. Transcranial direct current stimulation (tDCS) applied to the dorsolateral prefrontal cortex (DLPFC) has been shown to have small to moderate antidepressant effects. In clinical trials tDCS is typically applied at rest. However, tDCS has been shown to boost learning effects and might therefore be more effective if applied during a learning task. In our previous study, we found that tDCS applied to the DLPFC during a reinforcement learning task selectively increases learning from positive outcomes in healthy individuals. This effect might be beneficial in depression treatment to counteract negative biases. The aim of the present study was to test whether bifrontal tDCS applied during reinforcement learning can increase learning from positive outcomes in participants suffering from low mood. We hypothesised that tDCS applied during, but not before the learning task would increase measures of positive learning biases. 85 participants with a BDI score of 10 or above received tDCS during (n = 41) or before (n = 44) performing a learning task. Each participant attended two sessions in which real or sham tDCS was applied in counter-balanced order. The paradigm was the Information Bias Learning Task in which the volatility of win and loss outcomes was manipulated independently so that learning rates could be estimated separately for wins and losses. Our results indicate that bifrontal tDCS did not increase win learning rates in participants with low mood, independent of the time point of stimulation. Exploratory analyses revealed that tDCS applied during task performance increased the adjustment of loss learning rates between stable and volatile conditions. This finding is of potential clinical relevance since prior research suggests that anxiety and depression are associated with a deficit of adjusting learning rates to volatility. Further research should replicate this effect and test whether it can be increased through multiple sessions of combined tDCS and reinforcement learning.
",,,
129,1267,Thomas,Schatz,thomas.schatz@univ-amu.fr,Aix-Marseille University,+33-6-32-01-16-60,Marseille,,13008,France,"Before they even speak, infants become attuned to the sounds
of the language(s) they hear, processing native phonetic con-
trasts more easily than nonnative ones. For example, between
6 to 8 mo and 10 to 12 mo, infants learning American English
get better at distinguishing English and [l], as in “rock” vs.
“lock,” relative to infants learning Japanese. Influential accounts
of this early phonetic learning phenomenon initially proposed
that infants group sounds into native vowel- and consonant-like
phonetic categories—like and [l] in English—through a sta-
tistical clustering mechanism dubbed “distributional learning.”
The feasibility of this mechanism for learning phonetic cate-
gories has been challenged, however. Here, we demonstrate
that a distributional learning algorithm operating on naturalis-
tic speech can predict early phonetic learning, as observed in
Japanese and American English infants, suggesting that infants
might learn through distributional learning after all. We fur-
ther show, however, that, contrary to the original distributional
learning proposal, our model learns units too brief and too fine-
grained acoustically to correspond to phonetic categories. This
challenges the influential idea that what infants learn are pho-
netic categories. More broadly, our work introduces a mechanism-
driven approach to the study of early phonetic learning, together
with a quantitative modeling framework that can handle real-
istic input. This allows accounts of early phonetic learning to
be linked to concrete, systematic predictions regarding infants’
attunement.

The way listeners perceive speech sounds is largely de-
termined by the language(s) they were exposed to as a
child. For example, native speakers of Japanese have
a hard time discriminating between American English /ô/
and /l/, a phonetic contrast that has no equivalent in
Japanese. Such effects are typically attributed to knowl-
edge of sounds in the native language, but quantitative
models of how these effects arise from linguistic knowl-
edge are lacking. One possible source for such mod-
els is Automatic Speech Recognition (ASR) technology.
We implement models based on two types of systems
from the ASR literature—hidden Markov models (HMMs)
and the more recent, and more accurate, neural network
systems—and ask whether, in addition to showing better
performance, the neural network systems also provide
better models of human perception. We find that while
both types of systems can account for Japanese natives’
difficulty with American English /ô/ and /l/, only the neural
network system successfully accounts for Japanese na-
tives’ facility with Japanese vowel length contrasts. Our
work provides a new example, in the domain of speech
perception, of an often observed correlation between task
performance and similarity to human behavior.","Josh McDermott, Nikolaus Kriegeskorte",,
130,1432,Audrey,Sederberg,sede0018@umn.edu,University of Minnesota,609-240-1302,St. Paul,MN,55105,United States,"Understanding the activity of large populations of neurons is difficult due to the combinatorial complexity of possible cell-cell interactions. To reduce the complexity, coarse graining had been previously applied to experimental neural recordings, which showed over two decades of apparent scaling in free energy, activity variance, eigenvalue spectra, and correlation time, hinting that the mouse hippocampus operates in a critical regime. We model such data by simulating conditionally independent binary neurons coupled to a small number of long-timescale stochastic fields and then replicating the coarse-graining procedure and analysis. This reproduces the experimentally observed scalings, suggesting that they do not require fine-tuning of internal parameters, but will arise in any system, biological or not, where activity variables are coupled to latent dynamic stimuli. Parameter sweeps for our model suggest that emergence of scaling requires most of the cells in a population to couple to the latent stimuli, predicting that even the celebrated place cells must also respond to nonplace stimuli.

Modern recording methods enable sampling of thousands of neurons during the perfor- mance of behavioral tasks, raising the question of how recorded activity relates to theoreti- cal models. In the context of decision making, functional connectivity between choice- selective cortical neurons was recently reported. The straightforward interpretation of these data suggests the existence of selective pools of inhibitory and excitatory neurons. Compu- tationally investigating an alternative mechanism for these experimental observations, we find that a randomly connected network of excitatory and inhibitory neurons generates sin- gle-cell selectivity, patterns of pairwise correlations, and the same ability of excitatory and inhibitory populations to predict choice, as in experimental observations. Further, we predict that, for this task, there are no anatomically defined subpopulations of neurons representing choice, and that choice preference of a particular neuron changes with the details of the task. We suggest that distributed stimulus selectivity and functional organization in population codes could be emergent properties of randomly connected networks.
Learning to make external sensory stimulus predictions using internal correlations in populations of neurons
To compensate for sensory processing delays, the visual system must make predictions to ensure timely and appropriate behav- iors. Recent work has found predictive information about the stimulus in neural populations early in vision processing, starting in the retina. However, to utilize this information, cells down- stream must be able to read out the predictive information from the spiking activity of retinal ganglion cells. Here we investigate whether a downstream cell could learn efficient encoding of predictive information in its inputs from the correlations in the inputs themselves, in the absence of other instructive signals. We simulate learning driven by spiking activity recorded in salaman- der retina. We model a downstream cell as a binary neuron receiving a small group of weighted inputs and quantify the predictive information between activity in the binary neuron and future input. Input weights change according to spike timing– dependent learning rules during a training period. We characterize the readouts learned under spike timing–dependent synaptic up- date rules, finding that although the fixed points of learning dy- namics are not associated with absolute optimal readouts they convey nearly all of the information conveyed by the optimal readout. Moreover, we find that learned perceptrons transmit po- sition and velocity information of a moving-bar stimulus nearly as efficiently as optimal perceptrons. We conclude that predictive in- formation is, in principle, readable from the perspective of down- stream neurons in the absence of other inputs. This suggests an important role for feedforward prediction in sensory encoding.
",,,
131,1204,Philipp,Seidel,philipp.seidel@klinik.uni-regensburg.de,Regensburg University,+49-157-5425-7479,Regensburg,Bavaria,93053,Germany,"[I got the power: Learning to self-regulate brain states by means of real-time fMRI neurofeedback]
Patients suffering from emotion dysfunctions, such as anxiety or depression, may be stuck in brain states that cause or are a consequence of such disorders. Previous studies showed that it is possible, e.g., for chronic pain patients to reduce their pain levels after learning to downregulate the amplitude of the BOLD response in a certain brain area by means of neurofeedback. These effects led to the question whether it may be possible for us humans to learn to regulate whole-brain patterns to switch from, e.g., a “sad” brain state into a “happy” brain state. If possible, we could use this approach in patients with emotion dysfunctions to learn to drive their brains out of brain states associated with their disorder. To this end, I want to investigate this approach in the upcoming years by combining operant conditioning, real-time fMRI neurofeedback, and deep learning. This approach could be used as a potential complimentary therapy alongside or even replace e.g., medication or psychotherapy.  

[Real-time fMRI using brain-state classification] (not my own)
We have implemented a real-time functional magnetic resonance imaging system based on multivariate classification. This approach is distinctly different from spatially localized real-time implementations, since it does not require prior assumptions about functional localization and individual performance strategies, and has the ability to provide feedback based on intuitive translations of brain state rather than localized fluctuations. Thus this approach provides the capability for a new class of experimental designs in which real-time feedback control of the stimulus is possible—rather than using a fixed paradigm, experiments can adaptively evolve as subjects receive brain-state feedback. In this report, we describe our implementation and characterize its performance capabilities. We observed ~80% classification accuracy using whole brain, block-design, motor data. Within both left and right motor task conditions, important differences exist between the initial transient period produced by task switching (changing between rapid left or right index finger button presses) and the subsequent stable period during sustained activity. Further analysis revealed that very high accuracy is achievable during stable task periods, and that the responsiveness of the classifier to changes in task condition can be much faster than signal time-to-peak rates. Finally, we demonstrate the versatility of this implementation with respect to behavioral task, suggesting that our results are applicable across a spectrum of cognitive domains. Beyond basic research, this technology can complement electroencephalography-based brain computer interface research, and has potential applications in the areas of biofeedback rehabilitation, lie detection, learning studies, virtual reality-based training, and enhanced conscious awareness.

[Scientific Exploration and Explainable Artificial Intelligence] (not my own)
Models developed using machine learning are increasingly prevalent in scientific research. At the same time, these models are notoriously opaque. Explainable AI aims to mitigate the impact of opacity by rendering opaque models transparent. More than being just the solution to a problem, however, Explainable AI can also play an invaluable role in scientific exploration. This paper describes how post-hoc analytic techniques from Explainable AI can be used to refine target phenomena in medical science, to identify starting points for future investigations of (potentially) causal relationships, and to generate possible explanations of target phenomena in cognitive science. In this way, this paper describes how Explainable AI—over and above machine learning itself—contributes to the efficiency and scope of data-driven scientific research.",,,
132,1211,Yuta,Senzai,yuta.senzai@gmail.com,"University of California,
San Francisco",347-522-1521,San Francisco,CA,94107,United States,"[A cognitive process occurring during sleep revealed by rapid eye movements] Since the discovery of REM sleep, the nature of the rapid eye movements that characterize this sleep phase has remained elusive. Do they reveal gaze shifts in the virtual environment of dreams or simply reflect random brainstem activity? We harnessed the head direction (HD) system of the mouse thalamus, a neuronal population whose activity reports, in awake mice, their actual HD as they explore their environment and, in sleeping mice, their virtual HD. We discovered that the direction and amplitude of rapid eye movements during REM sleep reveal the direction and amplitude of the ongoing changes in virtual HD. Thus, rapid eye movements disclose gaze shifts in the virtual world of REM sleep, thereby providing a window in the cognitive processes of the sleeping brain.

[Layer-Specific Physiological Features and Interlaminar Interactions in the Primary Visual Cortex of the Mouse] The relationship between mesoscopic local field potentials (LFPs) and single-neuron firing in the multi-layered neocortex is poorly understood. Simultaneous recordings from all layers in the primary visual cortex (V1) of the behaving mouse revealed functionally defined layers in V1. The depth of maximum spike power and sink-source distributions of LFPs provided
consistent laminar landmarks across animals. Coherence of gamma oscillations (30–100 Hz) and spike-LFP coupling identified six physiological layers and
further sublayers. Firing rates, burstiness, and other electrophysiological features of neurons displayed unique layer and brain state dependence. Spike transmission strength from layer 2/3 cells to layer 5 pyramidal cells and interneurons was stronger during waking compared with non-REM sleep but stronger during non-REM sleep among deep-layer excitatory neurons. A subset of deep-layer neurons was active exclusively in the DOWN state of non-REM sleep. These results bridge mesoscopic LFPs and single neuron interactions with laminar structure in V1.

[Physiological Properties and Behavioral Correlates of Hippocampal Granule Cells and Mossy Cells] The hippocampal dentate gyrus is often viewed as a segregator of upstream information. Physiological support for such function has been hampered by a lack of well-defined characteristics that can identify granule cells and mossy cells. We developed an electrophysiology-based classification of dentate granule cells and mossy cells in mice that we validated by optogenetic tagging of mossy cells. Granule cells exhibited sparse firing, had a single place field, and showed only modest changes when the mouse was tested in different mazes in the same room. In contrast, mossy cells were more active, had multiple place fields and showed stronger remapping of place fields under the same conditions. Although the granule cell-mossy cell synapse was strong and facilitating, mossy cells rarely “inherited” place fields from single granule cells. Our findings suggest that the granule cells and mossy cells could be modulated separately and their joint action may be critical for pattern separation.",,,
133,1186,Janaki,Sheth,janaki.jrs@gmail.com,University of Pennsylvania,424-522-6427,Philadelphia,PA,19146,United States,"[Relevance, integration and learning influence sound categorization]
Auditory categorization e.g., identifying a song in a noisy environment, is a complex process. After all, our perception is corrupted by sensory uncertainty. Additionally, only some sounds that we hear may be relevant to the song. Thus, when making our decision, we need to appropriately integrate over the different sounds. Lastly, early verses in a song generally predict latter verses, making learning over time important. Here, we study how the interaction of these varied factors shapes human decision-making by formalizing multi-tone sound categorization as a Bayesian model and testing it with new behavioral experiments. We find that participants integrate tones that arrive as a sequence, while accounting for the relevance of the different tones. Additionally, history of categories modulates behavior. Importantly, our model reveals estimates of sensory uncertainty, relevance, integration, and learning, giving us a rich set of variables to parse the mechanisms of categorization in the brain.

[Bootstrapping Multilingual AMR with Contextual Word Alignments]
We develop high performance multilingual Abstract Meaning Representation (AMR) systems by projecting English AMR annotations to other languages with weak supervision. We achieve this goal by bootstrapping transformer-based multilingual word embeddings, in particular those from cross-lingual RoBERTa (XLM-R large). We develop a novel technique for foreign-text-to-English AMR alignment, using the contextual word alignment between English and foreign language tokens. This word alignment is weakly supervised and relies on the contextualized XLM-R word embeddings. We achieve a highly competitive performance that surpasses the best published results for German, Italian, Spanish and Chinese.

[Generalizing neural signal-to-text brain-computer interfaces]
Objective: Brain-Computer Interfaces (BCI) may help patients with faltering communication abilities due to neurodegenerative diseases produce text or speech by direct neural processing. However, their practical realization has proven difficult due to limitations in speed, accuracy, and generalizability of existing interfaces. The goal of this study is to evaluate the BCI performance of a robust speech decoding system that translates neural signals evoked by speech to a textual output. While previous studies have approached this problem by using neural signals to choose from a limited set of possible words, we employ a more general model that can type any word from a large corpus of English text. Approach: In this study, we create an end-to-end BCI that translates neural signals associated with overt speech into text output. Our decoding system first isolates frequency bands in the input depth-electrode signal encapsulating differential information regarding production of various phonemic classes. These bands form a feature set that then feeds into a Long Short-Term Memory (LSTM) model which discerns at each time point probability distributions across all phonemes uttered by a subject. Finally, a particle filtering algorithm temporally smooths these probabilities by incorporating prior knowledge of the English language to output text corresponding to the decoded word. The generalizability of our decoder is driven by the lack of a vocabulary constraint on this output word. Main result: This method was evaluated using a dataset of 6 neurosurgical patients implanted with intra-cranial depth electrodes to identify seizure foci for potential surgical treatment of epilepsy. We averaged 32% word accuracy and on the phoneme-level obtained 46% precision, 51% recall and 73.32% average phoneme error rate while also achieving significant increases in speed when compared to several other BCI approaches. Significance: Our study employs a more general neural signal-to-text model which could facilitate communication by patients in everyday environments.
",,Konrad Kording,
134,1436,Katerina,Simkova,KMS863@student.bham.ac.uk,University of Birmingham,07841184055,Birmingham,,B17 9BF,United Kingdom,"[The role of semantics in similarity judgements of scene stimuli ]
The conscious experience of one’s visual diet is one of full complexity and yet, the mental 
representations of visual objects and natural scenes can be quantified as a geometric model 
reflecting the relationship between the perceived images in a high-dimensional space. To 
better understand the level of description of the representational space captured with multiple 
arrangements (MA) of natural scene stimuli, we tested 58 participants across three different 
MA tasks arranging images, hidden images, or captions describing the images. We observed 
significant correlations between image similarity judgements and sentence similarity 
judgements; and stronger within-subject correlations, which indicate the presence of 
representational idiosyncrasies in one’s similarity judgements. Additionally, the behavioural 
representational dissimilarity matrices (RDMs) were significantly correlated with sentence 
embeddings as measured with Google Universal Sentence Encoder. Altogether our results 
provide evidence that multiple arrangements capture semantic-level representations

[Investigating high-level visual representations and their idiosyncratic nature in 
novel behavioural experiments]
While we have a fair understanding of the early transformations of visual information, much 
uncertainty still exists about the relationship between vision and semantics. Here, we use a novel 
semantic priming experiment to investigate the extent to which the semantic proximity of a prime and 
target influences visual search. We hypothesize that the semantically closer the primed sentence is to
the target image, the faster the reaction times. Although our results are not completely in line with 
this hypothesis, we found a weak negative correlation (r = -0.22) in two subjects. As a measure of the 
individual perception styles, we combine this experiment with a multiple arrangements task to infer 
the individually unique representational space of our stimulus set. We found some commonalities in 
the representations of sentence captions and natural scenes with a strong within-subject correlation.
These preliminary results suggest that our methods can account for the individually unique 
representations. But the visuo-semantic features of caption-level and natural scenes representations
and their relationship to a subject’s perceptual judgements are yet to be explored.

[Vietnamese-Czech bilingual adolescents on EEG - bilingual effect on cognitive advantage]
Neuroplasticity is an adaptive attribute of the constantly changing brain. Without this ability it would be impossible to develop throughout life or recover from a brain injury; we would be unable to adapt to the environment or learn. In this thesis, we review how early second language learning modifies the development of executive functions. In the practical part, we investigated how the so-called bilingual cognitive advantage facilitates the suppression of distractors using behavioural and event-related potentials (ERPs) measures. We hypothesized that the bilingual group would outperform the monolingual group in distractor-present events, and we expected to record changes in P3 amplitude. To elicit the inhibitory control, all subjects (n = 36) performed a letter detection task with auditory distractors. No differences in behavioural performance – reaction times and accuracy – were identified. But ERPs showed larger P3 amplitudes in the bilingual group – this might indicate better discrimination between the target and distractor.",,,
135,1073,Paul,Soulos,psoulos1@jh.edu,The Johns Hopkins University,917-669-0650,Baltimore,MD,21211,United States,"[Discovering the Compositional Structure of Vector Representations with Role Learning Networks]
How can neural networks perform so well on compositional tasks even though they lack explicit compositional representations? We use a novel analysis technique called ROLE to show that recurrent neural networks perform well on such tasks by converging to solutions which implicitly represent symbolic structure. This method uncovers a symbolic structure which, when properly embedded in vector space, closely approximates the encodings of a standard seq2seq network trained to perform the compositional SCAN task. We verify the causal importance of the discovered symbolic structure by showing that, when we systematically manipulate hidden embeddings based on this symbolic structure, the model’s output is changed in the way predicted by our analysis.

[Disentangled Face Representations in Deep Generative Models and the Human Brain]
Despite decades of research, much is still unknown about the computations carried out in the human face processing network. Recently deep networks have been proposed as a computational account of human visual processing. While they provide a good match to neural data throughout visual cortex, they lack interpretability. Here we use a new class of deep generative models, disentangled representation learning models, which learn a low-dimensional latent space that “disentangles” different interpretable dimensions of faces, such as rotation, lighting, or hairstyle. We show that these disentangled networks are a good encoding model for human fMRI data and allow us to investigate how semantically meaningful face features are represented in the brain. We find that several interpretable dimensions, including both identity-specific and identity-invariant dimensions, are distributed widely across the face processing system. The remaining “entangled” representations may be the basis of identity recognition in the brain. These disentangled encoding models provide an exciting alternative to standard “black box” deep learning models and have the potential to change the way we understand face processing in the human brain.","Irina Higgins, Felix Hill, Jacob Andreas, Andrew Lampinen, Doris Tsao, Luis Lamb",,46182659
136,1216,Simon,Steinkamp,simons@drcmr.dk,Copenhagen University Hospital - Amager and Hvidovre,+49-171-150-6090,Copenhagen,,2650,Denmark,"[Ergodicity-breaking reveals time optimal decision making in humans]
Ergodicity describes an equivalence between the expectation value and the time average of observables. Applied to human behaviour, ergodic theories of decision-making reveal how individuals should tolerate risk in different environments. To optimise wealth over time, agents should adapt their utility function according to the dynamical setting they face. Linear utility is optimal for additive dynamics, whereas logarithmic utility is optimal for multiplicative dynamics. Whether humans approximate time optimal behavior across different dynamics is unknown. Here we compare the effects of additive versus multiplicative gamble dynamics on risky choice. We show that utility functions are modulated by gamble dynamics in ways not explained by prevailing decision theories. Instead, as predicted by time optimality, risk aversion increases under multiplicative dynamics, distributing close to the values that maximise the time average growth of in-game wealth. We suggest that our findings motivate a need for explicitly grounding theories of decision-making on ergodic considerations.

[A distributional code for value in dopamine-based reinforcement learning]
Since its introduction, the reward prediction error (RPE) theory of dopamine has explained a wealth of empirical phenomena, providing a unifying framework for understanding the representation of reward and value in the brain1–3. According to the now canonical theory, reward predictions are represented as a single scalar quantity, which supports learning about the expectation, or mean, of stochastic outcomes. In the present work, we propose a novel account of dopamine-based reinforcement learning. Inspired by recent artificial intelligence research on distributional reinforcement learning4–6, we hypothesized that the brain represents possible future rewards not as a single mean, but instead as a probability distribution, effectively representing multiple future outcomes simultaneously and in parallel. This idea leads immediately to a set of empirical predictions, which we tested using single-unit recordings from mouse ventral tegmental area. Our findings provide strong evidence for a neural realization of distributional reinforcement learning.

[Dynamic causal modelling of brain-behaviour relationships]
In this work, we expose a mathematical treatment of brain-behaviour relationships, which we coin behavioural Dynamic Causal Modelling or bDCM. This approach aims at decomposing the brain's transformation of stimuli into behavioural outcomes, in terms of the relative contribution of brain regions and their connections. In brief, bDCM places the brain at the interplay between stimulus and behaviour: behavioural outcomes arise from coordinated activity in (hidden) neural networks, whose dynamics are driven by experimental inputs. Estimating neural parameters that control network connectivity and plasticity effectively performs a neurobiologically-constrained approximation to the brain's input-outcome transform. In other words, neuroimaging data essentially serves to enforce the realism of bDCM's decomposition of input-output relationships. In addition, post-hoc artificial lesions analyses allow us to predict induced behavioural deficits and quantify the importance of network features for funnelling input-output relationships. This is important, because this enables one to bridge the gap with neuropsychological studies of brain-damaged patients. We demonstrate the face validity of the approach using Monte-Carlo simulations, and its predictive validity using empirical fMRI/behavioural data from an inhibitory control task. Lastly, we discuss promising applications of this work, including the assessment of functional degeneracy (in the healthy brain) and the prediction of functional recovery after lesions (in neurological patients).

",,,12622663
137,1064,Dominik,Straub,dominik.straub@tu-darmstadt.de,Technische Universität Darmstadt,+49-157-3317-0826,Darmstadt,,64283,Germany,"Putting perception into action: Inverse optimal control for continuous psychophysics
Psychophysical methods are a cornerstone of psychology, cognitive science, and neuroscience where they have been used to quantify behavior and its neural correlates for a vast range of mental phenomena. Their power derives from the combination of controlled experiments and rigorous analysis through signal detection theory. Unfortunately, they require many tedious trials and preferably highly trained participants. A recently developed approach, continuous psychophysics, promises to transform the field by abandoning the rigid trial structure involving binary responses and replacing it with continuous behavioral adjustments to dynamic stimuli. However, what has precluded wide adoption of this approach is that current analysis methods recover perceptual thresholds, which are one order of magnitude larger compared to equivalent traditional psychophysical experiments. Here we introduce a computational analysis framework for continuous psychophysics based on Bayesian inverse optimal control. We show via simulations and on previously published data that this not only recovers the perceptual thresholds but additionally estimates subjects’ action variability, internal behavioral costs, and subjective beliefs about the experimental stimulus dynamics. Taken together, we provide further evidence for the importance of including acting uncertainties, subjective beliefs, and, crucially, the intrinsic costs of behavior, even in experiments seemingly only investigating perception.

Inverse Optimal Control Adapted to the Noise Characteristics of the Human Sensorimotor System
Computational level explanations based on optimal feedback control with signal-dependent noise have been able to account for a vast array of phenomena in human sensorimotor behavior. However, commonly a cost function needs to be assumed for a task and the optimality of human behavior is evaluated by comparing observed and predicted trajectories. Here, we introduce inverse optimal control with signal-dependent noise, which allows inferring the cost function from observed behavior. To do so, we formalize the problem as a partially observable Markov decision process and distinguish between the agent’s and the experimenter’s inference problems. Specifically, we derive a probabilistic formulation of the evolution of states and belief states and an approximation to the propagation equation in the linear-quadratic Gaussian problem with signal-dependent noise. We extend the model to the case of partial observability of state variables from the point of view of the experimenter. We show the feasibility of the approach through validation on synthetic data and application to experimental data. Our approach enables recovering the costs and benefits implicit in human sequential sensorimotor behavior, thereby reconciling normative and descriptive approaches in a computational framework.

Looking for Image Statistics: Active Vision With Avatars in a Naturalistic Virtual Environment
The efficient coding hypothesis posits that sensory systems are tuned to the regularities of their natural input. The statistics of natural image databases have been the topic of many studies, which have revealed biases in the distribution of orientations that are related to neural representations as well as behavior in psychophysical tasks. However, commonly used natural image databases contain images taken with a camera with a planar image sensor and limited field of view. Thus, these images do not incorporate the physical properties of the visual system and its active use reflecting body and eye movements. Here, we investigate quantitatively, whether the active use of the visual system influences image statistics across the visual field by simulating visual behaviors in an avatar in a naturalistic virtual environment. Images with a field of view of 120° were generated during exploration of a virtual forest environment both for a human and cat avatar. The physical properties of the visual system were taken into account by projecting the images onto idealized retinas according to models of the eyes' geometrical optics. Crucially, different active gaze behaviors were simulated to obtain image ensembles that allow investigating the consequences of active visual behaviors on the statistics of the input to the visual system. In the central visual field, the statistics of the virtual images matched photographic images regarding their power spectra and a bias in edge orientations toward cardinal directions. At larger eccentricities, the cardinal bias was superimposed with a gradually increasing radial bias. The strength of this effect depends on the active visual behavior and the physical properties of the eye. There were also significant differences between the upper and lower visual field, which became stronger depending on how the environment was actively sampled. Taken together, the results show that quantitatively relating natural image statistics to neural representations and psychophysical behavior requires not only to take the structure of the environment into account, but also the physical properties of the visual system, and its active use in behavior.",,,117610362
138,1262,Ghislain,St-Yves,gstyves@umn.edu,University of Minessota,843-230-1695,Mineapolis,MN,55455,United States,"[Individual differences among deep neural network models]
Deep neural networks (DNNs) excel at visual recognition tasks and are increasingly used as a
modeling framework for neural computations in the primate brain. Just like individual brains,
each DNN has a unique connectivity and representational profile. Here, we investigate
individual differences among DNN instances that arise from varying only the random initialization
of the network weights. Using tools typically employed in systems neuroscience, we
show that this minimal change in initial conditions prior to training leads to substantial
differences in intermediate and higher-level network representations despite similar networklevel
classification performance. We locate the origins of the effects in an under-constrained
alignment of category exemplars, rather than misaligned category centroids. These results
call into question the common practice of using single networks to derive insights into neural
information processing and rather suggest that computational neuroscientists working with
DNNs may need to base their inferences on groups of multiple network instances.

[Cortical-like dynamics in recurrent circuits optimized for sampling-based probabilistic inference]
Sensory cortices display a suite of ubiquitous dynamical features, such as ongoing noise variability, transient overshoots and
oscillations, that have so far escaped a common, principled theoretical account. We developed a unifying model for these
phenomena by training a recurrent excitatory–inhibitory neural circuit model of a visual cortical hypercolumn to perform
sampling-based probabilistic inference. The optimized network displayed several key biological properties, including divisive
normalization and stimulus-modulated noise variability, inhibition-dominated transients at stimulus onset and strong
gamma oscillations. These dynamical features had distinct functional roles in speeding up inferences and made predictions
that we confirmed in novel analyses of recordings from awake monkeys. Our results suggest that the basic motifs of cortical
dynamics emerge as a consequence of the efficient implementation of the same computational function—fast sampling-based
inference—and predict further properties of these motifs that can be tested in future experiments.

[Quantitative models reveal the organization of diverse cognitive functions in the brain]
Our daily life is realized by the complex orchestrations of diverse brain functions, including
perception, decision-making, and action. The essential goal of cognitive neuroscience is to
reveal the complete representations underlying these functions. Recent studies have characterised
perceptual experiences using encoding models. However, few attempts have been
made to build a quantitative model describing the cortical organization of multiple active,
cognitive processes. Here, we measure brain activity using fMRI, while subjects perform 103
cognitive tasks, and examine cortical representations with two voxel-wise encoding models.
A sparse task-type model reveals a hierarchical organization of cognitive tasks, together with
their representation in cognitive space and cortical mapping. A cognitive factor model utilizing
continuous, metadata-based intermediate features predicts brain activity and decodes
tasks, even under novel conditions. Collectively, our results show the usability of quantitative
models of cognitive processes, thus providing a framework for the comprehensive cortical
organization of human cognition.",,"Jesse Breedlove
Logan Dowdle
Thomas Naselaris
Kendrick Kay
Tiasha Roy",
139,1425,Qinhua,Sun,qinhuas@uci.edu,"University of California, Irvine",8645016625,Irvine,CA,92697,United States,"[Decision SincNet: Neurocognitive models of decision making that predict cognitive processes from neural signals]
Human decision making behavior is observed with choice-response time data during psychological experiments. Drift-diffusion models of this data consist of a Wiener first-passage time (WFPT) distribution and are described by cognitive parameters: drift rate, boundary separation, and starting point. These estimated parameters are of interest to neuroscientists as they can be mapped to features of cognitive processes of decision making (such as speed, caution, and bias) and related to brain activity. The observed patterns of RT also reflect the variability of cognitive processes from trial to trial mediated by neural dynamics. We adapted a SincNet-based shallow neural network architecture to fit the Drift-Diffusion model using EEG signals on every experimental trial. The model consists of a SincNet layer, a depthwise spatial convolution layer, and two separate FC layers that predict drift rate and boundary for each trial in-parallel. The SincNet layer parametrized the kernels in order to directly learn the low and high cutoff frequencies of bandpass filters that are applied to the EEG data to predict drift and boundary parameters. During training, model parameters were updated by minimizing the negative log likelihood function of WFPT distribution given trial RT. We developed separate decision SincNet models for each participant performing a two-alternative forced-choice task. Our results showed that single-trial estimates of drift and boundary performed better at predicting RTs than the median estimates in both training and test data sets, suggesting that our model can successfully use EEG features to estimate meaningful single-trial Diffusion model parameters. Furthermore, the shallow SincNet architecture identified time windows of information processing related to evidence accumulation and caution and the EEG frequency bands that reflect these processes within each participant. 

[Novel Probabilistic Perceptual Decision Making Task To investigate Trial-Level Computation of Evidence Accumulation in EEG and Behavior]

Sequential sampling frameworks hypothesize that patterns of choice response times are produced by integrating noisy sensory information until a threshold is reached. We introduce here a novel probabilistic Perceptual Decision Making (pPDM) task to examine how and when noisy evidence leads to a decision on a trial level. The task involves presenting stimuli as a succession of random samples with a known probability biased towards one of the two alternatives. Subjects were asked to indicate which category was more frequently observed. We present a chain of evidence to experimentally control the hypothesized random walk of decision variables in the brain.  ISI of the evidence chain is manipulated at three different levels: 0.05s, 0.1s, 0.25s. The pPDM task provides trial-level ground truth about the evolution of evidence prior to response to evaluate behavior and track evidence in the brain. 

The participants do not appear to accumulate evidence to a fixed threshold to respond (Fig A). Instead, the criterion for the Decision Variable to trigger a response depends on both the magnitude of peak level of evidence, and the timing of the peak. When evidence obviously favors one of the choices after a large number of samples, subjects respond seemingly at the peak of evidence.  When evidence peaks early, subjects wait for more samples to respond, even when they are uninformative, producing an offset between the peak of evidence accumulation and response (Fig B). We hypothesize that the brain tracks the peak of evidence to inform decisions, but not to trigger responses. We used  a novel interpretable neural network model to use EEG to predict the magnitude of evidence following the display at which the evidence peaks. The model can automatically identify the signals that are critical to predict level of peak evidence (Fig C, D), showing theta enhancement and alpha suppression at peak evidence and can predict peak evidence in new test data (Fig E).  These results show that while evidence accumulation is central to decision making, integration of noisy evidence to a bound does not by itself account for response times.     
","Konrad Kording,Malcolm A. MacIver",,
140,1124,Philipp,Thölke,philipp.thoelke@posteo.de,University of Montreal,+1-438-509-5650,Montréal,Québec,H2X 2G1,Canada,"[Analysis of Transformer attention in EEG signal classification]
While deep learning models show remarkable accuracy on electrophysiological time series such as EEG, the inner workings of these models are inherently hard to interpret. The scientific investigation of brain functions and dysfunction, however, strongly relies on the ability to characterize the properties and dynamics of neural changes across experimental states or groups. Here we present an approach to estimate feature importance in Transformers using attention weights and provide a proof-of-concept using the well studied setting of resting with eyes open versus eyes closed (n=109). In addition to feature importance we visualize the information flow throughout the network, providing means to distinguish the two conditions from the internal representation of the artificial neural network.

[Class imbalance should not throw you off balance: Choosing classifiers and performance metrics for brain decoding with imbalanced data]
Machine learning (ML) is becoming a standard tool in neuroscience and neuroimaging research. Yet, because it is such a powerful tool, the appropriate application of ML requires a sound understanding of its subtleties and limitations. In particular, applying ML to datasets with imbalanced classes, which are very common in neuroscience, can have severe consequences if not adequately addressed. With the neuroscience machine-learning user in mind, this technical note provides a didactic overview of the class imbalance problem and illustrates its impact through systematic manipulation of class imbalance ratios in both simulated data, and real electroencephalography (EEG) and magnetoencephalography (MEG) brain data. Our results illustrate how in highly imbalanced data, the commonly used Accuracy (Acc) metric yields misleadingly high performances by preferentially predicting the majority class, while other evaluation metrics (e.g. Balanced Accuracy (BAcc) defined as the mean of sensitivity and specificity, or the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC)) can still provide reliable performance evaluations. In terms of classifiers and cross-validation schemes, our data highlights the higher robustness of Random Forest (RF) and Stratified K-Fold cross-validation, compared to the other approaches tested. Critically, for neuroscience ML applications that seek to minimize overall classification error, we recommend the routine use of BAcc, rather than the more commonly used Acc metric. Importantly, we provide a best practices list of recommendations for dealing with imbalanced data, and open-source code to allow the neuroscience community to replicate our observations and further explore the best practices in handling imbalanced data.",,,2153862554
141,1230,Armin,Thomas,athms@stanford.edu,Stanford University,415-216-8742,San Francisco,California,94110,United States,"[Self-Supervised Learning Of Brain Dynamics From Broad Neuroimaging Data] Self-supervised learning techniques are celebrating immense success in natural language processing (NLP) by enabling models to learn from broad language data at unprecedented scales. Here, we aim to leverage the success of these techniques for mental state decoding, where researchers aim to identify specific mental states (such as an individual's experience of anger or happiness) from brain activity. To this end, we devise a set of novel self-supervised learning frameworks for neuroimaging data based on prominent learning frameworks in NLP. At their core, these frameworks learn the dynamics of brain activity by modeling sequences of activity akin to how NLP models sequences of text. We evaluate the performance of the proposed frameworks by pre-training models on a broad neuroimaging dataset spanning functional Magnetic Resonance Imaging (fMRI) data from 11,980 experimental runs of 1,726 individuals across 34 datasets and subsequently adapting the pre-trained models to two benchmark mental state decoding datasets. We show that the pre-trained models transfer well, outperforming baseline models when adapted to the data of only a few individuals, while models pre-trained in a learning framework based on causal language modeling clearly outperform the others.

[Challenges for cognitive decoding using deep learning methods] In cognitive decoding, researchers aim to characterize a brain region's representations by identifying the cognitive states (e.g., accepting/rejecting a gamble) that can be identified from the region's activity. Deep learning (DL) methods are highly promising for cognitive decoding, with their unmatched ability to learn versatile representations of complex data. Yet, their widespread application in cognitive decoding is hindered by their general lack of interpretability as well as difficulties in applying them to small datasets and in ensuring their reproducibility and robustness. We propose to approach these challenges by leveraging recent advances in explainable artificial intelligence and transfer learning, while also providing specific recommendations on how to improve the reproducibility and robustness of DL modeling results.",,,
142,1045,Mark,Thornton,mark.a.thornton@dartmouth.edu,Dartmouth College,559-999-7259,Hanover,NH,03755,United States,"[The Social Brain Automatically Predicts Others' Future Mental States]
Social life requires people to predict the future: people must anticipate others' thoughts, feelings, and actions to interact with them successfully. The theory of predictive coding suggests that the social brain may meet this need by automatically predicting others' social futures. If so, when representing others' current mental state, the brain should already start representing their future states. To test this hypothesis, we used fMRI to measure female and male human participants' neural representations of mental states. Representational similarity analysis revealed that neural patterns associated with mental states currently under consideration resembled patterns of likely future states more so than patterns of unlikely future states. This effect manifested in activity across the social brain network and in medial prefrontal cortex in particular. Repetition suppression analysis also supported the social predictive coding hypothesis: considering mental states presented in predictable sequences reduced activity in the precuneus relative to unpredictable sequences. In addition to demonstrating that the brain makes automatic predictions of others' social futures, the results also demonstrate that the brain leverages a 3D representational space to make these predictions. Proximity between mental states on the psychological dimensions of rationality, social impact, and valence explained much of the association between state-specific neural pattern similarity and state transition likelihood. Together, these findings suggest that the way the brain represents the social present gives people an automatic glimpse of the social future.

[Six dimensions describe action understanding: The ACT-FASTaxonomy]
Humans engage in a wide variety of different actions and activities. These range from simple motor actions like reaching for an object, to complex activities like governing a nation. Navigating everyday life requires people to make sense of this diversity of actions. We suggest that the mind simplifies this complex domain by attending primarily to the most essential features of actions. Using a parsimonious set of action dimensions, the mind can organize action knowledge in a low-dimensional representational space. In seven studies, we derive and validate such an action taxonomy. Study 1 uses large-scale text analyses to generate and test potential action dimensions. Study 2 validates interpretable labels for these dimensions. Studies 3–5 demonstrate that these dimensions can explain human judgments about actions. We perform model selection on data from these studies to arrive at the optimal set of six psychological dimensions, together forming the Abstraction, Creation, Tradition, Food, Animacy, Spiritualism Taxonomy (ACT-FAST). Study 6 demonstrates that ACT-FAST can predict socially relevant qualities of actions, including how, when, where, why, and by whom they are performed. Finally, Study 7 shows that ACT-FAST can explain action-related patterns of brain activity using naturalistic functional MRI (MRI). Together, these studies reveal the dimensional structure the mind applies to organize action concepts.

[People represent mental states in terms of rationality, social impact, and valence: Validating the 3d Mind Model]
Humans can experience a wide variety of different thoughts and feelings in the course of everyday life. To successfully navigate the social world, people need to perceive, understand, and predict others' mental states. Previous research suggests that people use three dimensions to represent mental states: rationality, social impact, and valence. This 3d Mind Model allows people to efficiently “see” the state of another person's mind by considering whether that state is rational or emotional, more or less socially impactful, and positive or negative. In the current investigation, we validate this model using neural, behavioral, and linguistic evidence. First, we examine the robustness of the 3d Mind Model by conducting a mega-analysis of four fMRI studies in which participants considered others' mental states. We find evidence that rationality, social impact, and valence each contribute to explaining the neural representation of mental states. Second, we test whether the 3d Mind Model offers the optimal combination of dimensions for describing neural representations of mental state. Results reveal that the 3d Mind Model achieve the best performance among a large set of candidate dimensions. Indeed, it offers a highly explanatory account of mental state representation, explaining over 80% of reliable neural variance. Finally, we demonstrate that all three dimensions of the model likewise capture convergent behavioral and linguistic measures of mental state representation. Together, these findings provide strong support for the 3d Mind Model, indicating that is it is a robust and generalizable account of how people think about mental states.
",,,
143,1393,Alexis,Thual,alexisthual@protonmail.com,"CEA, INRIA,
Université Paris-Saclay",+33-6-37-09-56-20,Gif-sur-Yvette,,91191,France,"[Aligning individual brains with fused unbalanced Gromov Wasserstein]
Individual brains vary in both anatomy and functional organization, even within a given species. Inter-individual variability is a major impediment when trying to draw generalizable conclusions from neuroimaging data collected on groups of subjects. Current co-registration procedures rely on limited data, and thus lead to very coarse inter-subject alignments. 
In this work, we present a novel method for inter-subject alignment based on Optimal Transport, denoted as Fused Unbalanced Gromov Wasserstein (FUGW). The method aligns two cortical surfaces based on the similarity of their functional signatures in response to a variety of stimuli, while penalizing large deformations of individual topographic organization.
We demonstrate that FUGW is suited for whole-brain landmark-free alignment. The unbalanced feature allows to deal with the fact that functional areas vary in size across subjects. Results show that FUGW alignment significantly increases between-subject correlation of activity during new independent fMRI tasks and runs, and leads to more precise maps of fMRI results at the group level.

[An empirical evaluation of functional alignment using inter-subject decoding]
Inter-individual variability in the functional organization of the brain presents a major obstacle to identifying generalizable neural coding principles. Functional alignment—a class of methods that matches subjects’ neural signals based on their functional similarity—is a promising strategy for addressing this variability. To date, however, a range of functional alignment methods have been proposed and their relative performance is still unclear. In this work, we benchmark five functional alignment methods for inter-subject decoding on four publicly available datasets. Specifically, we consider three existing methods: piecewise Procrustes, searchlight Procrustes, and piecewise Optimal Transport. We also introduce and benchmark two new extensions of functional alignment methods: piecewise Shared Response Modelling (SRM), and intra-subject alignment. We find that functional alignment generally improves inter-subject decoding accuracy though the best performing method depends on the research context. Specifically, SRM and Optimal Transport perform well at both the region-of-interest level of analysis as well as at the whole-brain scale when aggregated through a piecewise scheme. We also benchmark the computational efficiency of each of the surveyed methods, providing insight into their usability and scalability. Taking inter-subject decoding accuracy as a quantification of inter-subject similarity, our results support the use of functional alignment to improve inter-subject comparisons in the face of variable structure-function organization. We provide open implementations of all methods used.

[Cross-species cortical alignment identifies different types of anatomical reorganization in the primate temporal lobe]
Evolutionary adaptations of temporo-parietal cortex are considered to be a critical specialization of the human brain. Cortical adaptations, however, can affect different aspects of brain architecture, including local expansion of the cortical sheet or changes in connectivity between cortical areas. We distinguish different types of changes in brain architecture using a computational neuroanatomy approach. We investigate the extent to which between-species alignment, based on cortical myelin, can predict changes in connectivity patterns across macaque, chimpanzee, and human. We show that expansion and relocation of brain areas can predict terminations of several white matter tracts in temporo-parietal cortex, including the middle and superior longitudinal fasciculus, but not the arcuate fasciculus. This demonstrates that the arcuate fasciculus underwent additional evolutionary modifications affecting the temporal lobe connectivity pattern. This approach can flexibly be extended to include other features of cortical organization and other species, allowing direct tests of comparative hypotheses of brain organization.",,,
144,1310,Greta,Tuckute,gretatu@mit.edu,Massachusetts Institute of Technology,857-999-7895,Cambridge,MA,02139,United States,"1. [Intrinsically memorable words have unique associations with their meanings]
What makes a word memorable? Numerous factors, including lexical frequency, concreteness, imageability, and valence have been shown to affect recognition. One dimension that has not received sufficient attention is how uniquely a word picks out a particular meaning. If—as past work suggests—words are encoded primarily by their meanings, and not their forms, word-to-meaning relationship should be a central determinant of memorability. Following rational analysis, we hypothesized that memorable words convey a large amount of information about their meaning: they have few synonyms and few meanings. Across two recognition memory experiments (each with 2,222 target words and >600 participants, plus 3,780 participants for the norming experiments), we found that memory performance is overall high, on par with memory for images in a similar paradigm. Critically, as hypothesized, the most memorable words have a one-to-one relationship with their meanings, and this property explains >80% of explainable variance in memorability. 

2. [SentSpace: Large-Scale Benchmarking and Evaluation of Text using Cognitively Motivated Lexical, Syntactic, and Semantic Features]
SentSpace is a modular framework for streamlined evaluation of text. SentSpace characterizes textual input using diverse lexical, syntactic, and semantic features derived from corpora and psycholinguistic experiments. Core sentence features fall into three primary feature spaces: 1) Lexical, 2) Contextual, and 3) Embeddings. To aid in the analysis of computed features, SentSpace provides a web interface for interactive visualization and comparison with text from large corpora. The modular design of SentSpace allows researchers to easily integrate their own feature computation into the pipeline while benefiting from a common framework for evaluation and visualization. In this manuscript we will describe the design of SentSpace, its core feature spaces, and demonstrate an example use case by comparing human-written and machine-generated (GPT2-XL) sentences to each other. We find that while GPT2-XL-generated text appears fluent at the surface level, psycholinguistic norms and measures of syntactic processing reveal key differences between text produced by humans and machines. Thus, SentSpace provides a broad set of cognitively motivated linguistic features for evaluation of text within natural language processing, cognitive science, as well as the social sciences.

3. [The neural architecture of language: Integrative modeling converges on predictive processing]
The neuroscience of perception has recently been revolutionized with an integrative modeling approach in which computation, brain function, and behavior are linked across many datasets and many computational models. By revealing trends across models, this approach yields novel insights into cognitive and neural mechanisms in the target domain. We here present a systematic study taking this approach to higher-level cognition: human language processing, our species’ signature cognitive skill. We find that the most powerful “transformer” models predict nearly 100% of explainable variance in neural responses to sentences and generalize across different datasets and imaging modalities (functional MRI and electrocorticography). Models’ neural fits (“brain score”) and fits to behavioral responses are both strongly correlated with model accuracy on the next-word prediction task (but not other language tasks). Model architecture appears to substantially contribute
to neural fit. These results provide computationally explicit evidence that predictive processing fundamentally shapes the language comprehension mechanisms in the human brain.",,,1780803442
145,1164,Panteleimon,Vafeidis,pvafeidi@caltech.edu,Caltech,626-379-8995,Pasadena,CA,91106,United States,"Ring attractor models for angular path integration have received strong experimental support. To function as integrators, head direction circuits require precisely tuned connectivity, but it is currently unknown how such tuning could be achieved. Here, we propose a network model in which a local, biologically plausible learning rule adjusts synaptic efficacies during development, guided by supervisory allothetic cues. Applied to the Drosophila head direction system, the model learns to path-integrate accurately and develops a connectivity strikingly similar to the one reported in experiments. The mature network is a quasi-continuous attractor and reproduces key experiments in which optogenetic stimulation controls the internal representation of heading in flies, and where the network remaps to integrate with different gains in rodents. Our model predicts that path integration requires self-supervised learning during a developmental phase, and proposes a general framework to learn to path-integrate with gain-1 even in architectures that lack the physical topography of a ring.

Animals learn from experience the value of environmental stimuli to guide their behavior. At the core of conditioning lies the capacity to associate a neural activity pattern induced by an unconditioned stimulus (US) with the pattern arising in response to a conditioned stimulus (CS). Reward-modulated associative synaptic plasticity has been successful in explaining conditioning when the neural representations of behavioral stimuli are unmixed. However this assumption is inconsistent with the fact that neurons --- particularly in high-level, cognitive areas --- display mixed selectivity. Inspired by experimental findings on the associative power of single cortical pyramidal neurons, we propose a computational model that achieves generic pattern-to-pattern mappings at the population level. Our model incorporates a local learning rule operating in compartmentalized neurons, which mirrors the capacity of cortical pyramidal neurons to implement predictive learning through coincidence detection. The model accounts for a wide gamut of conditioning phenomena, offers a reductionist mechanism for causal inference, and produces experimentally testable predictions. In psychological terms, it corresponds to the stimulus substitution component of classical conditioning.","Kim Stachenfeld, Konrad Kording, Blake Richards, David Redish, Ida Momennejad, Mattia Rigotti",No fMRI/behavioral research,
146,1434,Jacobus,van den Bosch,japsai@gmail.com,University of Birmingham,+447384642049,Birmingham,,B1 1FJ,United Kingdom,"[We don’t hear what we see: Similarity judgements of dynamic visual and auditory stimuli]


Representational similarity analysis (RSA) has been widely used to compare the representation of objects presented as static images. However, to our knowledge it has not yet been used to compare objects as perceived through different sensory modalities. Given that in a natural setting we typically perceive the world in a dynamic fashion and with all of our senses, here we set out to compare similarity judgments across sensory modalities.
We acquired similarity judgements in 7 participants, for the same set of 50 stimuli in four different conditions: videos with sound, videos without sound, single representative static frames, and the video’s soundtracks only. These three second videos were sampled from a wide range of categories including people, animals, tools, vehicles and natural phenomena. 
An efficient way to measure subjective similarity for large object sets is the multiple arrangements task. Here participants arrange objects such that their distance in a circular arena represents their dissimilarity. After an initial trial with all objects, subsequent trials “zoom in” on clusters of objects, until a given evidence threshold is reached across all object pairs. A full representational dissimilarity matrix (RDM) is then assembled. For each participant, we calculated Spearman’s rank correlation between each condition’s RDM.
We observed strong and consistent (r-values reaching ~.5, p(bonf)<0.01) correlations between representations across all conditions (strongest in conditions which included visual information). Interestingly, the audio condition showed the weakest correlations to every other condition, and the audio-visual condition was better predicted by the visual conditions than the audio condition, suggesting a dominance of visual perception in similarity judgments.
The large correspondence across modalities suggests that similarity judgements are guided to some degree by modality-independent, high-level “semantic” components. Yet the markedly different similarity judgments observed for our audio condition suggest the involvement of different perceptual codes in audition. 
",,,
147,1129,Pieter,Verbeke,pjverbek.verbeke@ugent.be,Ghent University,+32-479-670-053,Ghent,,B9000,Belgium,"[Using top-down modulation to optimally balance shared versus separated task representations]

Human adaptive behavior requires continually learning and performing a wide variety of tasks, often with very little practice. To accomplish this, it is crucial to separate neural representations of different tasks in order to avoid interference. At the same time, sharing neural representations supports generalization and allows faster learning. Therefore, a crucial challenge is to find an optimal balance between shared versus separated representations. Typically, models of human cognition employ top-down modulatory signals to separate task representations, but there exist surprisingly little systematic computational investigations of how such modulation is best implemented. We identify and systematically evaluate two crucial features of modulatory signals. First, top-down input can be processed in an additive or multiplicative manner. Second, the modulatory signals can be adaptive (learned) or non-adaptive (random). We cross these two features, resulting in four modulation networks which are tested on a variety of input datasets and tasks with different degrees of stimulus-action mapping overlap. The multiplicative adaptive modulation network outperforms all other networks in terms of accuracy. Moreover, this network develops hidden units that optimally share representations between tasks. Specifically, different than the binary approach of currently popular latent state models, it exploits partial overlap between tasks.

[The dimensionality of neural representations for control]

Cognitive control allows us to think and behave flexibly based on our context and goals. At the heart of theories of cognitive control is a control representation that enables the same input to produce different outputs contingent on contextual factors. In this review, we focus on an important property of the control representation’s neural code: its representational dimensionality. Dimensionality of a neural representation balances a basic separability/generalizability trade-off in neural computation. We will discuss the implications of this trade-off for cognitive control. We will then briefly review current neuroscience findings regarding the dimensionality of control representations in the brain, particularly the prefrontal cortex. We conclude by highlighting open questions and crucial directions for future research.

[Theta oscillations shift towards optimal frequency for cognitive control]

Cognitive control allows to flexibly guide behaviour in a complex and ever-changing environment. It is supported by theta band (4–7?Hz) neural oscillations that coordinate distant neural populations. However, little is known about the precise neural mechanisms permitting such flexible control. Most research has focused on theta amplitude, showing that it increases when control is needed, but a second essential aspect of theta oscillations, their peak frequency, has mostly been overlooked. Here, using computational modelling and behavioural and electrophysiological recordings, in three independent datasets, we show that theta oscillations adaptively shift towards optimal frequency depending on task demands. We provide evidence that theta frequency balances reliable set-up of task representation and gating of task-relevant sensory and motor information and that this frequency shift predicts behavioural performance. Our study presents a mechanism supporting flexible control and calls for a reevaluation of the mechanistic role of theta oscillations in adaptive behaviour.",,,
148,1218,Peter,Vincent,peter.vincent.14@ucl.ac.uk,UCL,+44-7584-076-940,London,,W1T 4JN,United Kingdom,"The perception of sensory stimuli is frequently variable.  Previous studies have shown that observers account for uncertainty arising from internal variability when they combine sensory cues, integrate sensory input with prior expectation, or select actions under externally imposed cost functions.   But how does the distribution of internal uncertainty shape free perceptual report?   Behavioural models have assumed that unitary percepts may reflect means, modes or samples of internal belief distributions.    Here, we show that observers' reconstructions of the remembered orientation of a visual grating correspond to  means of the variability-induced likelihood, not the mode or a random sample.   This behaviour remains robust as either the distribution of stimuli, or the degree of internal variability, change.  These observations suggest that variability arises in encoding or recall, and is accurately taken into account at the point of perception or action.",Kim Stachenfeld,Maneesh Sahani,
149,1299,Gal,Vishne,gal.vishne@mail.huji.ac.il,Hebrew University of Jerusalem,+972-52-613-4023,Jerusalem,,91904,Israel,"[Multivariate Representation of Sustained Visual Content in a No-Report Paradigm]
Vision research has thoroughly characterized the information content of transient neural responses to stimuli onset. Yet, visual experience contains many stationary moments when we still maintain information about our environment, highlighting the need to understand this aspect of perception. To investigate visual representation over time, we utilize intracranial recordings from patients viewing images of variable durations and categories (focusing on no-response trials), an approach promoted recently to arbitrate between two prominent scientific theories of consciousness (Melloni et al., 2021). We previously showed single-electrode responses in category selective regions are dramatically attenuated after the onset (Gerber et al., 2017). Here, we examine category and single-item representation using multivariate state-space analyses, decoding, and representational similarity analysis. First, we show sustained and stable visual representation in ventral-temporal and occipital cortex, despite the reduction in activity. Second, we show transient representation of visual content in prefrontal cortex, even though no report was required. This suggests that ongoing content is maintained in sensory regions, and prefrontal cortex responds to changes regardless of the task. Importantly, as predictions of both consciousness theories bear out, it questions whether this approach can arbitrate between them.
[Representation of content in sustained viewing conditions: a case study for consciousness theories]
Objectives: Vision research has thoroughly characterized the information content of transient neural responses to stimuli onset. Yet, visual experience is composed of many stationary moments without change, when we still maintain information about our environment. We set out to characterize neural representation of visual content over time, using a passive viewing task with images of variable durations (300-1500ms) and categories. This approach was recently promoted as a method for arbitrating between two prominent scientific theories of consciousness (Integrated Information Theory (IIT) and Global Neuronal Workspace (GNW); COGITATE consortium).
Research question: What are the temporal dynamics and spatial profile of visual representation in sustained viewing conditions? Do neural signals represent information persistently even in a passive, changeless setting, and are different neural regions engaged in different temporal dynamics?
Materials and methods: We recorded intracranially from 10 patients undergoing epilepsy surgery (1067 electrodes), enabling us both high spatial and high temporal resolution. We measured information content both at the category level (face, object, animal), using multivariate decoding, and at the level of single exemplars, using representational similarity analysis. We used temporal generalization to measure stability over time.
Results: First, we show that category information persists throughout the viewing, as a stable pattern of activity across sites, despite a major reduction in response amplitude with time. Regionally, we find sustained category information in occipital and ventral-temporal areas, and onset (200-400ms) category information in prefrontal sites. Second, we find sustained information about the identity of single images, stable throughout the viewing period. Here too, sustained information was present in posterior areas, but we also find transient information in prefrontal areas. These results hold when controlling for the category structure and when focusing on single categories.
Conclusion: We find sustained visual information in posterior areas, predicted by IIT, alongside a short-lived “ignition” in anterior areas, including PFC (despite the no-report task), predicted by GNW. These results may suggest that ongoing content information is maintained in posterior areas, while prefrontal regions are more involved in changes in perception. However, as predictions of both theories bear out, it questions whether this approach can arbitrate between the theories.
[Pre-stimulus fronto-parietal modulation of task performance and visual representation]
Recent evidence has shown rhythmic fluctuations in performance on low-level sensory tasks, which is modulated by the phase of low-frequency fluctuations around stimulus onset (Helfrich et al., 2018). The mechanism behind this modulation is still unclear – is the phase modulation exerted directly on stimulus representation, or does it influence the outcome at later processing stages by modulating the decision threshold or confidence levels? We addressed this question using ECoG recordings from subjects undergoing surgery for epilepsy (Gerber et al., 2017). Instead of using pre-defined bands which conceal considerable inter-subject variability (Haegens et al., 2014), we identified putative oscillations in the range of 4-13Hz using a recently described parametrization approach (“fooof”, Donoghue et al., 2020). First, we extend the behavioral findings by showing that reaction-time on a high-level category detection task is modulated by the phase of low-frequency oscillations in the ventro-lateral frontal-cortex extending to the inferior postcentral gyrus. To quantify category representation, we used a multivariate decoding approach. We trained a classifier (LDA) to distinguish between viewing of different categories on a moment-by-moment basis. Classifier weights indicated that category information is represented mainly in the ventral-temporal cortex. We show that the phase and amplitude of low-frequency fluctuations in inferior parietal lobule (IPL) is correlated with the cumulative classifier evidence from 100ms to 300ms after stimulus onset, suggesting that the influence of prefrontal phase on behavior may be associated with changes in category representation in the ventral stream, but this is yet to be shown directly.
",,,
150,1325,Aria,Wang,yuanw3@andrew.cmu.edu,Carnegie Mellon University,206-790-0198,Pittsburgh,PA,15221,United States,"[Image embeddings informed by natural language improve predictions and understanding of human higher-level visual cortex]
We propose that high-level visual representations reflect multimodal constraints. Consequently, multimodal models such as Contrastive Language-Image Pre-training (CLIP) should better predict neural responses in visual cortex. We extracted image features using CLIP, which encodes visual concepts with supervision from natural language captions. We then used voxelwise encoding models based on CLIP features to predict brain responses to real-world images from the Natural Scenes Dataset. Remarkably, CLIP explains up to 78\% of variance in held out test data. CLIP also explains greater unique variance in higher-level visual areas compared to models trained only with image/label pairs (ImageNet trained ResNet) or text (BERT). Visualizations of model embeddings and Principal Component Analysis (PCA) reveal that, with the use of captions, CLIP captures both fine-grained and global semantic dimensions within visual cortex. We suggest that models trained jointly on images and text (or other modalities) lead to better models of representation.
[Revealing computational mechanisms of retinal prediction via model reduction]
Recently, deep feedforward neural networks have achieved considerable success in modeling biological sensory processing, in terms of reproducing the input-output map of sensory neurons. However, such models raise profound questions about the very nature of explanation in neuroscience. Are we simply replacing one complex system (a biological circuit) with another (a deep network), without understanding either? Moreover, beyond neural representations, are the deep network's {\it computational mechanisms} for generating neural responses the same as those in the brain? Without a systematic approach to extracting and understanding computational mechanisms from deep neural network models, it can be difficult both to assess the degree of utility of deep learning approaches in neuroscience, and to extract experimentally testable hypotheses from deep networks. We develop such a systematic approach by combining dimensionality reduction and modern attribution methods for determining the relative importance of interneurons for specific visual computations. We apply this approach to deep network models of the retina, revealing a conceptual understanding of how the retina acts as a predictive feature extractor that signals deviations from expectations for diverse spatiotemporal stimuli. For each stimulus, our extracted computational mechanisms are consistent with prior scientific literature, and in one case yields a new mechanistic hypothesis. Thus overall, this work not only yields insights into the computational mechanisms underlying the striking predictive capabilities of the retina, but also places the framework of deep networks as neuroscientific models on firmer theoretical foundations, by providing a new roadmap to go beyond comparing neural representations to extracting and understand computational mechanisms.","Surya Ganguli, ","Maggie Henderson, Andrew Luo",145164837
151,1317,Binxu,Wang,binxu_wang@hms.harvard.edu,Harvard Medical School,314-224-0648,Boston,MA,02215,United States,"[A Geometric Analysis of Deep Generative Image Models and Its Applications]

Generative adversarial networks (GANs) have emerged as a powerful unsupervised method to model the statistical patterns of real-world data sets, such as natural images. These networks are trained to map random inputs in their latent space to new samples representative of the learned data. However, the structure of the latent space is hard to intuit due to its high dimensionality and the non-linearity of the generator, which limits the usefulness of the models. Understanding the latent space requires a way to identify input codes for existing real-world images (inversion), and a way to identify directions with known image transformations (interpretability). Here, we use a geometric framework to address both issues simultaneously. We develop an architecture-agnostic method to compute the Riemannian metric of the image manifold created by GANs. The eigen-decomposition of the metric isolates axes that account for different levels of image variability. An empirical analysis of several pretrained GANs shows that image variation around each position is concentrated along surprisingly few major axes (the space is highly anisotropic) and the directions that create this large variation are similar at different positions in the space (the space is homogeneous). We show that many of the top eigenvectors correspond to interpretable transforms in the image space, with a substantial part of eigenspace corresponding to minor transforms which could be compressed out. This geometric understanding unifies key previous results related to GAN interpretability. We show that the use of this metric allows for more efficient optimization in the latent space (e.g. GAN inversion) and facilitates unsupervised discovery of interpretable axes. Our results illustrate that defining the geometry of the GAN image manifold can serve as a general framework for understanding GANs.

[Tuning landscapes of the ventral stream]

A fundamental goal in neuroscience is to define how cortical neurons respond to arbitrary natural images, such as those we encounter in daily life. Most studies of visual neurons rely on tuning curves, plots showing the dependence of evoked activity on image changes along one or two continuously changing parameters of simpler artificial stimuli. While it would be ideal to relate classic tuning curves to naturalistic images, but this has been difficult because naturalistic images are complex, containing many textures and objects, there is no agreement on how to describe them systematically using fixed a priori variables. In this study, we test the idea that all classic tuning curves can be viewed as slices of a higher-dimensional tuning landscape, i.e., the neuronal response as a function over the entire natural image space. We approximated the natural image space with a 4096-dimensional manifold, parametrized by a deep-learning image generator, and we investigated the tuning landscapes of ventral stream neurons on it. For each neuron we targeted, we used a closed-loop evolutionary approach to search for activation maximizing stimulus; using this stimulus (“prototype”) as a landmark, we sampled images around it and mapped the neuronal tuning. To probe the global distribution of the landmarks, we also searched for activation maximizing stimuli in a randomly chosen 50D subspace. By moving away from the prototype in diverse ways, we found that neurons showed smooth bell-shaped tuning consistent with radial basis functions. These tuning functions spanned a larger range of response and a wider extent in generator space, compared to the tuning measured in univariate image sets (Gabor, curvature). The width of tuning in generator space became narrower from V1 to IT, accompanied by a longer search convergence time and a larger decrease in response when we constrained the search to 50d random subspace. These trends pointed to a systematic difference in the landscape geometry throughout the ventral stream. One model that could explain all these trends is one where neurons in higher visual cortices have higher intrinsic dimensionality, i.e., more feature axes than neurons in early visual neurons. Overall, our results indicate that visual neurons should not be viewed as signaling parameter values in special tuning axes, but rather, they should be viewed as similarity (kernel) machines or radial basis functions, signaling distance to prototypes on the image manifold.

[High-performance Evolutionary Algorithms for Online Neuron Control]

Recently, optimization has become an emerging tool for neuroscientists to study neural code. In the visual system, neurons respond to images with graded and noisy responses. Image patterns eliciting highest responses are diagnostic of the coding content of the neuron. To find these patterns, we have used black-box optimizers to search a 4096d image space, leading to the evolution of images that maximize neuronal responses. Although genetic algorithm (GA) has been commonly used, there haven't been any systematic investigations to reveal the best performing optimizer or the underlying principles necessary to improve them.

Here, we conducted a large scale in silico benchmark of optimizers for activation maximization and found that Covariance Matrix Adaptation (CMA) excelled in its achieved activation. We compared CMA against GA and found that CMA surpassed the maximal activation of GA by 66% in silico and 44% in vivo. We analyzed the structure of Evolution trajectories and found that the key to success was not covariance matrix adaptation, but local search towards informative dimensions and an effective step size decay. Guided by these principles and the geometry of the image manifold, we developed SphereCMA optimizer which competed well against CMA, proving the validity of the identified principles. Code available at  (https://github.com/Animadversio/ActMax-Optimizer-Dev)","SueYeon Chung, Nikolaus Kriegeskorte, Carsen Stringer, Grace W. Lindsay, Jack Gallant, Eero Simoncelli","Kasper Vinken, Carlos Ramon Ponce, Saloni Sharma",40856972
152,1081,Nicholas,Watters,ndwatters@gmail.com,Massachusetts Institute of Technology,914-523-1918,Somerville,MA,02143,United States,"The role of mental simulation in primate physical inference abilities

Primates can richly parse sensory inputs to infer latent information, and adjust their behavior accordingly. It has been hypothesized that such flexible inferences are aided by simulations of internal models of the external world. However, evidence supporting this hypothesis has been based on behavioral models that do not emulate neural computations. Here, we test this hypothesis by directly comparing the behavior of humans and monkeys in a ball interception task to that of recurrent neural network (RNN) models with or without the capacity to “simulate” the underlying latent variables. Humans and monkeys had strikingly similar behavioral patterns suggesting common underlying neural computations. Comparison between primates and a large class of RNNs revealed that only RNNs that were optimized to simulate the position of the ball were able to accurately capture key features of the behavior such as systematic biases in the inference process. These results are consistent with the hypothesis that primates use mental simulation to make flexible inferences. Moreover, our work highlights a general strategy for using model neural systems to test computational hypotheses of higher brain function.","Matt Botvinick, Ilker Yildirim, Kim Stachenfeld, Doris Tsao, Tatiana Engel, Dan Yamins",Joshua Tenenbaum,
153,1273,Choong-Wan,Woo,choongwan.woo@gmail.com,Sungkyunkwan University,+82-31-299-4363,Suwon-si,Gyeonggi-do,16419,Korea (South),"[Life-Inspired Interoceptive Artificial Intelligence for Autonomous and Adaptive Agents]
Building autonomous and adaptive agents has been a long-term goal of artificial intelligence (AI) research. Here, we focus on interoception in artificial agents, which refers to the process of monitoring one’s internal environment with the main aim of keeping the internal environment within bounds which warrants a survival of biological agents (i.e., homeostasis and allostasis). To develop an artificial agent with interoception, we need to separate the variables representing the internal environment from those of the external and design parsimonious and low-dimensional essential variables that define the survival of—or, more simply, characterize—the agent. This paper offers a new perspective on how interoception in AI (i.e., interoceptive AI) can help build autonomous and adaptive agents by integrating the legacy of cybernetics with recent advances in theories of life, reinforcement learning, embodiment, and neuroscience. 
[Individual variability in brain representations of pain]
Characterizing cerebral contributions to individual variability in pain processing is crucial for personalized pain medicine, but has yet to be done. In the present study, we address this problem by identifying brain regions with high versus low interindividual variability in their relationship with pain. We trained idiographic pain-predictive models with 13 single-trial functional MRI datasets (n=?404, discovery set) and quantified voxel-level importance for individualized pain prediction. With 21 regions
identified as important pain predictors, we examined the interindividual variability of local pain-predictive weights in these regions. Higher-order transmodal regions, such as ventromedial and ventrolateral prefrontal cortices, showed larger individual variability, whereas unimodal regions, such as somatomotor cortices, showed more stable pain representations across individuals. We replicated this result in an independent dataset (n=?124). Overall, our study identifies cerebral sources of individual differences in pain processing, providing potential targets for personalized assessment and treatment of pain.
[Where Do Rewards Come From?]
Reinforcement learning has achieved broad and successful application in cognitive science in part because of its general formulation of the adaptive control problem as the maximization of a scalar reward function. The computational reinforcement learning framework is motivated by correspondences to animal reward processes, but it leaves the source and nature of the rewards unspecified. This paper advances a general computational framework for reward that places it in an evolutionary context, formulating a notion of an optimal reward function given a fitness function and some distribution of environments. Novel results from computational experiments show how traditional notions of extrinsically and intrinsically motivated behaviors may emerge from such optimal reward functions. In the experiments these rewards are discovered through automated search rather than crafted by hand. The precise form of the optimal reward functions need not bear a direct relationship to the fitness function, but may nonetheless confer significant advantages over rewards based only on fitness.","Chelsea Finn, Ida Mommenejad, Nikolaus Kriegeskorte, Nathaniel D. Daw, Jane X. Wang, Kimberly Stachenfeld","Sungwoo Lee, Jihoon Han, Sungmin Park",38550277
154,1048,Yudi,Xie,yu_xie@mit.edu,Massachusetts Institute of Technology,857-505-4706,Cambridge,MA,02139,United States,"[Human-like capacity limitation in multi-system models of working memory]
Working memory (WM) enables humans and other animals to hold information temporarily for various kinds of mental processing. WM has limited capacity (Cowan, 2001) and the maintenance of information in WM involves interactions between multiple brain regions (Christophel, Klink, Spitzer, Roelfsema, & Haynes, 2017). To account for such properties, we built multi-system models of WM, i.e., models that involve both sensory and cognitive systems, and their interactions. Our contributions are twofold, involving engineering and science. Engineering-wise, we built a framework to systematically construct such models to generate and test hypotheses in neuroscience research. Our models take sensory stimuli in their raw form and reproduce diverse behavioral and neural findings across classical and recent WM experiments. Science-wise, our framework allows us to dissect the sensory and cognitive system’s contribution to WM capacity limitation. Our models reproduced behavioral findings in several WM tasks commonly used to assess capacity limitation. We found human-like capacity limitations arise in models with sensory systems pre-trained to recognize natural images, but not in models trained end-to-end on WM tasks. Our results suggest that WM capacity limitation is partly attributed to the sensory system when it is optimized for naturalistic objectives other than tasks artificially designed to probe WM.

[Computational advantages of multiple reinforcements signals in the brain]
Reinforcement learning theory frame the objective of learning as optimizing a master reward signal. However, the nature of this reward signal is not clearly defined, and sophisticated reward engineering is often needed to have the agent learn the desired behavior. In the natural environment, diverse objectives drive animal behavior, and it has been observed that there are multiple reinforcement signals in the brain. However, how do animals learn from multiple reinforcement signals and what computational advantages it implies are not well-understood. In this work, we proposed a collection of ideas that demonstrate having multiple separate learning signals operating concurrently is more advantageous than a single reward-based learning system. We showed that having separate learning signals is beneficial in learning different features of the environment that have different statistics and drastically different impacts on the agent, like reward and threat. Furthermore, separate learning for multiple learning signals with different scales is beneficial considering realistic coding assumptions of the brain, like range adaptation. In addition, separate learning is more efficient than learning through a master reward signal when the actions are learned in a context-specific or modality-specific manner. Overall, inspired by neuroscientific findings, our work presents a collection of ideas that shows four computational advantages brought by multiple learning signals.
",,,
155,1350,Huadong,Xiong,hdx@arizona.edu,University of Arizona,+86-188-1792-2123,Tucson,Arizona,85719,United States,"[Neural mechanism of utilizing prior information in working memory]
Neural systems can adapt to the statistical regularities in the environments and leverage this prior information to optimize behaviors. However, how neural systems utilize stimuli statistics through recurrent interactions between neurons remains unclear. Here we trained recurrent neural networks (RNNs) to perform a working memory task with informative stimuli distribution. We found this goal-directed optimization made RNNs bias their behaviors towards the prior distribution of stimuli statistics. Analysis of RNN revealed that this behavioral bias was enabled by warped representations geometry and non-uniform discrete attractors dynamics. Our trained models are also capable of reproducing various experimental results both at behavioral and neural levels. 

[Use linear reinforcement learning model in computational psychiatry]
Previous research (Gagne et al., 2021) use models with many parameters, leading high cognitive load to understand results. The default policy and control cost in linear RL framework (Piray & Daw, 2021) have intuitive connections with some syndromes in psychiatry. I apply the linear RL model to their dataset to get a simpler explanation.

[Transfer of task rules in artificial neural network]
We train MLP to perform the task in Collins & Frank, (2016) with reinforcement learning. We study whether the neural nets can learn the task rule, to what extent it can be generalize. We also analyze the representations of the same task rule in different contexts. Lastly we use model-based analysis to find the RPE in the neural activities.","Michael Frank, Yael Niv, Matthew Botvinick, Audrey Sederberg, Bradley Love, Guangyu Yang, Bruno Averbeck, Yudi Xie",,
156,1112,Zhexin,Xu,z.xu@rochester.edu,University of Rochester,585-532-8180,Rochester,NY,14623,United States,"One of the fundamental tasks of the visual system is to infer the motion and depth of objects based on their 2D retinal images. This is often complicated by the observer’s body and eye movements. Depending on the viewing geometry, the combination of retinal image motion and eye movements can be used to perform different computations: summation to compute object motion in the world (“coordinate transformation”, CT); or division for inferring depth of the object (“depth from motion parallax”, MP). We investigated how the same signals, retinal motion and eye movements, mediate the perception of motion and depth under different viewing contexts in both humans and recurrent neural networks. We asked human subjects to estimate the motion and depth of an object while simulating different viewing contexts with optic flow, and we found distinct patterns of biases between the CT and MP contexts. Furthermore, an RNN trained on the same tasks represents task-relevant variables similarly to our previous findings on neural responses in area MT. Our study demonstrates that the interaction between retinal and eye velocities can lead to very different percepts, depending on the interpretation of the viewing context, and our RNN model provides novel predictions for neural representations. ",,,
157,1328,Huzheng,Yang,huze.yann@gmail.com,University of Electronic Science and Technology of China,747-333-8613,Philadelphia,Pennsylvania,19104,China,"[Voxel-wise Encoding Models with Hierarchical Task-optimized Brain Atlas]
A central goal of visual neuroscience is to build computational models that predict and explain neural responses to visual inputs in the cortex. Recent studies attempt to borrow the representation power of deep neural network to predict the brain response and suggest correspondence between artificial and biological neural networks on a certain level. However, each ANN instance is often specified for certain computer vision tasks while each brain area may vary its role across different tasks. This inconsistency harms the overall model capacity of naturalistic representation therefore weakens the reliability of the established principle and restricting the model’s utility. Thus if we want to further explore the principles of vision through ANN approaches, it is of first importance to build a unified high-accuracy prediction-based explanatory model to generally facilitate the investigation of the brain’s neural representation mechanism.
For end-to-end trained voxel-wise encoding models, sharing parameters with a large number of voxels is a common practice to reduce overfitting caused by single-voxel noise, it also explores the underlying network interaction between voxels. However, voxels that don't share the same learning dynamics and convergence speed will suffer from global early stopping criteria. To address this challenge, we propose a novel brain atlas named Hierarchical Task-optimized ROI: first extract task-optimized voxel embeddings from the encoding model, then cluster voxels by embeddings, similar embeddings imply similar learning dynamics and convergence speed. Applying it to a pre-trained SwinTransformer model, we achieved new state-of-the-art results on the Algonauts 2021 Challenge full-track whole-brain prediction task, boosting from 0.3715 to 0.3857 explainable variance explained without accessing pre-defined anatomical ROI information, 0.3917 when ensembling with anatomical ROI.

[Effective Ensemble of Deep Neural Networks Predicts Neural Responses to Naturalistic Videos]
This report provides a review of our submissions to the Algonauts Challenge 2021. In this challenge, neural responses in the visual cortex were recorded using functional neuroimaging when participants were watching naturalistic videos. The goal of the challenge is to develop voxel-wise encoding models which predict such neural signals based on the input videos. Here we built an ensemble of models that extract representations based on the input videos from 4 perspectives: image streams, motion, edges, and audio. We showed that adding new modules into the ensemble consistently improved our prediction performance. Our methods achieved state-of-the-art performance on both the mini track and the full track tasks. ","Ghislain St-Yves, Emily J. Allen, Yihan Wu, Kendrick Kay, Thomas Naselaris, Radoslaw Cichy",,
158,1078,Qihang,Yao,qyao33@gatech.edu,Georgia Institute of Technology,404-947-1605,Atlanta,Georgia,30339,United States,"[A Weighted Network Analysis Framework for the Hourglass Effect — and its Application in the C. Elegans Connectome]
Understanding hierarchy and modularity in natural as well as technological networks is of utmost importance. A major aspect of such analysis involves identifying the nodes that are crucial to the overall processing structure of the network. More recently, the approach of hourglass analysis has been developed for the purpose of quantitatively analyzing whether only a few intermediate nodes mediate the information processing between a large number of inputs and outputs of a network. We develop a new framework for hourglass analysis that takes network weights into account while identifying the core nodes and the extent of hourglass effect in a given weighted network. We use this framework to study the structural connectome of the C. elegans and identify intermediate neurons that form the core of sensori-motor pathways in the organism. Our results show that the neurons forming the core of the connectome show significant differences across the male and hermaphrodite sexes, with most core nodes in the male concentrated in sex-organs while they are located in the head for the hermaphrodite. Our work demonstrates that taking weights into account for network analysis framework leads to emergence of different network patterns in terms of identification of core nodes and hourglass structure in the network, which otherwise would be missed by unweighted approaches. 

[Root-Cause Analysis of Activation Cascade Differences in Brain Networks]
Diffusion MRI imaging and tractography algorithms have enabled the mapping of the macro-scale connectome of the entire brain. At the functional level, probably the simplest way to study the dynamics of macro-scale brain activity is to compute the ""activation cascade"" that follows the artificial stimulation of a source region. Such cascades can be computed using the Linear Threshold model on a weighted graph representation of the connectome. The question we focus on is: if we are given such activation cascades for two groups, say A and B (e.g. Controls versus a mental disorder), what is the smallest set of brain connectivity (graph edge weight) changes that are sufficient to explain the observed differences in the activation cascades between the two groups? We have developed and computationally validated an efficient algorithm, TRACED, to solve the previous problem. We argue that this approach to compare the connectomes of two groups, based on activation cascades, is more insightful than simply identifying ""static"" network differences (such as edges with large weight or centrality differences). We have also applied the proposed method in the comparison between a Major Depressive Disorder (MDD) group versus healthy controls and briefly report the resulting set of connections that cause most of the observed cascade differences. 

[Using complex network analysis to learn structure-function relationships in deep neural networks]
To understand the efficacy of an architecture to learn a task, typically the network needs to be trained and then tested over many configurations, which proves to be highly expensive given the size of modern-day models.  Further, the iterative process does not provide any insight into why a model is either good or bad. Thus, richer measures that provide a lens into how well a network is learning, how the architecture of the network and task need to be matched, and knowing where certain types of information are being represented in the network, are needed moving forward. The overall objective of this research is to understand the interaction between structure (a.k.a., architecture or topology) and function in neural networks. To achieve these goals, we will leverage tools from the study of adaptive networks in complex systems. Network analysis provides graph theoretic concepts, metrics, and algorithms that can be used for the analysis and design of neural network architectures. ",,,
159,1085,Taehyun,Yoo,yth2654@dgist.ac.kr,Daegu Gyeongbuk Institute of Science and Technology (DGIST),+82-10-4579-2654,Daegu,,42988,Korea (South),"[Modality specificity and generality in the hierarchical levels of cognitive control]
The prefrontal cortex (PFC) is organized hierarchically along a posterior-to-anterior axis. This theoretical framework has been studied mostly using visual stimuli. Here, we investigated this functional organization of the PFC, particularly using multiple sensory information with multivoxel pattern analysis (MVPA) and functional connectivity (FC). We used two different sensory modalities—auditory cues and visual targets—to establish different levels of hierarchical processing. We found that the posterior-to-anterior pattern of activations along the precentral gyrus (PreCG), inferior frontal gyrus (IFG), and middle frontal gyrus (MFG) was observed as the level of hierarchy increased. Furthermore, MVPA results showed that the more anterior regions specifically encoded information for higher-level processing. More interestingly, sensory areas had stronger FC with the posterior region of the PFC than the anterior region. We suggest that the PFC has different functional associations with sensory modality depending on the levels of cognitive control.
",Uri Hasson,,
160,1153,Leyao,Yu,yul09@nyu.edu,New York University,615-602-6648,Berkeley,CA,94704,United States,"[Distinct prefrontal networks for semantic integration and articulatory planning]
The spatiotemporal neural dynamics in frontal cortex underlying speech production and word retrieval remain poorly understood. Traditionally, the inferior frontal gyrus (IFG) has been implicated in various aspects of language processing including articulatory, syntactic and semantic processes (Frederici, 2012; Hickok & Poeppel, 2007). Growing evidence has implicated the posterior MFG (area 55b) and dorsal precentral gyrus as critical for language (Chang, 2020; Ozker, 2022). In order to investigate the spatiotemporal dynamics of speech articulation and semantic integration, we leveraged a battery of language production tasks within a cohort of 24 neurosurgical patients undergoing Electrocorticographic monitoring. We hypothesized that articulatory preparation, lexical retrieval, and semantic integration may not be localized to one region but rather may be distributed in nature. All participants underwent the same tasks including picture naming, word reading, auditory naming, auditory word repetition, and auditory sentence completion, designed to produce the same unique 50 words in each task but with a distinct route of retrieval (randomly interspersed within the block). Focusing on high gamma broadband (70-150 Hz), we report the spatiotemporal dynamics across peri-sylvian cortices locked to language perception and articulation.  
A region of interest analysis provided unique profiles of task-related neural recruitment per region during perception and production across all significant electrodes (sustained activity for over 100ms compared to baseline, t-test p<0.05). The neural profiles replicated previous temporal dynamics for sensory perception (STG and occipital cortices), speech planning (IFG), and motor execution (pre- and post- central gyri). However, we found specific enhancement in IFG and MFG related to semantic load. In order to test how distributed these networks were, we employed an unsupervised clustering approach to uncover dynamics in a data driven manner without an anatomical bias. This approach revealed two new networks in frontal cortex with differentiated articulatory and semantic specificity. One cluster of electrodes, centered in IFG and precentral cortices, showed comparable activity prior to speech onset across all tasks. The second cluster, centered on the border of IFG and MFG, showed strong task-selectivity prior to articulation with significantly greater (t-test, p<0.001) activity for tasks requiring semantic access. In order to investigate the nature of these two clusters, we performed representational similarity analysis across the auditory naming and sentence completion tasks. Neural covariance across time locked to perception in the semantic cluster highly correlated with the semantic embeddings of the changing auditory stimuli (based on the last 3 layers of a deep neural network GPT-2 model, spearman correlation p<0.001 rho= 0.53, 0.27, respectively for each task) which quickly diminished as speech production onset approached. The articulatory cluster, however, was strongly correlated with phonetic information rather than semantic embeddings prior to speech onset across tasks. Our results suggest two distinct language production components distributed across frontal cortices, a preparatory motor-related component agnostic to task, and a component recruited specifically as semantic integration is required.
","Zaid Zada, Ariel Goldstein, Christijan Armeni",,46274982
161,1255,Zaid,Zada,zaid@princeton.edu,Princeton University,571-426-9416,Princeton,NJ,08540,United States,"[Shared computational principles for language processing in humans and deep language models]
Departing from traditional linguistic models, advances in deep learning have resulted in a new type of predictive (autoregressive) deep language models (DLMs). Using a self-supervised next-word prediction task, these models generate appropriate linguistic responses in a given context. In the current study, nine participants listened to a 30-min podcast while their brain responses were recorded using electrocorticography (ECoG). We provide empirical evidence that the human brain and autoregressive DLMs share three fundamental computational principles as they process the same natural narrative: (1) both are engaged in continuous next-word prediction before word onset; (2) both match their pre-onset predictions to the incoming word to calculate post-onset surprise; (3) both rely on contextual embeddings to represent words in natural contexts. Together, our findings suggest that autoregressive DLMs provide a new and biologically feasible computational framework for studying the neural basis of language.

[Brain embeddings with shared geometry to artificial contextual embeddings, as a code for representing language in the human brain]
Contextual embeddings, derived from deep language models (DLMs), provide a continuous vectorial representation of language. This embedding space differs fundamentally from the symbolic representations posited by traditional psycholinguistics. Do language areas in the human brain, similar to DLMs, rely on a continuous embedding space to represent language? To test this hypothesis, we densely recorded the neural activity in the Inferior Frontal Gyrus (IFG, also known as Broca’s area) of three participants using dense intracranial arrays while they listened to a 30-minute podcast. From these fine-grained spatiotemporal neural recordings, we derived for each patient a continuous vectorial representation for each word (i.e., a brain embedding). Using stringent, zero-shot mapping, we demonstrated that brain embeddings in the IFG and the DLM contextual embedding space have strikingly similar geometry. This shared geometry allows us to precisely triangulate the position of unseen words in both the brain embedding space (zero-shot encoding) and the DLM contextual embedding space (zero-shot decoding). The continuous brain embedding space provides an alternative computational framework for how natural language is represented in cortical language areas.

[Correspondence between the layered structure of deep language models and temporal structure of natural language processing in the human brain]
Deep language models (DLMs) provide a novel computational paradigm for how the brain processes natural language. Unlike symbolic, rule-based models described in psycholinguistics, DLMs encode words and their context as continuous numerical vectors. These “embeddings” are constructed by a sequence of layered computations to ultimately capture surprisingly sophisticated representations of linguistic structures. How does this layered hierarchy map onto the human brain during natural language comprehension? In this study, we used ECoG to record neural activity in language areas along the superior temporal gyrus and inferior frontal gyrus while human participants listened to a 30-minute spoken narrative. We supplied this same narrative to a high-performing DLM (GPT2-XL) and extracted the contextual embeddings for each word in the story across all 48 layers of the model. We next trained a set of linear encoding models to predict the temporally-evolving neural activity from the embeddings at each layer. We found a striking correspondence between the layer-by-layer sequence of embeddings from GPT2-XL and the temporal sequence of neural activity in language areas. In addition, we found evidence for the gradual accumulation of recurrent information along the linguistic processing hierarchy. However, we also noticed additional neural processes that took place in the brain, but not in DLMs, during the processing of surprising (unpredictable) words. These findings point to a connection between language processing in humans and DLMs where the layer-by-layer accumulation of contextual information in DLM embeddings matches the temporal dynamics of neural activity in high-order language areas.",,"Samuel Nastase, Sreejan Kumar",
162,1266,Alicia,Zeng,aliciaxiaozeng@gmail.com,"University of California, Berkeley",615-944-5311,Richmond,CA,94804,United States,"
[A deep learning framework for neuroscience]
Systems neuroscience seeks explanations for how the brain implements a wide variety of perceptual, cognitive and motor tasks. Conversely, artificial intelligence attempts to design computational systems based on the tasks they will have to solve. In artificial neural networks, the three components specified by design are the objective functions, the learning rules and the architectures. With the growing success of deep learning, which utilizes brain-inspired architectures, these three designed components have increasingly become central to how we model, engineer and optimize complex artificial learning systems. Here we argue that a greater focus on these components would also benefit systems neuroscience. We give examples of how this optimization-based framework can drive theoretical and experimental progress in neuroscience. We contend that this principled perspective on systems neuroscience will help 
[Joint representation of working memory and uncertainty in human cortex]
Neural representations of visual working memory (VWM) are noisy, and thus, decisions based on VWM are inevitably subject to uncertainty. However, the mechanisms by which the brain simultaneously represents the content and uncertainty of memory remain largely unknown. Here, inspired by the theory of probabilistic population codes, we test the hypothesis that the human brain represents an item maintained in VWM as a probability distribution over stimulus feature space, thereby capturing both its content and uncertainty. We used a neural generative model to decode probability distributions over memorized locations from fMRI activation patterns. We found that the mean of the probability distribution decoded from retinotopic cortical areas predicted memory reports on a trial-by-trial basis. Moreover, in several of the same mid-dorsal stream areas, the spread of the distribution predicted subjective trial-by-trial 
[The successor representation in human reinforcement learning]
Theories of reward learning in neuroscience have focused on two families of algorithms thought to capture deliberative versus habitual choice. ‘Model-based’ algorithms compute the value of candidate actions from scratch, whereas ‘model-free’ algorithms make choice more efficient but less flexible by storing pre-computed action values. We examine an intermediate algorithmic family, the successor representation, which balances flexibility and efficiency by storing partially computed action values: predictions about future events. These pre-computation strategies differ in how they update their choices following changes in a task. The successor representation’s reliance on stored predictions about future states predicts a unique signature of insensitivity to changes in the task’s sequence of events, but flexible adjustment following changes to rewards. We provide evidence for such differential sensitivity in two","Wei Ji Ma, Ida Momennejad, Konrad Kording, Tom Griffins, Deepak Pathak","Jack Gallant, Christine Tseng, Michele Winter, Tianjiao Zhang, Lily Gong, Emily Meschke, Cathy Chen, fMRI, Ethics",
163,1102,Ruiyi,Zhang,rz31@nyu.edu,New York University,646-420-8519,Jersey City,New Jersey,07304,United States,"[Generalization Demands Task-Appropriate Modular Neural Architectures]
Although both use reward-based learning, it is still unclear why deep reinforcement learning (RL) agents, in general, perform worse than the brain in novel out-of-distribution (OOD) tasks. Here, we propose one reason: generalization requires task-appropriate modular neural architectures like the brain; inferior generalization abilities result from using architectures without task-appropriate modules. To verify this hypothesis, we used a spatial navigation task to train RL agents with different neural architectures varying in modularity, then compared their generalization abilities in a novel task presenting novel sensorimotor mappings. We found that, although all agents could master the training task, only those with highly modular architectures separating computations of task variables learned a robust state representation, which supported generalization in the novel task. Our work exemplifies the rationale of the architectural modularity, i.e., supporting generalization.","Jessica Hamrick, Kimberly Stachenfeld, Jane Wang, Ida Momennejad, Constantin Rothkopf, Kelsey Allen","Yizhou Chen, Zhexin Xu, kaushik lakshminarasimhan",
164,1181,Tianjiao,Zhang,t.zhang@berkeley.edu,"University of California, Berkeley",925-247-4060,Berkeley,California,94707,United States,"[Navigation representations during active navigation are predominantly goal-directed]
Previous experiments revealed that the human brain represents many different navigational features. However, because most fMRI experiments study individual representations in isolation, the relative importance of these many different features to navigation remains unclear. To compare these representations, we recorded BOLD activity while subjects performed a taxi driver task in a large, realistic virtual world. Voxelwise modelling was performed with 21,283 stimulus- and task-related features that encompass 33 different types of information that might be represented during naturalistic navigation. The fit models show that navigational information is represented broadly across the cortex, including in many areas outside known navigation-related ROIs. Among navigational models, goal-directed representations account the most variance, while passive perceptual representations account for much less variance. These data suggest that representations during active naturalistic navigation are predominantly goal-directed.

[An active naturalistic navigation task induces large attentional shifts in semantic representation]
One important effect of visual selective attention is to shift perceptual representations toward attended semantic categories, increasing representation of attended categories at the cost of representation of other unattended categories (Çukur et al., 2013). However, prior work on this topic has only examined attentional tuning shifts under passive viewing conditions. Here we sought to determine whether similar shifts occur during active, naturalistic tasks. We used fMRI to record brain activity in three subjects while they performed an active navigation task in a large virtual world (110-180 minutes of data per subject). Data from the video game engine was used retrospectively to semantically segment the navigation video. Voxelwise encoding models were then estimated for each subject separately by using banded ridge regression to find a set of semantic weights that optimally predicted brain activity. A held-out dataset was used to test statistical significance, prediction accuracy and generalization. For comparison we also used fMRI to record brain activity while subjects passively watched naturalistic videos (210 minutes per subject; Huth et al., 2012). Videos were semantically labeled using WordNet. Voxelwise encoding models were estimated using the same procedure as in the active navigation task. Attentional shifts in semantic tuning between the active and passive tasks were identified by comparing the semantic maps obtained in the two different conditions. This comparison shows that active navigation elicits semantic tuning shifts broadly across the cerebral cortex. Strong attentional tuning shifts in semantic representation are found in RSC, OPA, PPA, IPS, and FEF. Somewhat weaker shifts are found in EBA, FFA, and TPJ. Anterior visual regions appear to shift semantic tuning preferentially towards semantic categories that are particularly relevant for navigation, such as traffic signs. Thus, our results show that selective attention induces strong attentional tuning shifts between active and passive tasks.

[A naturalistic navigation task reveals rich distributed representations of information across the human cerebral cortex]
Navigation in the natural world is a challenging problem that engages many cognitive systems, including cognitive maps, attention, motor control, and planning. However, most fMRI navigation studies use highly simplified environments and tasks that are unlikely to engage all navigational processes. Thus, they cannot create detailed functional cortical maps of the many different types of information that are likely relevant for natural navigation. To recover detailed cortical maps of navigation-related information, we used fMRI to record whole-brain activity while subjects performed a taxi driver task in virtual reality. The pilot environment is a 1×1 km town without other agents. The main environment is a 1×2 mile city with traffic, pedestrians, and various neighborhoods and off-road areas. Subjects drove using an MR-compatible steering wheel and pedals constructed in our lab. One subject participated in the pilot for 130 minutes and the main environment for 260 minutes. A second subject participated in the pilot for 90 minutes. We applied the voxelwise modeling framework developed in our lab to the data. We extracted stimulus and task features from the experiment, and used banded ridge regression to find optimal weights for each feature for every voxel in each subject. We evaluated 16 feature spaces that captures various aspects of navigation. We used a separate dataset to test statistical significance and generalization of models in each subject. The recovered cortical maps show the PPA, RSC, and OPA represent information about roads, buildings, and boundaries. RSC and OPA also represent navigational affordances. RSC and precuneus tracks route progression. Visual motion-energy is represented across visual cortex, including the posterior parts of RSC, OPA, and PPA. FFA and EBA represent information about pedestrians and other vehicles. These results show that naturalistic navigation elicits rich cortical activity and navigation information is represented in distributed networks of brain regions.
",,,
165,1147,Chengxu,Zhuang,chengxuz@mit.edu,Massachusetts Institute of Technology,650-521-7258,Wilmington,DE,19801,United States,"[Unsupervised neural network models of the ventral visual stream]

Deep neural networks currently provide the best quantitative models of the response patterns of neurons throughout the primate ventral visual stream. However, such networks have remained implausible as a model of the development of the ventral stream, in part because they are trained with supervised methods requiring many more labels than are accessible to infants during development. Here, we report that recent rapid progress in unsupervised learning has largely closed this gap. We find that neural network models learned with deep unsupervised contrastive embedding methods achieve neural prediction accuracy in multiple ventral visual cortical areas that equals or exceeds that of models derived using today’s best supervised methods and that the mapping of these neural network models’ hidden layers is neuroanatomically consistent across the ventral stream. Strikingly, we find that these methods produce brain-like representations even when trained solely with real human child developmental data collected from head-mounted cameras, despite the fact that these datasets are noisy and limited. We also find that semisupervised deep contrastive embeddings can leverage small numbers of labeled examples to produce representations with substantially improved error-pattern consistency to human behavior. Taken together, these results illustrate a use of unsupervised learning to provide a quantitative model of a multiarea cortical brain system and present a strong candidate for a biologically plausible computational theory of primate sensory learning.

[Local aggregation for unsupervised learning of visual embeddings]

Unsupervised approaches to learning in neural networks are of substantial interest for furthering artificial intelligence, both because they would enable the training of networks without the need for large numbers of expensive annotations, and because they would be better models of the kind of general-purpose learning deployed by humans. However, unsupervised networks have long lagged behind the performance of their supervised counterparts, especially in the domain of large-scale visual recognition. Recent developments in training deep convolutional embeddings to maximize non-parametric instance separation and clustering objectives have shown promise in closing this gap. Here, we describe a method that trains an embedding function to maximize a metric of local aggregation, causing similar data instances to move together in the embedding space, while allowing dissimilar instances to separate. This aggregation metric is dynamic, allowing soft clusters of different scales to emerge. We evaluate our procedure on several large-scale visual recognition datasets, achieving state-of-the-art unsupervised transfer learning performance on object recognition in ImageNet, scene recognition in Places 205, and object detection in PASCAL VOC.

[Unsupervised learning from video with deep neural embeddings]

Because of the rich dynamical structure of videos andtheir ubiquity in everyday life, it is a natural idea that video data could serve as a powerful unsupervised learning signal for visual representations. However, instantiating this idea, especially at large scale, has remained a significant artificial intelligence challenge. Here we present the Video Instance Embedding (VIE) framework, which trains deep nonlinear embeddings on video sequence inputs. By learning embedding dimensions that identify and group similar videos together, while pushing inherently different videos apart in the embedding space, VIE captures the strong statistical structure inherent in videos, without the need for external annotation labels. We find that, when trained on a large-scale video dataset, VIE yields powerful representations both for action recognition and single-frame object categorization, showing substantially improving on the state of the art wherever direct comparisons are possible. We show that a two-pathway model with both static and dynamic processingpathways is optimal, provide analyses indicating how the model works, and perform ablation studies showing the importance of key architecture and loss function choices. Our results suggest that deep neural embeddings are a promising approach to unsupervised video learning fora wide variety of task domains.",,"Stanford University, MIT",